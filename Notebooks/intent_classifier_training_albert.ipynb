{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94rGcLZitQr0",
        "outputId": "83f00ee4-8f52-4bb1-cac4-3924941f098a"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJESVIrdr_wR"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertForSequenceClassification, AdamW, AutoTokenizer, AlbertForMultipleChoice\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DONPfoN4vgGz"
      },
      "outputs": [],
      "source": [
        "# Set up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "Raloz0yMydAo",
        "outputId": "4695120c-af72-4f07-d44a-adc82d4ef6b5"
      },
      "outputs": [],
      "source": [
        "# Load the labeled data\n",
        "total_df = pd.read_csv('/content/drive/MyDrive/NLP_project/dataset/intent_data/train_data.csv')\n",
        "total_df = total_df.dropna()\n",
        "print(total_df.shape)\n",
        "total_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZqaU9ww9UNS"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itjo7VqmzIYt"
      },
      "outputs": [],
      "source": [
        "# df = total_df.sample(frac=0.2, replace=True, random_state=1)\n",
        "# df.shape\n",
        "# Assuming 'label' is the name of the column containing the labels\n",
        "# total_df_stratified = total_df.groupby('Label', group_keys=False).apply(lambda x: x.sample(frac=0.2, replace=True, random_state=1))\n",
        "df = total_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OpkRkhevxS2"
      },
      "outputs": [],
      "source": [
        "# Tokenize the input text and convert to PyTorch tensors\n",
        "inputs = tokenizer(df['Question'].tolist(), padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
        "labels = torch.tensor(df['Label'].tolist()).to('cuda')\n",
        "# Create a PyTorch dataset and dataloader\n",
        "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eThPuaJAv5GF"
      },
      "outputs": [],
      "source": [
        "# train_size = int(0.8 * len(dataset))\n",
        "# val_size = len(dataset) - train_size\n",
        "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk9Bxw_9wC32",
        "outputId": "a841e1ae-b9d7-4665-e42f-2b213d003d2d"
      },
      "outputs": [],
      "source": [
        "# load model pretrained\n",
        "model = AlbertForSequenceClassification.from_pretrained('albert-base-v2').to('cuda')\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
        "\n",
        "# Set up the optimizer and loss function\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(dataloader):\n",
        "        input_ids, attention_mask, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids.to('cuda'), attention_mask=attention_mask.to('cuda'), labels=labels.to('cuda'))\n",
        "        loss = criterion(outputs.logits, labels.to('cuda'))\n",
        "        loss.backward()\n",
        "        \n",
        "        if (i+1) % 4 == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    print(f\"Epoch {epoch+1} loss: {running_loss/len(dataloader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PamZgP109CGq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# assuming your trained model is stored in a variable called `model`\n",
        "# and you want to save it in a file called `my_model.pt`\n",
        "torch.save(model.state_dict(), 'my_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hl08fGpBMuc"
      },
      "outputs": [],
      "source": [
        "# Load the labeled data\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP_project/dataset/intent_data/test_data.csv')\n",
        "test_df = test_df.dropna()\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uNvhrT0BOrA"
      },
      "outputs": [],
      "source": [
        "# df = total_df.sample(frac=0.2, replace=True, random_state=1)\n",
        "# df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBiNlrwyBfSV"
      },
      "outputs": [],
      "source": [
        "# Tokenize the input text and convert to PyTorch tensors\n",
        "test_inputs = tokenizer(test_df['Question'].tolist(), padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
        "test_labels = torch.tensor(test_df['Label'].tolist()).to('cuda')\n",
        "# Create a PyTorch dataset and dataloader\n",
        "test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKPQxK1mGGvF"
      },
      "outputs": [],
      "source": [
        "num_samples = len(test_dataset)\n",
        "print(num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6qeqO-PGLlt"
      },
      "outputs": [],
      "source": [
        "tensor_shape = test_dataset[0][0].size()\n",
        "print(tensor_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GP0A5ZgXAaKc"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # assuming your trained model is stored in a variable called `model`\n",
        "# # and your test dataset is stored in a variable called `test_dataset`\n",
        "# # and you have defined your evaluation criterion as `criterion`\n",
        "\n",
        "# # put your test dataset in a DataLoader\n",
        "# batch_size = 32\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# # set your model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# # evaluate the model on the test dataset\n",
        "# test_loss = 0.0\n",
        "# correct = 0\n",
        "# total_samples = 0\n",
        "\n",
        "# # loop over the test data\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_dataloader:\n",
        "#         # unpack the input and target tensors\n",
        "#         inputs, targets, _ = batch # assuming there are 3 tensors in the batch\n",
        "        \n",
        "#         # move the tensors to the same device as the model\n",
        "#         inputs = inputs.to(device)\n",
        "#         targets = targets.to(device)\n",
        "\n",
        "#         # compute the model's predictions\n",
        "#         outputs = model(inputs)\n",
        "\n",
        "#         # check the shape of the target tensor\n",
        "#         print(targets.shape)\n",
        "\n",
        "#         # reshape the target tensor to a 1D tensor\n",
        "#         # targets = targets.view(-1)\n",
        "\n",
        "#         # check the shape of the target tensor again\n",
        "#         print(outputs.logits.shape)\n",
        "\n",
        "#         # compute the loss\n",
        "#         loss = criterion(outputs.logits, targets)\n",
        "\n",
        "#         # accumulate the total loss for the batch\n",
        "#         test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "#         # compute the number of correct predictions\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         correct += (predicted == targets).sum().item()\n",
        "\n",
        "# # compute the average loss and accuracy\n",
        "# test_loss /= len(test_dataset)\n",
        "# accuracy = correct / len(test_dataset)\n",
        "\n",
        "# # print the results\n",
        "# print(f'Test Loss: {test_loss:.4f} Accuracy: {accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k0B0Ia_FADL"
      },
      "outputs": [],
      "source": [
        "print(\"Creation of the results' folder...\")\n",
        "!mkdir results\n",
        "\n",
        "import csv\n",
        "\n",
        "def get_probs_from_logits(logits):\n",
        "    \"\"\"\n",
        "    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    return probs.detach().cpu().numpy()\n",
        "\n",
        "def test_prediction(net, device, dataloader, with_labels=True, result_file=\"results/output.csv\"):\n",
        "    \"\"\"\n",
        "    Predict the probabilities on a dataset with or without labels and print the result in a file\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    header = [\"probability\"]\n",
        "    with open(result_file, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(header)\n",
        "        with torch.no_grad():\n",
        "            if with_labels:\n",
        "                for input_ids, attn_mask, labels in tqdm(dataloader):\n",
        "                    input_ids, attn_mask, labels = input_ids.to(device), attn_mask.to(device), labels.to(device)\n",
        "                    outputs = net(input_ids, attention_mask=attn_mask, labels=labels)\n",
        "                    loss = criterion(outputs.logits, labels)\n",
        "                    probs = get_probs_from_logits(outputs.logits.squeeze(-1)).squeeze(-1)\n",
        "                    for prob in probs.tolist():\n",
        "                        writer.writerow([prob])\n",
        "            else:\n",
        "                for input_ids, attn_mask in tqdm(dataloader):\n",
        "                    input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n",
        "                    logits = net(input_ids, attn_mask)\n",
        "                    probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n",
        "                    for prob in probs.tolist():\n",
        "                        writer.writerow([prob])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZM4zLkmFPLm"
      },
      "outputs": [],
      "source": [
        "path_to_model = '/content/my_model.pt'  \n",
        "# path_to_model = '/content/models/...'  # You can add here your trained model\n",
        "\n",
        "path_to_output_file = 'results/output.csv'\n",
        "\n",
        "print(\"Reading test data...\")\n",
        "# test_set = CustomDataset(df_test, maxlen, bert_model)\n",
        "batch_size = 32\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "# test_loader = DataLoader(test_set, batch_size=bs, num_workers=5)\n",
        "\n",
        "# model = AlbertForSequenceClassification.from_pretrained('albert-base-v2').to('cuda')\n",
        "# if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
        "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "#     model = nn.DataParallel(model)\n",
        "\n",
        "print()\n",
        "print(\"Loading the weights of the model...\")\n",
        "model.load_state_dict(torch.load(path_to_model))\n",
        "model.to(device)\n",
        "\n",
        "print(\"Predicting on test data...\")\n",
        "test_prediction(net=model, device=device, dataloader=test_dataloader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\n",
        "                result_file=path_to_output_file)\n",
        "print()\n",
        "print(\"Predictions are available in : {}\".format(path_to_output_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTw1fvSKKU-M"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVrp8wTbQnAT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/NLP_project/dataset/intent_data/test_data.csv')\n",
        "test_df = test_df.dropna()\n",
        "print(test_df.shape)\n",
        "\n",
        "# Load the true labels from the test dataframe\n",
        "labels_test = test_df['Label']\n",
        "\n",
        "# Load the predicted probabilities from the output.csv file\n",
        "probs_test = pd.read_csv('/content/results/output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9OGe9DtVA_H"
      },
      "outputs": [],
      "source": [
        "probs_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XtTuXygN9gG"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "# Convert the probability string to a list of probabilities\n",
        "probs_test['probability'] = probs_test['probability'].apply(lambda x: ast.literal_eval(x))\n",
        "probs_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks3I9jO0M2qj"
      },
      "outputs": [],
      "source": [
        "# Get index with the highest probability\n",
        "probs_test['max_index'] = probs_test['probability'].apply(lambda x: x.index(max(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-0WL6w6M8qd"
      },
      "outputs": [],
      "source": [
        "probs_test['max_index'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnY0X4h8Oce1"
      },
      "outputs": [],
      "source": [
        "# probs_test.drop(columns=['probability'], inplace=True)\n",
        "probs_test.to_csv('/content/drive/MyDrive/NLP_project/dataset/results/outputs.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnpuqhhlUG-C"
      },
      "outputs": [],
      "source": [
        "# for line in probs_test:\n",
        "#     # Remove the square brackets and split the line into a list of strings\n",
        "#     probs_str = line.strip()[1:-1].split(', ')\n",
        "#     print(\"probs_str: \", probs_str)\n",
        "#     # Convert the list of strings to a list of floats\n",
        "#     probs = [float(p) for p in probs_str]\n",
        "#     print(\"probs: \", probs)\n",
        "#     # Append the list of probabilities to the main list\n",
        "#     # probs_list.append(probs)\n",
        "#     break;\n",
        "\n",
        "# # Convert the list of lists to a numpy array\n",
        "# # probs_np = np.array(probs_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJkumyuSUL31"
      },
      "outputs": [],
      "source": [
        "# # Determine the predicted class label for each prediction\n",
        "# preds_test = [np.argmax(prob) for prob in probs_test]\n",
        "# print(len(preds_test))\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(labels_test, preds_test)\n",
        "\n",
        "# # Calculate f1 score\n",
        "# f1 = f1_score(labels_test, preds_test)\n",
        "\n",
        "# # Calculate confusion matrix\n",
        "# cm = confusion_matrix(labels_test, preds_test)\n",
        "\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "# print(\"F1 score:\", f1)\n",
        "# print(\"Confusion matrix:\")\n",
        "# print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paTQyCZXPQ9w"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBpDvSYGJ18y"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "path_to_output_file = 'results/output.csv'  # path to the file with prediction probabilities\n",
        "\n",
        "labels_test = test_df['Label']  # true labels\n",
        "preds_test = pd.read_csv('/content/drive/MyDrive/NLP_project/dataset/results/outputs.csv')\n",
        "\n",
        "# probs_test = pd.read_csv(path_to_output_file, header=None)[0]  # prediction probabilities\n",
        "# threshold = 0.5   # you can adjust this threshold for your own dataset\n",
        "# preds_test=(probs_test.astype('float')>=threshold).astype('uint8') # predicted labels using the above fixed threshold\n",
        "\n",
        "preds_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rXn2OkBPZfT"
      },
      "outputs": [],
      "source": [
        "print(labels_test.shape)\n",
        "print(preds_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drPChTeUP_Iq"
      },
      "outputs": [],
      "source": [
        "labels_test = labels_test.to_numpy().reshape(-1, 1)\n",
        "print(labels_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-Dz2r0qQR30"
      },
      "outputs": [],
      "source": [
        "preds_test = preds_test['max_index'].to_numpy().reshape(-1, 1)\n",
        "print(preds_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNWSnFX-KJG6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"glue\", \"mrpc\")\n",
        "# Compute the accuracy and F1 scores\n",
        "metric._compute(predictions=preds_test, references=labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iLHxBqwRPTs"
      },
      "outputs": [],
      "source": [
        "# Determine the predicted class label for each prediction\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(labels_test, preds_test)\n",
        "\n",
        "# Calculate f1 score\n",
        "f1 = f1_score(labels_test, preds_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(labels_test, preds_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 score:\", f1)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXv1Ayp9RlrC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/NLP_project/dataset/intent_data/test_data.csv')\n",
        "preds_test = pd.read_csv('/content/drive/MyDrive/NLP_project/dataset/results/outputs.csv')\n",
        "\n",
        "combined_test_pred = pd.concat([test_data, preds_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wXZ2irSX6q"
      },
      "outputs": [],
      "source": [
        "combined_test_pred = combined_test_pred.rename(columns={'max_index': 'prediction'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H470pPjwTFUT"
      },
      "outputs": [],
      "source": [
        "combined_test_pred.to_csv('/content/drive/MyDrive/NLP_project/dataset/results/prediction_actual.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
