{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsb-Cvf6lnVX"
      },
      "source": [
        "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>\n",
        "\n",
        "**Author**: Stephen Roller ([GitHub](https://github.com/stephenroller), [Twitter](https://twitter.com/stephenroller))\n",
        "\n",
        "\n",
        "# Welcome to the ParlAI interactive tutorial\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "- Chat with a neural network model!\n",
        "- Show how to use common commands in ParlAI, like inspecting data and model outputs.\n",
        "- See where to find information about many options.\n",
        "- Show how to fine-tune a pretrained model on a specific task\n",
        "- Add our own datasets to ParlAI\n",
        "- And add our own models to ParlAI\n",
        "\n",
        "We won't be running any examples of using Amazon Mechanical Turk, or connecting to Chat services, but you can check out our [docs](https://parl.ai/docs/) for more information on these areas.\n",
        "\n",
        "**Note:** *Make sure you're running this session with a GPU attached.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bFnOWslsj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6104e962-131d-42ec-f99c-59a6340f4d3b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 13 15:00:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMxd1KIRl9Xm"
      },
      "source": [
        "## Installing parlai\n",
        "\n",
        "We need to install ParlAI. Since we're in Google Colab, we can assume PyTorch and similar dependencies are installed already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i93Mn_I7MOEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18dff4ad-f0f0-4aa4-e964-0f3e3b22d45e"
      },
      "source": [
        "!pip install -q parlai\n",
        "!pip install -q subword_nmt # extra requirement we need for this tutorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 10.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 35.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 285 kB 52.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.0 MB 31.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 38.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 547 kB 38.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 235 kB 52.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 208 kB 45.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 49.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 49.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 190 kB 43.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 49.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 748 kB 26.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 38.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 52.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 52.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 47.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 32.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 51.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 792 kB/s \n",
            "\u001b[K     |████████████████████████████████| 100 kB 9.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 121 kB 54.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 890 kB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVz5dCUmFkN"
      },
      "source": [
        "# Chatting with a model\n",
        "\n",
        "Let's start by chatting interactively with a model file from our model zoo! We'll pick our \"tutorial transformer generator\" model, which is a generative transformer trained on pushshift.io Reddit. You can take a look at the [model zoo](https://parl.ai/docs/zoo.html) for a more complete list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJGRtMKmIWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e286328-fe89-4b44-8e0d-1ab1cf5ac656"
      },
      "source": [
        "# Import the Interactive script\n",
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "# call it with particular args\n",
        "Interactive.main(\n",
        "    # the model_file is a filename path pointing to a particular model dump.\n",
        "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
        "    # They'll be automatically downloaded when you ask to use them.\n",
        "    model_file='zoo:tutorial_transformer_generator/model'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:02:27 | building data: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "15:02:27 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [01:30<00:00, 12.4MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:04:22 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
            "15:04:22 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "15:04:22 | Using CUDA\n",
            "15:04:22 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "15:04:22 | num words = 54944\n",
            "15:04:22 | TransformerGenerator: full interactive mode on.\n",
            "15:04:23 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "15:04:38 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "15:04:38 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "15:04:39 | Opt:\n",
            "15:04:39 |     activation: gelu\n",
            "15:04:39 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "15:04:39 |     adam_eps: 1e-06\n",
            "15:04:39 |     add_p1_after_newln: False\n",
            "15:04:39 |     aggregate_micro: False\n",
            "15:04:39 |     allow_missing_init_opts: False\n",
            "15:04:39 |     attention_dropout: 0.0\n",
            "15:04:39 |     batch_length_range: 5\n",
            "15:04:39 |     batch_sort_cache_type: pop\n",
            "15:04:39 |     batch_sort_field: text\n",
            "15:04:39 |     batchsize: 48\n",
            "15:04:39 |     beam_block_full_context: False\n",
            "15:04:39 |     beam_block_list_filename: None\n",
            "15:04:39 |     beam_block_ngram: 3\n",
            "15:04:39 |     beam_context_block_ngram: 3\n",
            "15:04:39 |     beam_delay: 30\n",
            "15:04:39 |     beam_length_penalty: 0.65\n",
            "15:04:39 |     beam_min_length: 10\n",
            "15:04:39 |     beam_min_n_best: 3\n",
            "15:04:39 |     beam_size: 8\n",
            "15:04:39 |     betas: '[0.9, 0.98]'\n",
            "15:04:39 |     bpe_add_prefix_space: None\n",
            "15:04:39 |     bpe_debug: False\n",
            "15:04:39 |     bpe_dropout: None\n",
            "15:04:39 |     bpe_merge: None\n",
            "15:04:39 |     bpe_vocab: None\n",
            "15:04:39 |     checkpoint_activations: False\n",
            "15:04:39 |     compute_tokenized_bleu: False\n",
            "15:04:39 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:04:39 |     datatype: train:stream\n",
            "15:04:39 |     delimiter: '\\n'\n",
            "15:04:39 |     dict_build_first: True\n",
            "15:04:39 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:04:39 |     dict_endtoken: __end__\n",
            "15:04:39 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "15:04:39 |     dict_include_test: False\n",
            "15:04:39 |     dict_include_valid: False\n",
            "15:04:39 |     dict_initpath: None\n",
            "15:04:39 |     dict_language: english\n",
            "15:04:39 |     dict_loaded: True\n",
            "15:04:39 |     dict_lower: True\n",
            "15:04:39 |     dict_max_ngram_size: -1\n",
            "15:04:39 |     dict_maxexs: -1\n",
            "15:04:39 |     dict_maxtokens: -1\n",
            "15:04:39 |     dict_minfreq: 0\n",
            "15:04:39 |     dict_nulltoken: __null__\n",
            "15:04:39 |     dict_starttoken: __start__\n",
            "15:04:39 |     dict_textfields: text,labels\n",
            "15:04:39 |     dict_tokenizer: bpe\n",
            "15:04:39 |     dict_unktoken: __unk__\n",
            "15:04:39 |     display_add_fields: \n",
            "15:04:39 |     display_examples: False\n",
            "15:04:39 |     display_prettify: False\n",
            "15:04:39 |     distributed_world_size: 64\n",
            "15:04:39 |     download_path: None\n",
            "15:04:39 |     dropout: 0.1\n",
            "15:04:39 |     dynamic_batching: None\n",
            "15:04:39 |     embedding_projection: random\n",
            "15:04:39 |     embedding_size: 512\n",
            "15:04:39 |     embedding_type: random\n",
            "15:04:39 |     embeddings_scale: True\n",
            "15:04:39 |     eval_batchsize: None\n",
            "15:04:39 |     evaltask: None\n",
            "15:04:39 |     ffn_size: 2048\n",
            "15:04:39 |     force_fp16_tokens: True\n",
            "15:04:39 |     fp16: True\n",
            "15:04:39 |     fp16_impl: safe\n",
            "15:04:39 |     gpu: 0\n",
            "15:04:39 |     gradient_clip: 10.0\n",
            "15:04:39 |     hide_labels: False\n",
            "15:04:39 |     history_add_global_end_token: None\n",
            "15:04:39 |     history_reversed: False\n",
            "15:04:39 |     history_size: -1\n",
            "15:04:39 |     image_cropsize: 224\n",
            "15:04:39 |     image_mode: raw\n",
            "15:04:39 |     image_size: 256\n",
            "15:04:39 |     inference: beam\n",
            "15:04:39 |     init_model: None\n",
            "15:04:39 |     init_opt: None\n",
            "15:04:39 |     interactive_mode: True\n",
            "15:04:39 |     interactive_task: True\n",
            "15:04:39 |     invsqrt_lr_decay_gamma: -1\n",
            "15:04:39 |     is_debug: False\n",
            "15:04:39 |     label_truncate: 128\n",
            "15:04:39 |     learn_positional_embeddings: True\n",
            "15:04:39 |     learningrate: 0.0005\n",
            "15:04:39 |     local_human_candidates_file: None\n",
            "15:04:39 |     log_every_n_secs: 30.0\n",
            "15:04:39 |     log_keep_fields: all\n",
            "15:04:39 |     loglevel: info\n",
            "15:04:39 |     lr_scheduler: invsqrt\n",
            "15:04:39 |     lr_scheduler_decay: 0.5\n",
            "15:04:39 |     lr_scheduler_patience: 3\n",
            "15:04:39 |     max_train_time: -1\n",
            "15:04:39 |     metrics: default\n",
            "15:04:39 |     model: transformer/generator\n",
            "15:04:39 |     model_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "15:04:39 |     model_parallel: False\n",
            "15:04:39 |     momentum: 0\n",
            "15:04:39 |     multitask_weights: [1]\n",
            "15:04:39 |     n_decoder_layers: -1\n",
            "15:04:39 |     n_encoder_layers: -1\n",
            "15:04:39 |     n_heads: 16\n",
            "15:04:39 |     n_layers: 8\n",
            "15:04:39 |     n_positions: 512\n",
            "15:04:39 |     n_segments: 0\n",
            "15:04:39 |     nesterov: True\n",
            "15:04:39 |     no_cuda: False\n",
            "15:04:39 |     num_epochs: 5.0\n",
            "15:04:39 |     numthreads: 1\n",
            "15:04:39 |     numworkers: 4\n",
            "15:04:39 |     nus: [0.7]\n",
            "15:04:39 |     optimizer: fused_adam\n",
            "15:04:39 |     outfile: \n",
            "15:04:39 |     output_scaling: 1.0\n",
            "15:04:39 |     override: \"{'model_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model'}\"\n",
            "15:04:39 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:04:39 |     person_tokens: False\n",
            "15:04:39 |     port: 61337\n",
            "15:04:39 |     pytorch_context_length: -1\n",
            "15:04:39 |     pytorch_datapath: None\n",
            "15:04:39 |     pytorch_include_labels: True\n",
            "15:04:39 |     pytorch_preprocess: False\n",
            "15:04:39 |     pytorch_teacher_batch_sort: False\n",
            "15:04:39 |     pytorch_teacher_dataset: None\n",
            "15:04:39 |     pytorch_teacher_task: None\n",
            "15:04:39 |     rank_candidates: False\n",
            "15:04:39 |     relu_dropout: 0.0\n",
            "15:04:39 |     save_after_valid: True\n",
            "15:04:39 |     save_every_n_secs: -1\n",
            "15:04:39 |     save_format: conversations\n",
            "15:04:39 |     share_word_embeddings: True\n",
            "15:04:39 |     short_final_eval: True\n",
            "15:04:39 |     show_advanced_args: False\n",
            "15:04:39 |     shuffle: False\n",
            "15:04:39 |     single_turn: False\n",
            "15:04:39 |     skip_generation: False\n",
            "15:04:39 |     special_tok_lst: None\n",
            "15:04:39 |     split_lines: False\n",
            "15:04:39 |     starttime: Oct13_15-04\n",
            "15:04:39 |     task: internal:new_reddit:presorted\n",
            "15:04:39 |     temperature: 1.0\n",
            "15:04:39 |     tensorboard_log: False\n",
            "15:04:39 |     text_truncate: 512\n",
            "15:04:39 |     topk: 10\n",
            "15:04:39 |     topp: 0.9\n",
            "15:04:39 |     truncate: -1\n",
            "15:04:39 |     update_freq: 1\n",
            "15:04:39 |     use_reply: label\n",
            "15:04:39 |     validation_cutoff: 1.0\n",
            "15:04:39 |     validation_every_n_epochs: -1\n",
            "15:04:39 |     validation_every_n_secs: 1800.0\n",
            "15:04:39 |     validation_max_exs: 9920\n",
            "15:04:39 |     validation_metric: ppl\n",
            "15:04:39 |     validation_metric_mode: min\n",
            "15:04:39 |     validation_patience: 0\n",
            "15:04:39 |     validation_share_agent: False\n",
            "15:04:39 |     variant: xlm\n",
            "15:04:39 |     verbose: False\n",
            "15:04:39 |     warmup_rate: 0.0001\n",
            "15:04:40 |     warmup_updates: 20000\n",
            "15:04:40 |     weight_decay: 0.01\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "15:04:40 | creating task(s): interactive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfUEgovmWay"
      },
      "source": [
        "The same on the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hGrZGGmaWF"
      },
      "source": [
        "# Taking a look at some data\n",
        "\n",
        "We can look at look into a specific dataset. Let's look into the \"empathetic dialogues\" dataset, which aims to teach models how to respond with text expressing the appropriate emotion. We have over existing 80 datasets in ParlAI. You can take a full look in our [task list](https://parl.ai/docs/tasks.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqckSXqlmWuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45fd031-547e-4389-c7c9-57819bfbbca0"
      },
      "source": [
        "# The display_data script is used to show the contents of a particular task.\n",
        "# By default, we show the train\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21:19:40 | Opt:\n",
            "21:19:40 |     allow_missing_init_opts: False\n",
            "21:19:40 |     batchsize: 1\n",
            "21:19:40 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:19:40 |     datatype: train:ordered\n",
            "21:19:40 |     dict_class: None\n",
            "21:19:40 |     display_add_fields: \n",
            "21:19:40 |     download_path: None\n",
            "21:19:40 |     dynamic_batching: None\n",
            "21:19:40 |     hide_labels: False\n",
            "21:19:40 |     ignore_agent_reply: True\n",
            "21:19:40 |     image_cropsize: 224\n",
            "21:19:40 |     image_mode: raw\n",
            "21:19:40 |     image_size: 256\n",
            "21:19:40 |     init_model: None\n",
            "21:19:40 |     init_opt: None\n",
            "21:19:40 |     loglevel: info\n",
            "21:19:40 |     max_display_len: 1000\n",
            "21:19:40 |     model: None\n",
            "21:19:40 |     model_file: None\n",
            "21:19:40 |     multitask_weights: [1]\n",
            "21:19:40 |     mutators: None\n",
            "21:19:40 |     num_examples: 5\n",
            "21:19:40 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
            "21:19:40 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:19:40 |     remove_political_convos: False\n",
            "21:19:40 |     starttime: Apr16_21-19\n",
            "21:19:40 |     task: empathetic_dialogues\n",
            "21:19:40 |     train_experiencer_only: False\n",
            "21:19:40 |     verbose: False\n",
            "21:19:40 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered\n",
            "[building data: /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues]\n",
            "21:19:40 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.7/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:02<00:00, 10.9MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:19:45 | \u001b[33mparlai.tasks.empathetic_dialogues.agents.DefaultTeacher' is outputting dicts instead of messages. If this is a teacher that is part of ParlAI, please file an issue on GitHub. If it is your own teacher, please return a Message object instead.\u001b[0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "21:19:45 | loaded 39057 episodes with a total of 64636 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9C6oHq87zGx"
      },
      "source": [
        "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
        "\n",
        "We can also ask to see fewer examples, and get them from the validation set instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNSBetWmfGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e13563e-a749-49f6-b6b0-19d2c7473448"
      },
      "source": [
        "# we can instead ask to see fewer examples, and get them from the valid set.\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21:19:49 | Opt:\n",
            "21:19:49 |     allow_missing_init_opts: False\n",
            "21:19:49 |     batchsize: 1\n",
            "21:19:49 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:19:49 |     datatype: valid\n",
            "21:19:49 |     dict_class: None\n",
            "21:19:49 |     display_add_fields: \n",
            "21:19:49 |     download_path: None\n",
            "21:19:49 |     dynamic_batching: None\n",
            "21:19:49 |     hide_labels: False\n",
            "21:19:49 |     ignore_agent_reply: True\n",
            "21:19:49 |     image_cropsize: 224\n",
            "21:19:49 |     image_mode: raw\n",
            "21:19:49 |     image_size: 256\n",
            "21:19:49 |     init_model: None\n",
            "21:19:49 |     init_opt: None\n",
            "21:19:49 |     loglevel: info\n",
            "21:19:49 |     max_display_len: 1000\n",
            "21:19:49 |     model: None\n",
            "21:19:49 |     model_file: None\n",
            "21:19:49 |     multitask_weights: [1]\n",
            "21:19:49 |     mutators: None\n",
            "21:19:49 |     num_examples: 3\n",
            "21:19:49 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 3, 'datatype': 'valid'}\"\n",
            "21:19:49 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:19:49 |     remove_political_convos: False\n",
            "21:19:49 |     starttime: Apr16_21-19\n",
            "21:19:49 |     task: empathetic_dialogues\n",
            "21:19:49 |     train_experiencer_only: False\n",
            "21:19:49 |     verbose: False\n",
            "21:19:49 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "21:19:49 | loaded 2769 episodes with a total of 5738 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrgRrEmdS-"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "```\n",
        "or a bit shorter\n",
        "```\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M8Zr86n2_G"
      },
      "source": [
        "# Training a model\n",
        "\n",
        "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
        "\n",
        "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhVQycSn2q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e424398c-1dc2-4bde-b723-6b2df62211cb"
      },
      "source": [
        "# we'll save it in the \"from_scratch_model\" directory\n",
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # we MUST provide a filename\n",
        "    model_file='from_scratch_model/model',\n",
        "    # train on empathetic dialogues\n",
        "    task='empathetic_dialogues',\n",
        "    # limit training time to 2 minutes, and a batchsize of 16\n",
        "    max_train_time=2 * 60,\n",
        "    batchsize=16,\n",
        "    \n",
        "    # we specify the model type as seq2seq\n",
        "    model='seq2seq',\n",
        "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
        "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
        "    # time to download.\n",
        "    attention='dot',\n",
        "    # tie the word embeddings of the encoder/decoder/softmax.\n",
        "    lookuptable='all',\n",
        "    # truncate text and labels at 64 tokens, for memory and time savings\n",
        "    truncate=64,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21:19:55 | building dictionary first...\n",
            "21:19:55 | Opt:\n",
            "21:19:55 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "21:19:55 |     adam_eps: 1e-08\n",
            "21:19:55 |     add_p1_after_newln: False\n",
            "21:19:55 |     aggregate_micro: False\n",
            "21:19:55 |     allow_missing_init_opts: False\n",
            "21:19:55 |     attention: dot\n",
            "21:19:55 |     attention_length: 48\n",
            "21:19:55 |     attention_time: post\n",
            "21:19:55 |     batchsize: 1\n",
            "21:19:55 |     beam_block_full_context: True\n",
            "21:19:55 |     beam_block_list_filename: None\n",
            "21:19:55 |     beam_block_ngram: -1\n",
            "21:19:55 |     beam_context_block_ngram: -1\n",
            "21:19:55 |     beam_delay: 30\n",
            "21:19:55 |     beam_length_penalty: 0.65\n",
            "21:19:55 |     beam_min_length: 1\n",
            "21:19:55 |     beam_size: 1\n",
            "21:19:55 |     betas: '(0.9, 0.999)'\n",
            "21:19:55 |     bidirectional: False\n",
            "21:19:55 |     bpe_add_prefix_space: None\n",
            "21:19:55 |     bpe_debug: False\n",
            "21:19:55 |     bpe_dropout: None\n",
            "21:19:55 |     bpe_merge: None\n",
            "21:19:55 |     bpe_vocab: None\n",
            "21:19:55 |     compute_tokenized_bleu: False\n",
            "21:19:55 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:19:55 |     datatype: train\n",
            "21:19:55 |     decoder: same\n",
            "21:19:55 |     delimiter: '\\n'\n",
            "21:19:55 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:19:55 |     dict_endtoken: __end__\n",
            "21:19:55 |     dict_file: from_scratch_model/model.dict\n",
            "21:19:55 |     dict_include_test: False\n",
            "21:19:55 |     dict_include_valid: False\n",
            "21:19:55 |     dict_initpath: None\n",
            "21:19:55 |     dict_language: english\n",
            "21:19:55 |     dict_loaded: False\n",
            "21:19:55 |     dict_lower: False\n",
            "21:19:55 |     dict_max_ngram_size: -1\n",
            "21:19:55 |     dict_maxexs: -1\n",
            "21:19:55 |     dict_maxtokens: -1\n",
            "21:19:55 |     dict_minfreq: 0\n",
            "21:19:55 |     dict_nulltoken: __null__\n",
            "21:19:55 |     dict_starttoken: __start__\n",
            "21:19:55 |     dict_textfields: text,labels\n",
            "21:19:55 |     dict_tokenizer: re\n",
            "21:19:55 |     dict_unktoken: __unk__\n",
            "21:19:55 |     display_examples: False\n",
            "21:19:55 |     download_path: None\n",
            "21:19:55 |     dropout: 0.1\n",
            "21:19:55 |     dynamic_batching: None\n",
            "21:19:55 |     embedding_projection: random\n",
            "21:19:55 |     embedding_type: random\n",
            "21:19:55 |     embeddingsize: 128\n",
            "21:19:55 |     eval_batchsize: None\n",
            "21:19:55 |     eval_dynamic_batching: None\n",
            "21:19:55 |     evaltask: None\n",
            "21:19:55 |     force_fp16_tokens: False\n",
            "21:19:55 |     fp16: False\n",
            "21:19:55 |     fp16_impl: safe\n",
            "21:19:55 |     gpu: -1\n",
            "21:19:55 |     gradient_clip: 0.1\n",
            "21:19:55 |     hiddensize: 128\n",
            "21:19:55 |     hide_labels: False\n",
            "21:19:55 |     history_add_global_end_token: None\n",
            "21:19:55 |     history_reversed: False\n",
            "21:19:55 |     history_size: -1\n",
            "21:19:55 |     image_cropsize: 224\n",
            "21:19:55 |     image_mode: no_image_model\n",
            "21:19:55 |     image_size: 256\n",
            "21:19:55 |     inference: greedy\n",
            "21:19:55 |     init_model: None\n",
            "21:19:55 |     init_opt: None\n",
            "21:19:55 |     input_dropout: 0.0\n",
            "21:19:55 |     interactive_mode: False\n",
            "21:19:55 |     invsqrt_lr_decay_gamma: -1\n",
            "21:19:55 |     label_truncate: None\n",
            "21:19:55 |     learningrate: 1\n",
            "21:19:55 |     load_from_checkpoint: True\n",
            "21:19:55 |     log_every_n_secs: 10\n",
            "21:19:55 |     loglevel: info\n",
            "21:19:55 |     lookuptable: all\n",
            "21:19:55 |     lr_scheduler: reduceonplateau\n",
            "21:19:55 |     lr_scheduler_decay: 0.5\n",
            "21:19:55 |     lr_scheduler_patience: 3\n",
            "21:19:55 |     max_lr_steps: -1\n",
            "21:19:55 |     max_train_time: 120.0\n",
            "21:19:55 |     metrics: default\n",
            "21:19:55 |     model: seq2seq\n",
            "21:19:55 |     model_file: from_scratch_model/model\n",
            "21:19:55 |     momentum: 0\n",
            "21:19:55 |     multitask_weights: [1]\n",
            "21:19:55 |     mutators: None\n",
            "21:19:55 |     nesterov: True\n",
            "21:19:55 |     no_cuda: False\n",
            "21:19:55 |     num_epochs: -1\n",
            "21:19:55 |     numlayers: 2\n",
            "21:19:55 |     numsoftmax: 1\n",
            "21:19:55 |     nus: (0.7,)\n",
            "21:19:55 |     optimizer: sgd\n",
            "21:19:55 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "21:19:55 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:19:55 |     person_tokens: False\n",
            "21:19:55 |     rank_candidates: False\n",
            "21:19:55 |     remove_political_convos: False\n",
            "21:19:55 |     rnn_class: lstm\n",
            "21:19:55 |     save_after_valid: False\n",
            "21:19:55 |     save_every_n_secs: -1\n",
            "21:19:55 |     short_final_eval: False\n",
            "21:19:55 |     skip_generation: False\n",
            "21:19:55 |     special_tok_lst: None\n",
            "21:19:55 |     split_lines: False\n",
            "21:19:55 |     starttime: Apr16_21-19\n",
            "21:19:55 |     task: empathetic_dialogues\n",
            "21:19:55 |     temperature: 1.0\n",
            "21:19:55 |     tensorboard_log: False\n",
            "21:19:55 |     tensorboard_logdir: None\n",
            "21:19:55 |     text_truncate: None\n",
            "21:19:55 |     topk: 10\n",
            "21:19:55 |     topp: 0.9\n",
            "21:19:55 |     train_experiencer_only: False\n",
            "21:19:55 |     truncate: 64\n",
            "21:19:55 |     update_freq: 1\n",
            "21:19:55 |     use_reply: label\n",
            "21:19:55 |     validation_cutoff: 1.0\n",
            "21:19:55 |     validation_every_n_epochs: -1\n",
            "21:19:55 |     validation_every_n_secs: -1\n",
            "21:19:55 |     validation_max_exs: -1\n",
            "21:19:55 |     validation_metric: accuracy\n",
            "21:19:55 |     validation_metric_mode: None\n",
            "21:19:55 |     validation_patience: 10\n",
            "21:19:55 |     validation_share_agent: False\n",
            "21:19:55 |     verbose: False\n",
            "21:19:55 |     wandb_log: False\n",
            "21:19:55 |     wandb_name: None\n",
            "21:19:55 |     wandb_project: None\n",
            "21:19:55 |     warmup_rate: 0.0001\n",
            "21:19:55 |     warmup_updates: -1\n",
            "21:19:55 |     weight_decay: None\n",
            "21:19:56 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train:ordered:stream\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:02<00:00, 23.1kex/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "21:19:59 | Saving dictionary to from_scratch_model/model.dict\n",
            "21:19:59 | dictionary built with 22419 tokens in 0.0s\n",
            "21:19:59 | No model with opt yet at: from_scratch_model/model(.opt)\n",
            "21:19:59 | Using CUDA\n",
            "21:19:59 | loading dictionary from from_scratch_model/model.dict\n",
            "21:19:59 | num words = 22419\n",
            "21:19:59 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "21:19:59 | Opt:\n",
            "21:19:59 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "21:19:59 |     adam_eps: 1e-08\n",
            "21:19:59 |     add_p1_after_newln: False\n",
            "21:19:59 |     aggregate_micro: False\n",
            "21:19:59 |     allow_missing_init_opts: False\n",
            "21:19:59 |     attention: dot\n",
            "21:19:59 |     attention_length: 48\n",
            "21:19:59 |     attention_time: post\n",
            "21:19:59 |     batchsize: 16\n",
            "21:19:59 |     beam_block_full_context: True\n",
            "21:19:59 |     beam_block_list_filename: None\n",
            "21:19:59 |     beam_block_ngram: -1\n",
            "21:19:59 |     beam_context_block_ngram: -1\n",
            "21:19:59 |     beam_delay: 30\n",
            "21:19:59 |     beam_length_penalty: 0.65\n",
            "21:19:59 |     beam_min_length: 1\n",
            "21:19:59 |     beam_size: 1\n",
            "21:19:59 |     betas: '(0.9, 0.999)'\n",
            "21:19:59 |     bidirectional: False\n",
            "21:19:59 |     bpe_add_prefix_space: None\n",
            "21:19:59 |     bpe_debug: False\n",
            "21:19:59 |     bpe_dropout: None\n",
            "21:19:59 |     bpe_merge: None\n",
            "21:19:59 |     bpe_vocab: None\n",
            "21:19:59 |     compute_tokenized_bleu: False\n",
            "21:19:59 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:19:59 |     datatype: train\n",
            "21:19:59 |     decoder: same\n",
            "21:19:59 |     delimiter: '\\n'\n",
            "21:19:59 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:19:59 |     dict_endtoken: __end__\n",
            "21:19:59 |     dict_file: from_scratch_model/model.dict\n",
            "21:19:59 |     dict_include_test: False\n",
            "21:19:59 |     dict_include_valid: False\n",
            "21:19:59 |     dict_initpath: None\n",
            "21:19:59 |     dict_language: english\n",
            "21:19:59 |     dict_loaded: True\n",
            "21:19:59 |     dict_lower: False\n",
            "21:19:59 |     dict_max_ngram_size: -1\n",
            "21:19:59 |     dict_maxexs: -1\n",
            "21:19:59 |     dict_maxtokens: -1\n",
            "21:19:59 |     dict_minfreq: 0\n",
            "21:19:59 |     dict_nulltoken: __null__\n",
            "21:19:59 |     dict_starttoken: __start__\n",
            "21:19:59 |     dict_textfields: text,labels\n",
            "21:19:59 |     dict_tokenizer: re\n",
            "21:19:59 |     dict_unktoken: __unk__\n",
            "21:19:59 |     display_examples: False\n",
            "21:19:59 |     download_path: None\n",
            "21:19:59 |     dropout: 0.1\n",
            "21:19:59 |     dynamic_batching: None\n",
            "21:19:59 |     embedding_projection: random\n",
            "21:19:59 |     embedding_type: random\n",
            "21:19:59 |     embeddingsize: 128\n",
            "21:19:59 |     eval_batchsize: None\n",
            "21:19:59 |     eval_dynamic_batching: None\n",
            "21:19:59 |     evaltask: None\n",
            "21:19:59 |     force_fp16_tokens: False\n",
            "21:19:59 |     fp16: False\n",
            "21:19:59 |     fp16_impl: safe\n",
            "21:19:59 |     gpu: -1\n",
            "21:19:59 |     gradient_clip: 0.1\n",
            "21:19:59 |     hiddensize: 128\n",
            "21:19:59 |     hide_labels: False\n",
            "21:19:59 |     history_add_global_end_token: None\n",
            "21:19:59 |     history_reversed: False\n",
            "21:19:59 |     history_size: -1\n",
            "21:19:59 |     image_cropsize: 224\n",
            "21:19:59 |     image_mode: raw\n",
            "21:19:59 |     image_size: 256\n",
            "21:19:59 |     inference: greedy\n",
            "21:19:59 |     init_model: None\n",
            "21:19:59 |     init_opt: None\n",
            "21:19:59 |     input_dropout: 0.0\n",
            "21:19:59 |     interactive_mode: False\n",
            "21:19:59 |     invsqrt_lr_decay_gamma: -1\n",
            "21:19:59 |     label_truncate: None\n",
            "21:19:59 |     learningrate: 1\n",
            "21:19:59 |     load_from_checkpoint: True\n",
            "21:19:59 |     log_every_n_secs: 10\n",
            "21:19:59 |     loglevel: info\n",
            "21:19:59 |     lookuptable: all\n",
            "21:19:59 |     lr_scheduler: reduceonplateau\n",
            "21:19:59 |     lr_scheduler_decay: 0.5\n",
            "21:19:59 |     lr_scheduler_patience: 3\n",
            "21:19:59 |     max_lr_steps: -1\n",
            "21:19:59 |     max_train_time: 120.0\n",
            "21:19:59 |     metrics: default\n",
            "21:19:59 |     model: seq2seq\n",
            "21:19:59 |     model_file: from_scratch_model/model\n",
            "21:19:59 |     momentum: 0\n",
            "21:19:59 |     multitask_weights: [1]\n",
            "21:19:59 |     mutators: None\n",
            "21:19:59 |     nesterov: True\n",
            "21:19:59 |     no_cuda: False\n",
            "21:19:59 |     num_epochs: -1\n",
            "21:19:59 |     numlayers: 2\n",
            "21:19:59 |     numsoftmax: 1\n",
            "21:19:59 |     nus: (0.7,)\n",
            "21:19:59 |     optimizer: sgd\n",
            "21:19:59 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "21:19:59 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:19:59 |     person_tokens: False\n",
            "21:19:59 |     rank_candidates: False\n",
            "21:19:59 |     remove_political_convos: False\n",
            "21:19:59 |     rnn_class: lstm\n",
            "21:19:59 |     save_after_valid: False\n",
            "21:19:59 |     save_every_n_secs: -1\n",
            "21:19:59 |     short_final_eval: False\n",
            "21:19:59 |     skip_generation: False\n",
            "21:19:59 |     special_tok_lst: None\n",
            "21:19:59 |     split_lines: False\n",
            "21:19:59 |     starttime: Apr16_21-19\n",
            "21:19:59 |     task: empathetic_dialogues\n",
            "21:19:59 |     temperature: 1.0\n",
            "21:19:59 |     tensorboard_log: False\n",
            "21:19:59 |     tensorboard_logdir: None\n",
            "21:19:59 |     text_truncate: None\n",
            "21:19:59 |     topk: 10\n",
            "21:19:59 |     topp: 0.9\n",
            "21:19:59 |     train_experiencer_only: False\n",
            "21:19:59 |     truncate: 64\n",
            "21:19:59 |     update_freq: 1\n",
            "21:19:59 |     use_reply: label\n",
            "21:19:59 |     validation_cutoff: 1.0\n",
            "21:19:59 |     validation_every_n_epochs: -1\n",
            "21:19:59 |     validation_every_n_secs: -1\n",
            "21:19:59 |     validation_max_exs: -1\n",
            "21:19:59 |     validation_metric: accuracy\n",
            "21:19:59 |     validation_metric_mode: None\n",
            "21:19:59 |     validation_patience: 10\n",
            "21:19:59 |     validation_share_agent: False\n",
            "21:19:59 |     verbose: False\n",
            "21:19:59 |     wandb_log: False\n",
            "21:19:59 |     wandb_name: None\n",
            "21:19:59 |     wandb_project: None\n",
            "21:19:59 |     warmup_rate: 0.0001\n",
            "21:19:59 |     warmup_updates: -1\n",
            "21:19:59 |     weight_decay: None\n",
            "21:20:00 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "21:20:00 | training...\n",
            "21:20:10 | time:10s total_exs:2432 epochs:0.04\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 458.4  6937 242.1 2432  1.009   .02616 9.008   1 256.2  3877 8172     .09417                  152 714.6 10814 15.13\n",
            "\n",
            "21:20:20 | time:20s total_exs:4960 epochs:0.08\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 444.7  6991 251.5 2528  1.093   .02616 8.364   1 257.6  4050 4289      .1293                  310 702.3 11040 15.72\n",
            "\n",
            "21:20:30 | time:30s total_exs:7408 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 447.4  6812 243.6 2448  1.152   .02616 7.967   1 256.8  3910 2883      .1478                  463 704.3 10722 15.22\n",
            "\n",
            "21:20:40 | time:40s total_exs:9808 epochs:0.15\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 457.2  6843 239.5 2400  1.189   .02617 7.736   1 262.1  3923 2290      .1529                  613 719.3 10766 14.97\n",
            "\n",
            "21:20:51 | time:50s total_exs:12272 epochs:0.19\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 452.6  6937 245.2 2464  1.232   .02617  7.51   1 258.7  3965 1827      .1636                  767 711.2 10902 15.33\n",
            "\n",
            "21:21:01 | time:60s total_exs:14752 epochs:0.23\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps  ups\n",
            "       1 454.2  7039 247.9 2480  1.238   .02617 7.344   1 262.7  4071 1547      .1683                  922 716.9 11110 15.5\n",
            "\n",
            "21:21:11 | time:70s total_exs:17168 epochs:0.27\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 448.7  6751 240.7 2416  1.264   .02617 7.181   1 262.8  3954 1315      .1753                 1073 711.5 10705 15.05\n",
            "\n",
            "21:21:21 | time:80s total_exs:19600 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 450.9  6837 242.6 2432  1.295   .02617  7.03   1 262.7  3982 1130      .1814                 1225 713.6 10819 15.16\n",
            "\n",
            "21:21:31 | time:90s total_exs:22064 epochs:0.34\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps  ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 453.1  6964 245.9 2464  1.318   .02617 6.946   1 257.5  3958 1039      .1835                 1379 710.6 10922 15.37\n",
            "\n",
            "21:21:41 | time:101s total_exs:24528 epochs:0.38\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates   tpb   tps   ups\n",
            "       1 455.7  6990 245.4 2464  1.322    .0261 6.869   1 261.9  4017 961.8      .1863                 1533 717.6 11007 15.34\n",
            "\n",
            "21:21:51 | time:111s total_exs:26960 epochs:0.42\n",
            "    clip  ctpb  ctps  exps  exs  gnorm  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps   ups\n",
            "       1 449.1  6811 242.6 2432   1.35   .02618 6.813   1 258.9  3926 909.3      .1854                 1685  708 10737 15.16\n",
            "\n",
            "21:22:00 | max_train_time elapsed:120.02368354797363s\n",
            "21:22:00 | Using CUDA\n",
            "21:22:00 | loading dictionary from from_scratch_model/model.dict\n",
            "21:22:00 | num words = 22419\n",
            "21:22:00 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "21:22:00 | Loading existing model params from from_scratch_model/model\n",
            "21:22:00 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "21:22:01 | running eval: valid\n",
            "21:22:36 | eval completed in 34.88s\n",
            "21:22:36 | \u001b[1mvalid:\n",
            "    accuracy    bleu-4  ctpb  ctps  exps  exs    f1  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  \\\n",
            "           0 8.027e-07 572.6  5913 164.6 5738 .1084 .0009844 6.406   1 249.1  2572 605.5      .2173                 1826   \n",
            "     tpb  tps  \n",
            "   821.7 8486\n",
            "\u001b[0m\n",
            "21:22:36 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "21:22:36 | running eval: test\n",
            "21:23:08 | eval completed in 32.26s\n",
            "21:23:08 | \u001b[1mtest:\n",
            "    accuracy    bleu-4  ctpb  ctps  exps  exs    f1  gpu_mem  loss  lr  ltpb  ltps   ppl  token_acc  total_train_updates  \\\n",
            "           0 9.994e-07 604.5  6168 163.1 5259 .1081 .0009528 6.425   1 252.6  2577 617.4      .2153                 1826   \n",
            "     tpb  tps  \n",
            "   857.1 8745\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(8.027e-07),\n",
              "  'ctpb': GlobalAverageMetric(572.6),\n",
              "  'ctps': GlobalTimerMetric(5913),\n",
              "  'exps': GlobalTimerMetric(164.6),\n",
              "  'exs': SumMetric(5738),\n",
              "  'f1': F1Metric(0.1084),\n",
              "  'gpu_mem': GlobalAverageMetric(0.0009844),\n",
              "  'loss': AverageMetric(6.406),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(249.1),\n",
              "  'ltps': GlobalTimerMetric(2572),\n",
              "  'ppl': PPLMetric(605.5),\n",
              "  'token_acc': AverageMetric(0.2173),\n",
              "  'total_train_updates': GlobalFixedMetric(1826),\n",
              "  'tpb': GlobalAverageMetric(821.7),\n",
              "  'tps': GlobalTimerMetric(8486)},\n",
              " {'accuracy': ExactMatchMetric(0),\n",
              "  'bleu-4': BleuMetric(9.994e-07),\n",
              "  'ctpb': GlobalAverageMetric(604.5),\n",
              "  'ctps': GlobalTimerMetric(6168),\n",
              "  'exps': GlobalTimerMetric(163.1),\n",
              "  'exs': SumMetric(5259),\n",
              "  'f1': F1Metric(0.1081),\n",
              "  'gpu_mem': GlobalAverageMetric(0.0009528),\n",
              "  'loss': AverageMetric(6.425),\n",
              "  'lr': GlobalAverageMetric(1),\n",
              "  'ltpb': GlobalAverageMetric(252.6),\n",
              "  'ltps': GlobalTimerMetric(2577),\n",
              "  'ppl': PPLMetric(617.4),\n",
              "  'token_acc': AverageMetric(0.2153),\n",
              "  'total_train_updates': GlobalFixedMetric(1826),\n",
              "  'tpb': GlobalAverageMetric(857.1),\n",
              "  'tps': GlobalTimerMetric(8745)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA77Zwkoviq"
      },
      "source": [
        "Our perplexity and F1 (word overlap) scores are pretty bad, and our BLEU-4 score is nearly 0. That's okay, we would normally want to train for well over an hour. Feel free to change the max_train_time above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QTiTn7aoxv9"
      },
      "source": [
        "## Performance is pretty bad there. Can we improve it?\n",
        "\n",
        "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Jt9bHTn1dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6082dcd0-3581-4526-f793-d032395cb6b9"
      },
      "source": [
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    # similar to before\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    max_train_time=600, validation_every_n_epochs=0.25,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21:24:01 | building dictionary first...\n",
            "21:24:01 | No model with opt yet at: from_pretrained/model(.opt)\n",
            "21:24:01 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,eval_dynamic_batching: None,load_from_checkpoint: True,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,mutators: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages\u001b[0m\n",
            "21:24:01 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "21:24:01 | Using CUDA\n",
            "21:24:01 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "21:24:01 | num words = 54944\n",
            "21:24:03 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "21:24:03 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "21:24:04 | \u001b[33mNot loading optim state since optim class changed.\u001b[0m\n",
            "21:24:04 | Opt:\n",
            "21:24:04 |     activation: gelu\n",
            "21:24:04 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "21:24:04 |     adam_eps: 1e-08\n",
            "21:24:04 |     add_p1_after_newln: False\n",
            "21:24:04 |     aggregate_micro: False\n",
            "21:24:04 |     allow_missing_init_opts: False\n",
            "21:24:04 |     attention_dropout: 0.0\n",
            "21:24:04 |     batchsize: 12\n",
            "21:24:04 |     beam_block_full_context: True\n",
            "21:24:04 |     beam_block_list_filename: None\n",
            "21:24:04 |     beam_block_ngram: -1\n",
            "21:24:04 |     beam_context_block_ngram: -1\n",
            "21:24:04 |     beam_delay: 30\n",
            "21:24:04 |     beam_length_penalty: 0.65\n",
            "21:24:04 |     beam_min_length: 1\n",
            "21:24:04 |     beam_size: 1\n",
            "21:24:04 |     betas: '(0.9, 0.999)'\n",
            "21:24:04 |     bpe_add_prefix_space: None\n",
            "21:24:04 |     bpe_debug: False\n",
            "21:24:04 |     bpe_dropout: None\n",
            "21:24:04 |     bpe_merge: None\n",
            "21:24:04 |     bpe_vocab: None\n",
            "21:24:04 |     compute_tokenized_bleu: False\n",
            "21:24:04 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:24:04 |     datatype: train\n",
            "21:24:04 |     delimiter: '\\n'\n",
            "21:24:04 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:24:04 |     dict_endtoken: __end__\n",
            "21:24:04 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "21:24:04 |     dict_include_test: False\n",
            "21:24:04 |     dict_include_valid: False\n",
            "21:24:04 |     dict_initpath: None\n",
            "21:24:04 |     dict_language: english\n",
            "21:24:04 |     dict_loaded: True\n",
            "21:24:04 |     dict_lower: True\n",
            "21:24:04 |     dict_max_ngram_size: -1\n",
            "21:24:04 |     dict_maxexs: -1\n",
            "21:24:04 |     dict_maxtokens: -1\n",
            "21:24:04 |     dict_minfreq: 0\n",
            "21:24:04 |     dict_nulltoken: __null__\n",
            "21:24:04 |     dict_starttoken: __start__\n",
            "21:24:04 |     dict_textfields: text,labels\n",
            "21:24:04 |     dict_tokenizer: bpe\n",
            "21:24:04 |     dict_unktoken: __unk__\n",
            "21:24:04 |     display_examples: False\n",
            "21:24:04 |     download_path: None\n",
            "21:24:04 |     dropout: 0.0\n",
            "21:24:04 |     dynamic_batching: full\n",
            "21:24:04 |     embedding_projection: random\n",
            "21:24:04 |     embedding_size: 512\n",
            "21:24:04 |     embedding_type: random\n",
            "21:24:04 |     embeddings_scale: True\n",
            "21:24:04 |     eval_batchsize: None\n",
            "21:24:04 |     eval_dynamic_batching: None\n",
            "21:24:04 |     evaltask: None\n",
            "21:24:04 |     ffn_size: 2048\n",
            "21:24:04 |     force_fp16_tokens: False\n",
            "21:24:04 |     fp16: True\n",
            "21:24:04 |     fp16_impl: mem_efficient\n",
            "21:24:04 |     gpu: -1\n",
            "21:24:04 |     gradient_clip: 0.1\n",
            "21:24:04 |     hide_labels: False\n",
            "21:24:04 |     history_add_global_end_token: None\n",
            "21:24:04 |     history_reversed: False\n",
            "21:24:04 |     history_size: -1\n",
            "21:24:04 |     image_cropsize: 224\n",
            "21:24:04 |     image_mode: raw\n",
            "21:24:04 |     image_size: 256\n",
            "21:24:04 |     inference: greedy\n",
            "21:24:04 |     init_model: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "21:24:04 |     init_opt: None\n",
            "21:24:04 |     interactive_mode: False\n",
            "21:24:04 |     invsqrt_lr_decay_gamma: -1\n",
            "21:24:04 |     label_truncate: 128\n",
            "21:24:04 |     learn_positional_embeddings: True\n",
            "21:24:04 |     learningrate: 1e-05\n",
            "21:24:04 |     load_from_checkpoint: True\n",
            "21:24:04 |     log_every_n_secs: 10\n",
            "21:24:04 |     loglevel: info\n",
            "21:24:04 |     lr_scheduler: reduceonplateau\n",
            "21:24:04 |     lr_scheduler_decay: 0.5\n",
            "21:24:04 |     lr_scheduler_patience: 3\n",
            "21:24:04 |     max_lr_steps: -1\n",
            "21:24:04 |     max_train_time: 600.0\n",
            "21:24:04 |     metrics: default\n",
            "21:24:04 |     model: transformer/generator\n",
            "21:24:04 |     model_file: from_pretrained/model\n",
            "21:24:04 |     model_parallel: False\n",
            "21:24:04 |     momentum: 0\n",
            "21:24:04 |     multitask_weights: [1]\n",
            "21:24:04 |     mutators: None\n",
            "21:24:04 |     n_decoder_layers: -1\n",
            "21:24:04 |     n_encoder_layers: -1\n",
            "21:24:04 |     n_heads: 16\n",
            "21:24:04 |     n_layers: 8\n",
            "21:24:04 |     n_positions: 512\n",
            "21:24:04 |     n_segments: 0\n",
            "21:24:04 |     nesterov: True\n",
            "21:24:04 |     no_cuda: False\n",
            "21:24:04 |     num_epochs: -1\n",
            "21:24:04 |     nus: (0.7,)\n",
            "21:24:04 |     optimizer: mem_eff_adam\n",
            "21:24:04 |     output_scaling: 1.0\n",
            "21:24:04 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "21:24:04 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:24:04 |     person_tokens: False\n",
            "21:24:04 |     rank_candidates: False\n",
            "21:24:04 |     relu_dropout: 0.0\n",
            "21:24:04 |     remove_political_convos: False\n",
            "21:24:04 |     save_after_valid: False\n",
            "21:24:04 |     save_every_n_secs: -1\n",
            "21:24:04 |     share_word_embeddings: True\n",
            "21:24:04 |     short_final_eval: False\n",
            "21:24:04 |     skip_generation: True\n",
            "21:24:04 |     special_tok_lst: None\n",
            "21:24:04 |     split_lines: False\n",
            "21:24:04 |     starttime: Apr16_21-24\n",
            "21:24:04 |     task: empathetic_dialogues\n",
            "21:24:04 |     temperature: 1.0\n",
            "21:24:04 |     tensorboard_log: False\n",
            "21:24:04 |     tensorboard_logdir: None\n",
            "21:24:04 |     text_truncate: 512\n",
            "21:24:04 |     topk: 10\n",
            "21:24:04 |     topp: 0.9\n",
            "21:24:04 |     train_experiencer_only: False\n",
            "21:24:04 |     truncate: -1\n",
            "21:24:04 |     update_freq: 1\n",
            "21:24:04 |     use_reply: label\n",
            "21:24:04 |     validation_cutoff: 1.0\n",
            "21:24:04 |     validation_every_n_epochs: 0.25\n",
            "21:24:04 |     validation_every_n_secs: -1\n",
            "21:24:04 |     validation_max_exs: -1\n",
            "21:24:04 |     validation_metric: ppl\n",
            "21:24:04 |     validation_metric_mode: None\n",
            "21:24:04 |     validation_patience: 10\n",
            "21:24:04 |     validation_share_agent: False\n",
            "21:24:04 |     variant: xlm\n",
            "21:24:04 |     verbose: False\n",
            "21:24:04 |     wandb_log: False\n",
            "21:24:04 |     wandb_name: None\n",
            "21:24:04 |     wandb_project: None\n",
            "21:24:04 |     warmup_rate: 0.0001\n",
            "21:24:04 |     warmup_updates: 100\n",
            "21:24:04 |     weight_decay: None\n",
            "21:24:04 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? False, datatype: train\n",
            "21:24:10 | training...\n",
            "21:24:12 | Overflow: setting loss scale to 65536.0\n",
            "21:24:20 | time:10s total_exs:3412 epochs:0.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss        lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9697  2545  8386 340.6 3412             75466  4.284    .5316  2.86 3.301e-06  1654  5448 17.47      .3938   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     33 4199 13834 3.295\n",
            "\n",
            "21:24:28 | Overflow: setting loss scale to 32768.0\n",
            "21:24:30 | time:20s total_exs:6828 epochs:0.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss      lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9722  3092 11125 341.4 3416             59164  4.125    .4922 2.786 6.9e-06  1561  5617 16.22      .4022   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                     69 4653 16742 3.598\n",
            "\n",
            "21:24:40 | time:30s total_exs:10188 epochs:0.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2826  9580   335 3360             32768   4.44    .5339 2.809 1e-05  1672  5669 16.59      .3988   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    103 4498 15249 3.39\n",
            "\n",
            "21:24:50 | time:41s total_exs:13404 epochs:0.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2998 10403 318.8 3216             32768  4.333    .5183  2.72 1e-05  1494  5183 15.18      .4120   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    138 4491 15586 3.47\n",
            "\n",
            "21:24:59 | time:50s total_exs:16200 epochs:0.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2850  9473 309.8 2796             32768  3.946    .5224 2.738 1e-05  1644  5464 15.45      .4087   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    168 4494 14937 3.324\n",
            "\n",
            "21:24:59 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "21:25:01 | running eval: valid\n",
            "21:25:02 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n",
            "21:25:08 | eval completed in 6.70s\n",
            "21:25:08 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 36115 892.8 5738   .08897 2.534 1e-05  1351 14294 12.61      .4314                  168 4764 50410\n",
            "\u001b[0m\n",
            "21:25:08 | \u001b[1;32mnew best ppl: 12.61\u001b[0m\n",
            "21:25:08 | saving best valid model: from_pretrained/model\n",
            "21:25:08 | Saving dictionary to from_pretrained/model.dict\n",
            "21:25:23 | time:73s total_exs:19592 epochs:0.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2976 10034 336.3 3392             32768   4.06    .5316 2.658 1e-05  1579  5322 14.27      .4146   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    202 4555 15357 3.372\n",
            "\n",
            "21:25:33 | time:83s total_exs:22768 epochs:0.35\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2976 10485 310.8 3176             32768  4.327    .5237 2.661 1e-05  1482  5222 14.3      .4181   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    238 4458 15707 3.524\n",
            "\n",
            "21:25:43 | time:93s total_exs:25964 epochs:0.40\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2905 10074 316.6 3196             32768  4.219    .5237 2.732 1e-05  1547  5364 15.36      .4058   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    273 4452 15438 3.468\n",
            "\n",
            "21:25:53 | time:104s total_exs:29148 epochs:0.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2878  9914 313.4 3184             32768  4.244    .5316 2.663 1e-05  1513  5212 14.34      .4137   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    308 4390 15126 3.445\n",
            "\n",
            "21:26:03 | time:113s total_exs:32364 epochs:0.50\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2949  9778 333.3 3216             32768  4.363    .5157 2.671 1e-05  1641  5442 14.46      .4155   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    340 4590 15220 3.316\n",
            "\n",
            "21:26:03 | running eval: valid\n",
            "21:26:09 | eval completed in 6.28s\n",
            "21:26:09 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3571 38576 953.6 5738   .08899 2.503 1e-05  1413 15268 12.22      .4365                  340 4984 53845\n",
            "\u001b[0m\n",
            "21:26:09 | \u001b[1;32mnew best ppl: 12.22 (previous best was 12.61)\u001b[0m\n",
            "21:26:09 | saving best valid model: from_pretrained/model\n",
            "21:26:23 | time:134s total_exs:35580 epochs:0.55\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3068 10013 318.1 3216             32768  4.117    .5097  2.66 1e-05  1646  5372 14.3      .4174   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    373 4713 15385 3.264\n",
            "\n",
            "21:26:33 | time:144s total_exs:39004 epochs:0.60\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2789 10249   340 3424             32768   4.27    .5183 2.672 1e-05  1535  5640 14.46      .4155   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    410 4324 15889 3.675\n",
            "\n",
            "21:26:44 | time:154s total_exs:42600 epochs:0.66\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2850  9998 360.4 3596             32768  3.951    .5237 2.667 1e-05  1670  5857 14.39      .4161   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    445 4520 15855 3.508\n",
            "\n",
            "21:26:54 | time:164s total_exs:45552 epochs:0.70\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2859 10312 295.8 2952             32768  4.431    .5317 2.696 1e-05  1436  5181 14.82      .4132   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    481 4295 15494 3.608\n",
            "\n",
            "21:27:03 | time:173s total_exs:48612 epochs:0.75\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3068 10848 327.9 3060             32768  4.473    .5097 2.638 1e-05  1539  5443 13.98      .4180   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    514 4607 16292 3.537\n",
            "\n",
            "21:27:03 | running eval: valid\n",
            "21:27:09 | eval completed in 6.38s\n",
            "21:27:09 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3571 38978 963.6 5738   .08898 2.488 1e-05  1413 15428 12.03      .4386                  514 4984 54406\n",
            "\u001b[0m\n",
            "21:27:09 | \u001b[1;32mnew best ppl: 12.03 (previous best was 12.22)\u001b[0m\n",
            "21:27:09 | saving best valid model: from_pretrained/model\n",
            "21:27:23 | time:194s total_exs:51816 epochs:0.80\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3034 10628 320.6 3204             32768  4.251    .5224 2.631 1e-05  1520  5324 13.89      .4198   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    549 4554 15952 3.503\n",
            "\n",
            "21:27:34 | time:204s total_exs:54944 epochs:0.85\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2844  9555 309.1 3128             32768   4.29    .4946 2.693 1e-05  1618  5435 14.78      .4135   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    583 4462 14990 3.36\n",
            "\n",
            "21:27:35 | Overflow: setting loss scale to 16384.0\n",
            "21:27:44 | time:214s total_exs:58148 epochs:0.90\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "   .9706  2911  9755 315.8 3204             17830   3.89    .5220 2.643 1e-05  1528  5120 14.06      .4212   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    617 4439 14875 3.351\n",
            "\n",
            "21:27:54 | time:224s total_exs:61400 epochs:0.95\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2804  9889 318.6 3252             16384  4.203    .5157 2.661 1e-05  1543  5441 14.31      .4135   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    653 4347 15331 3.527\n",
            "\n",
            "21:28:04 | time:234s total_exs:64496 epochs:1.00\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2966 10077 309.3 3096             16384  4.432    .5183 2.654 1e-05  1567  5324 14.22      .4190   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    687 4534 15402 3.398\n",
            "\n",
            "21:28:05 | time:235s total_exs:64776 epochs:1.00\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  4003 13538 315.5  280             16384    4.4    .4614 2.567 1e-05  1280  4327 13.02      .4230   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    690 5283 17866 3.385\n",
            "\n",
            "21:28:05 | running eval: valid\n",
            "21:28:12 | eval completed in 6.85s\n",
            "21:28:12 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 34995 865.1 5738   .08899 2.478 1e-05  1371 13851 11.91      .4404                  690 4835 48846\n",
            "\u001b[0m\n",
            "21:28:12 | \u001b[1;32mnew best ppl: 11.91 (previous best was 12.03)\u001b[0m\n",
            "21:28:12 | saving best valid model: from_pretrained/model\n",
            "21:28:26 | time:257s total_exs:68164 epochs:1.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2999 10560 331.4 3388             16384  4.186    .4946  2.64 1e-05  1569  5526 14.01      .4202   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    726 4568 16087 3.522\n",
            "\n",
            "21:28:36 | time:267s total_exs:71696 epochs:1.11\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2969 10277 349.3 3532             16384  3.913    .5012 2.647 1e-05  1667  5771 14.11      .4167   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    761 4637 16048 3.461\n",
            "\n",
            "21:28:47 | time:277s total_exs:75100 epochs:1.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3069 10542   334 3404             16384  4.277    .5237 2.613 1e-05  1565  5375 13.64      .4228   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    796 4634 15917 3.435\n",
            "\n",
            "21:28:57 | time:287s total_exs:78384 epochs:1.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2838  9836 325.1 3284             16384  4.164    .5097 2.645 1e-05  1587  5500 14.08      .4172   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    831 4426 15337 3.466\n",
            "\n",
            "21:29:05 | time:295s total_exs:80968 epochs:1.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  2913 10147 321.5 2584             16384  4.131    .5097 2.595 1e-05  1552  5407 13.4      .4251   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    859 4465 15555 3.484\n",
            "\n",
            "21:29:05 | running eval: valid\n",
            "21:29:12 | eval completed in 6.68s\n",
            "21:29:12 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 35911 887.7 5738   .08897 2.469 1e-05  1351 14214 11.81      .4415                  859 4764 50124\n",
            "\u001b[0m\n",
            "21:29:12 | \u001b[1;32mnew best ppl: 11.81 (previous best was 11.91)\u001b[0m\n",
            "21:29:12 | saving best valid model: from_pretrained/model\n",
            "21:29:26 | time:316s total_exs:84320 epochs:1.30\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3006 10776 333.8 3352             16384  4.423    .5237 2.617 1e-05  1541  5524 13.7      .4219   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    895 4547 16300 3.585\n",
            "\n",
            "21:29:36 | time:326s total_exs:87468 epochs:1.35\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2617  9255 309.3 3148             16384  4.308    .5183  2.64 1e-05  1528  5406 14.02      .4194   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    931 4145 14661 3.537\n",
            "\n",
            "21:29:46 | time:336s total_exs:90784 epochs:1.40\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2861  9440 331.6 3316             16384  3.909    .5097 2.589 1e-05  1623  5354 13.31      .4268   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                    964 4484 14795  3.3\n",
            "\n",
            "21:29:56 | time:347s total_exs:93944 epochs:1.45\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3257 10849 309.5 3160             16384   4.19    .4946 2.624 1e-05  1553  5173 13.79      .4209   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                    998 4810 16022 3.331\n",
            "\n",
            "21:30:07 | time:357s total_exs:97128 epochs:1.50\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2737  9456 314.3 3184             16384  4.159    .5017 2.598 1e-05  1501  5184 13.44      .4253   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1033 4238 14640 3.455\n",
            "\n",
            "21:30:07 | time:357s total_exs:97128 epochs:1.50\n",
            "    gpu_mem    lr  total_train_updates\n",
            "     .08894 1e-05                 1033\n",
            "\n",
            "21:30:07 | running eval: valid\n",
            "21:30:13 | eval completed in 6.45s\n",
            "21:30:13 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 37529 927.6 5738   .08899 2.462 1e-05  1392 14854 11.73      .4421                 1033 4909 52384\n",
            "\u001b[0m\n",
            "21:30:13 | \u001b[1;32mnew best ppl: 11.73 (previous best was 11.81)\u001b[0m\n",
            "21:30:13 | saving best valid model: from_pretrained/model\n",
            "21:30:27 | time:378s total_exs:100556 epochs:1.56\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3046 10677 343.3 3428             16384  4.144    .4946 2.614 1e-05  1616  5663 13.65      .4245   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1068 4662 16340 3.505\n",
            "\n",
            "21:30:38 | time:388s total_exs:103908 epochs:1.61\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3017 10580 326.5 3352             16384  4.237    .4967  2.56 1e-05  1568  5498 12.94      .4290   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1104 4585 16078 3.507\n",
            "\n",
            "21:30:48 | time:398s total_exs:106980 epochs:1.66\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3061 10406 307.2 3072             16384  4.255    .4947 2.601 1e-05  1525  5186 13.47      .4232   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1138 4586 15593  3.4\n",
            "\n",
            "21:30:58 | time:408s total_exs:110080 epochs:1.70\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3051 10679   310 3100             16384  4.174    .5097 2.586 1e-05  1488  5208 13.28      .4238   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1173 4539 15887 3.501\n",
            "\n",
            "21:31:07 | time:417s total_exs:113328 epochs:1.75\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2973  9929 361.6 3248             16384   3.81    .4922 2.585 1e-05  1803  6022 13.26      .4252   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1203 4776 15951 3.34\n",
            "\n",
            "21:31:07 | running eval: valid\n",
            "21:31:13 | eval completed in 6.36s\n",
            "21:31:13 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3627 37958 938.3 5738   .08896 2.457 1e-05  1435 15024 11.67      .4430                 1203 5062 52982\n",
            "\u001b[0m\n",
            "21:31:13 | \u001b[1;32mnew best ppl: 11.67 (previous best was 11.73)\u001b[0m\n",
            "21:31:13 | saving best valid model: from_pretrained/model\n",
            "21:31:28 | time:438s total_exs:116608 epochs:1.80\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2891 10088   327 3280             16384   4.83    .5053 2.583 1e-05  1544  5388 13.23      .4275   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1238 4435 15477 3.49\n",
            "\n",
            "21:31:38 | time:448s total_exs:119608 epochs:1.85\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3191 10811 298.9 3000             16384  4.218    .5053 2.619 1e-05  1617  5479 13.72      .4204   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1272 4808 16290 3.388\n",
            "\n",
            "21:31:48 | time:458s total_exs:123152 epochs:1.91\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3138 10942   353 3544             16384  4.099    .4980 2.572 1e-05  1637  5706 13.09      .4270   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1307 4775 16648 3.487\n",
            "\n",
            "21:31:58 | time:468s total_exs:126636 epochs:1.96\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2722  9240 347.9 3484             16384  3.896    .4946 2.601 1e-05  1695  5753 13.48      .4244   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1341 4416 14994 3.395\n",
            "\n",
            "21:32:07 | time:477s total_exs:129488 epochs:2.00\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3130 11141 317.3 2852             16384  4.209    .5012 2.562 1e-05  1487  5294 12.97      .4301   \n",
            "    total_train_updates  tpb   tps  ups  \n",
            "                   1373 4617 16436 3.56\n",
            "\n",
            "21:32:07 | running eval: valid\n",
            "21:32:13 | eval completed in 6.69s\n",
            "21:32:13 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3413 37239 920.6 5738   .08898 2.453 1e-05  1351 14739 11.62      .4435                 1373 4764 51978\n",
            "\u001b[0m\n",
            "21:32:13 | \u001b[1;32mnew best ppl: 11.62 (previous best was 11.67)\u001b[0m\n",
            "21:32:13 | saving best valid model: from_pretrained/model\n",
            "21:32:28 | time:498s total_exs:132748 epochs:2.05\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2891  9788 324.6 3260             16384  4.554    .5097 2.583 1e-05  1602  5423 13.24      .4278   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1407 4493 15211 3.385\n",
            "\n",
            "21:32:38 | time:508s total_exs:135836 epochs:2.10\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3290 11820 308.1 3088             16384  4.345    .5053 2.569 1e-05  1506  5412 13.05      .4263   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1443 4797 17232 3.593\n",
            "\n",
            "21:32:48 | time:519s total_exs:139348 epochs:2.16\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2769  9403 340.8 3512             16384  4.594    .5237 2.582 1e-05  1692  5745 13.22      .4248   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1478 4461 15149 3.396\n",
            "\n",
            "21:32:58 | time:529s total_exs:142672 epochs:2.21\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2864  9730 332.1 3324             16384   4.02    .5183 2.556 1e-05  1643  5580 12.88      .4319   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1512 4507 15310 3.397\n",
            "\n",
            "21:33:08 | time:538s total_exs:145700 epochs:2.25\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3194 10909 323.2 3028             16384  4.009    .5228 2.566 1e-05  1548  5288 13.02      .4276   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1544 4742 16197 3.416\n",
            "\n",
            "21:33:08 | running eval: valid\n",
            "21:33:14 | eval completed in 6.61s\n",
            "21:33:14 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 36485 901.9 5738   .08897 2.449 1e-05  1392 14441 11.57      .4445                 1544 4909 50926\n",
            "\u001b[0m\n",
            "21:33:14 | \u001b[1;32mnew best ppl: 11.57 (previous best was 11.62)\u001b[0m\n",
            "21:33:14 | saving best valid model: from_pretrained/model\n",
            "21:33:28 | time:559s total_exs:149044 epochs:2.31\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps  ppl  token_acc  \\\n",
            "       1  3089 11130 334.7 3344             16384  4.439    .4946 2.573 1e-05  1532  5520 13.1      .4275   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1580 4621 16650 3.603\n",
            "\n",
            "21:33:39 | time:569s total_exs:152524 epochs:2.36\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  2901 10311 343.5 3480             16384  4.426    .5224 2.564 1e-05  1581  5618 12.98      .4294   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1616 4482 15929 3.554\n",
            "\n",
            "21:33:49 | time:579s total_exs:155664 epochs:2.41\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3104 10373 308.6 3140             16384  4.192    .5157 2.605 1e-05  1645  5498 13.53      .4223   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1650 4749 15870 3.342\n",
            "\n",
            "21:33:59 | time:589s total_exs:159032 epochs:2.46\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3092 11049 334.3 3368             16384  4.343    .4979 2.544 1e-05  1503  5370 12.73      .4307   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1686 4595 16419 3.573\n",
            "\n",
            "21:34:08 | time:598s total_exs:161964 epochs:2.51\n",
            "    clip  ctpb  ctps  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  \\\n",
            "       1  3041 10372 333.3 2932             16384  4.077    .5157 2.584 1e-05  1653  5640 13.25      .4249   \n",
            "    total_train_updates  tpb   tps   ups  \n",
            "                   1716 4694 16011 3.411\n",
            "\n",
            "21:34:08 | running eval: valid\n",
            "21:34:14 | eval completed in 6.34s\n",
            "21:34:14 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3517 38054 940.7 5738   .08897 2.446 1e-05  1392 15062 11.54      .4445                 1716 4909 53116\n",
            "\u001b[0m\n",
            "21:34:14 | \u001b[1;32mnew best ppl: 11.54 (previous best was 11.57)\u001b[0m\n",
            "21:34:14 | saving best valid model: from_pretrained/model\n",
            "21:34:19 | max_train_time elapsed:609.3528401851654s\n",
            "21:34:19 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "21:34:19 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "21:34:19 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,eval_dynamic_batching: None,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,mutators: None,train_experiencer_only: False,remove_political_convos: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,max_lr_steps: -1,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.7/dist-packages,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.7/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "21:34:19 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "21:34:19 | Using CUDA\n",
            "21:34:19 | loading dictionary from from_pretrained/model.dict\n",
            "21:34:19 | num words = 54944\n",
            "21:34:21 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "21:34:21 | Loading existing model params from from_pretrained/model\n",
            "21:34:24 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "21:34:26 | running eval: valid\n",
            "21:34:32 | eval completed in 6.78s\n",
            "21:34:32 | \u001b[1mvalid:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3464 35914 887.8 5738   .07747 2.446 1e-05  1371 14215 11.54      .4445                 1716 4835 50129\n",
            "\u001b[0m\n",
            "21:34:32 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: test\n",
            "21:34:35 | running eval: test\n",
            "21:34:42 | eval completed in 6.64s\n",
            "21:34:42 | \u001b[1mtest:\n",
            "    ctpb  ctps  exps  exs  gpu_mem  loss    lr  ltpb  ltps   ppl  token_acc  total_train_updates  tpb   tps\n",
            "    3535 36888 844.3 5259   .07745 2.469 1e-05  1313 13704 11.81      .4422                 1716 4848 50593\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'ctpb': GlobalAverageMetric(3464),\n",
              "  'ctps': GlobalTimerMetric(3.591e+04),\n",
              "  'exps': GlobalTimerMetric(887.8),\n",
              "  'exs': SumMetric(5738),\n",
              "  'gpu_mem': GlobalAverageMetric(0.07747),\n",
              "  'loss': AverageMetric(2.446),\n",
              "  'lr': GlobalAverageMetric(1e-05),\n",
              "  'ltpb': GlobalAverageMetric(1371),\n",
              "  'ltps': GlobalTimerMetric(1.421e+04),\n",
              "  'ppl': PPLMetric(11.54),\n",
              "  'token_acc': AverageMetric(0.4445),\n",
              "  'total_train_updates': GlobalFixedMetric(1716),\n",
              "  'tpb': GlobalAverageMetric(4835),\n",
              "  'tps': GlobalTimerMetric(5.013e+04)},\n",
              " {'ctpb': GlobalAverageMetric(3535),\n",
              "  'ctps': GlobalTimerMetric(3.689e+04),\n",
              "  'exps': GlobalTimerMetric(844.3),\n",
              "  'exs': SumMetric(5259),\n",
              "  'gpu_mem': GlobalAverageMetric(0.07745),\n",
              "  'loss': AverageMetric(2.469),\n",
              "  'lr': GlobalAverageMetric(1e-05),\n",
              "  'ltpb': GlobalAverageMetric(1313),\n",
              "  'ltps': GlobalTimerMetric(1.37e+04),\n",
              "  'ppl': PPLMetric(11.81),\n",
              "  'token_acc': AverageMetric(0.4422),\n",
              "  'total_train_updates': GlobalFixedMetric(1716),\n",
              "  'tpb': GlobalAverageMetric(4848),\n",
              "  'tps': GlobalTimerMetric(5.059e+04)})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iBZXTLRvIjb"
      },
      "source": [
        "## Wow that's a lot of options? Where do I find more info?\n",
        "\n",
        "As you might have noticed, there are a LOT of options to ParlAI. You're best reading the [ParlAI docs](https://parl.ai/docs) to find a list of hyperparameters. We provide lists of the command-line args for both models\n",
        "\n",
        "You can get some guidance in this notebook by using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pl8VVl5plfm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "453c1f82-a7ff-4df7-c7ad-f728c6e67854"
      },
      "source": [
        "# note that if you want to see model-specific arguments, you must specify a model name\n",
        "print(TrainModel.help(model='seq2seq'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: TrainModel [-h] [-o INIT_OPT] [-v] [-t TASK]\n",
            "                  [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
            "                  [-nt NUMTHREADS] [-bs BATCHSIZE] [-dynb {None,batchsort,full}]\n",
            "                  [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL]\n",
            "                  [-et EVALTASK] [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME]\n",
            "                  [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
            "                  [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS]\n",
            "                  [-vp VALIDATION_PATIENCE] [-vmt VALIDATION_METRIC]\n",
            "                  [-vmm {max,min}] [-mcs METRICS] [-micro AGGREGATE_MICRO]\n",
            "                  [-tblog TENSORBOARD_LOG] [-hs HIDDENSIZE] [-esz EMBEDDINGSIZE]\n",
            "                  [-nl NUMLAYERS] [-dr DROPOUT] [-bi BIDIRECTIONAL]\n",
            "                  [-att {none,concat,general,dot,local}]\n",
            "                  [-attl ATTENTION_LENGTH] [--attention-time {pre,post}]\n",
            "                  [-rnn {rnn,gru,lstm}] [-dec {same,shared}]\n",
            "                  [-lt {unique,enc_dec,dec_out,all}] [-soft NUMSOFTMAX]\n",
            "                  [-idr INPUT_DROPOUT] [--beam-size BEAM_SIZE]\n",
            "                  [--beam-min-length BEAM_MIN_LENGTH]\n",
            "                  [--beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM]\n",
            "                  [--beam-block-ngram BEAM_BLOCK_NGRAM]\n",
            "                  [--beam-length-penalty BEAM_LENGTH_PENALTY]\n",
            "                  [--inference {topk,beam,nucleus,delayedbeam,greedy}]\n",
            "                  [--topk TOPK] [--topp TOPP] [--beam-delay BEAM_DELAY]\n",
            "                  [--temperature TEMPERATURE]\n",
            "                  [--compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU]\n",
            "                  [-i INTERACTIVE_MODE]\n",
            "                  [-emb {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}]\n",
            "                  [-embp EMBEDDING_PROJECTION] [--fp16 FP16]\n",
            "                  [--fp16-impl {apex,mem_efficient}]\n",
            "                  [-opt {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}]\n",
            "                  [-lr LEARNINGRATE] [-clip GRADIENT_CLIP]\n",
            "                  [--adafactor-eps ADAFACTOR_EPS] [-mom MOMENTUM]\n",
            "                  [--nesterov NESTEROV] [-nu NUS] [-beta BETAS]\n",
            "                  [-wdecay WEIGHT_DECAY] [-rc RANK_CANDIDATES] [-tr TRUNCATE]\n",
            "                  [--text-truncate TEXT_TRUNCATE]\n",
            "                  [--label-truncate LABEL_TRUNCATE] [-histsz HISTORY_SIZE]\n",
            "                  [-pt PERSON_TOKENS] [--split-lines SPLIT_LINES]\n",
            "                  [--delimiter DELIMITER] [-gpu GPU | --no-cuda]\n",
            "                  [--bpe-vocab BPE_VOCAB] [--bpe-merge BPE_MERGE]\n",
            "                  [--lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}]\n",
            "                  [--lr-scheduler-patience LR_SCHEDULER_PATIENCE]\n",
            "                  [--lr-scheduler-decay LR_SCHEDULER_DECAY]\n",
            "                  [--max-lr-steps MAX_LR_STEPS]\n",
            "                  [--invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA]\n",
            "\n",
            "Train a model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "      show this help message and exit\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -o, --init-opt INIT_OPT\n",
            "      Path to json file of options. Note: Further Command-line arguments\n",
            "      override file-based options. (default: None)\n",
            "  -v, --show-advanced-args\n",
            "      Show hidden command line options (advanced users only) (default: False)\n",
            "  -t, --task TASK\n",
            "      ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
            "      choose from: train, train:ordered, valid, test. to stream data add\n",
            "      \":stream\" to any option (e.g., train:stream). by default: train is random\n",
            "      with replacement, valid is ordered, test is ordered. (default: train)\n",
            "  -nt, --numthreads NUMTHREADS\n",
            "      number of threads. Used for hogwild if batchsize is 1, else for number of\n",
            "      threads in threadpool loading, (default: 1)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "      batch size for minibatch training schemes (default: 1)\n",
            "  -dynb, --dynamic-batching {None,batchsort,full}\n",
            "      Use dynamic batching (default: None)\n",
            "  -dp, --datapath DATAPATH\n",
            "      path to datasets, defaults to {parlai_dir}/data (default: None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "      the model class name. can match parlai/agents/<model> for agents in that\n",
            "      directory, or can provide a fully specified module for `from X import Y`\n",
            "      via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)\n",
            "      (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "      model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "      load model weights and dict from this file (default: None)\n",
            "\n",
            "Training Loop Arguments:\n",
            "  -et, --evaltask EVALTASK\n",
            "      task to use for valid/test (defaults to the one used for training)\n",
            "      (default: None)\n",
            "  -eps, --num-epochs NUM_EPOCHS\n",
            "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
            "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
            "      Validate every n seconds. Saves model to model_file (if set) whenever best\n",
            "      val metric is found (default: -1)\n",
            "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
            "      Saves the model to model_file.checkpoint after every n seconds (default\n",
            "      -1, never). (default: -1)\n",
            "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
            "      Saves the model to model_file.checkpoint after every validation (default\n",
            "      False).\n",
            "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
            "      Validate every n epochs. Saves model to model_file (if set) whenever best\n",
            "      val metric is found (default: -1)\n",
            "  -vp, --validation-patience VALIDATION_PATIENCE\n",
            "      number of iterations of validation where result does not improve before we\n",
            "      stop training (default: 10)\n",
            "  -vmt, --validation-metric VALIDATION_METRIC\n",
            "      key into report table for selecting best validation (default: accuracy)\n",
            "  -vmm, --validation-metric-mode {max,min}\n",
            "      how to optimize validation metric (max or min) (default: None)\n",
            "  -mcs, --metrics METRICS\n",
            "      list of metrics to show/compute, e.g. all, default,or give a list split by\n",
            "      , like ppl,f1,accuracy,hits@1,rouge,bleuthe rouge metrics will be computed\n",
            "      as rouge-1, rouge-2 and rouge-l (default: default)\n",
            "  -micro, --aggregate-micro AGGREGATE_MICRO\n",
            "      Report micro-averaged metrics instead of macro averaged metrics. (default:\n",
            "      False)\n",
            "\n",
            "Tensorboard Arguments:\n",
            "  -tblog, --tensorboard-log TENSORBOARD_LOG\n",
            "      Tensorboard logging of metrics, default is False\n",
            "\n",
            "Seq2Seq Arguments:\n",
            "  -hs, --hiddensize HIDDENSIZE\n",
            "      size of the hidden layers (default: 128)\n",
            "  -esz, --embeddingsize EMBEDDINGSIZE\n",
            "      size of the token embeddings (default: 128)\n",
            "  -nl, --numlayers NUMLAYERS\n",
            "      number of hidden layers (default: 2)\n",
            "  -dr, --dropout DROPOUT\n",
            "      dropout rate (default: 0.1)\n",
            "  -bi, --bidirectional BIDIRECTIONAL\n",
            "      whether to encode the context with a bidirectional rnn (default: False)\n",
            "  -att, --attention {none,concat,general,dot,local}\n",
            "      Choices: none, concat, general, local. If set local, also set attention-\n",
            "      length. (see arxiv.org/abs/1508.04025) (default: none)\n",
            "  -attl, --attention-length ATTENTION_LENGTH\n",
            "      Length of local attention. (default: 48)\n",
            "  --attention-time {pre,post}\n",
            "      Whether to apply attention before or after decoding. (default: post)\n",
            "  -rnn, --rnn-class {rnn,gru,lstm}\n",
            "      Choose between different types of RNNs. (default: lstm)\n",
            "  -dec, --decoder {same,shared}\n",
            "      Choose between different decoder modules. Default \"same\" uses same class\n",
            "      as encoder, while \"shared\" also uses the same weights. Note that shared\n",
            "      disabled some encoder options--in particular, bidirectionality. (default:\n",
            "      same)\n",
            "  -lt, --lookuptable {unique,enc_dec,dec_out,all}\n",
            "      The encoder, decoder, and output modules can share weights, or not. Unique\n",
            "      has independent embeddings for each. Enc_dec shares the embedding for the\n",
            "      encoder and decoder. Dec_out shares decoder embedding and output weights.\n",
            "      All shares all three weights. (default: unique)\n",
            "  -soft, --numsoftmax NUMSOFTMAX\n",
            "      default 1, if greater then uses mixture of softmax (see\n",
            "      arxiv.org/abs/1711.03953). (default: 1)\n",
            "  -idr, --input-dropout INPUT_DROPOUT\n",
            "      Probability of replacing tokens with UNK in training. (default: 0.0)\n",
            "\n",
            "Torch Generator Agent:\n",
            "  --beam-size BEAM_SIZE\n",
            "      Beam size, if 1 then greedy search (default: 1)\n",
            "  --beam-min-length BEAM_MIN_LENGTH\n",
            "      Minimum length of prediction to be generated by the beam search (default:\n",
            "      1)\n",
            "  --beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM\n",
            "      Size n-grams to block in beam search from the context. val <= 0 implies no\n",
            "      blocking (default: -1)\n",
            "  --beam-block-ngram BEAM_BLOCK_NGRAM\n",
            "      Size n-grams to block in beam search. val <= 0 implies no blocking\n",
            "      (default: -1)\n",
            "  --beam-length-penalty BEAM_LENGTH_PENALTY\n",
            "      Applies a length penalty. Set to 0 for no penalty. (default: 0.65)\n",
            "  --inference {topk,beam,nucleus,delayedbeam,greedy}\n",
            "      Generation algorithm (default: greedy)\n",
            "  --topk TOPK\n",
            "      K used in Top K sampling (default: 10)\n",
            "  --topp TOPP\n",
            "      p used in nucleus sampling (default: 0.9)\n",
            "  --beam-delay BEAM_DELAY\n",
            "      used in delayedbeam search (default: 30)\n",
            "  --temperature TEMPERATURE\n",
            "      temperature to add during decoding (default: 1.0)\n",
            "  --compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU\n",
            "      if true, compute tokenized bleu scores (default: False)\n",
            "\n",
            "TorchAgent Arguments:\n",
            "  -i, --interactive-mode INTERACTIVE_MODE\n",
            "      Whether in full interactive mode or not, which means generating text or\n",
            "      retrieving from a full set of candidates, which is necessary to actually\n",
            "      do full dialogue. However, during training or quick validation (e.g. PPL\n",
            "      for generation or ranking a few candidates for ranking models) you might\n",
            "      want these set to off. Typically, scripts can set their preferred default\n",
            "      behavior at the start, e.g. eval scripts. (default: False)\n",
            "  -emb, --embedding-type {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}\n",
            "      Choose between different strategies for initializing word embeddings.\n",
            "      Default is random, but can also preinitialize from Glove or Fasttext.\n",
            "      Preinitialized embeddings can also be fixed so they are not updated during\n",
            "      training. (default: random)\n",
            "  -embp, --embedding-projection EMBEDDING_PROJECTION\n",
            "      If pretrained embeddings have a different dimensionality than your\n",
            "      embedding size, strategy for projecting to the correct size. If the\n",
            "      dimensions are the same, this is ignored unless you append \"-force\" to\n",
            "      your choice. (default: random)\n",
            "  --fp16 FP16\n",
            "      Use fp16 computations. (default: False)\n",
            "  --fp16-impl {apex,mem_efficient}\n",
            "      Implementation of FP16 to use (default: apex)\n",
            "  -rc, --rank-candidates RANK_CANDIDATES\n",
            "      Whether the model should parse candidates for ranking. (default: False)\n",
            "  -tr, --truncate TRUNCATE\n",
            "      Truncate input lengths to increase speed / use less memory. (default: -1)\n",
            "  --text-truncate TEXT_TRUNCATE\n",
            "      Text input truncation length: if not specified, this will default to\n",
            "      `truncate` (default: None)\n",
            "  --label-truncate LABEL_TRUNCATE\n",
            "      Label truncation length: if not specified, this will default to `truncate`\n",
            "      (default: None)\n",
            "  -histsz, --history-size HISTORY_SIZE\n",
            "      Number of past dialog utterances to remember. (default: -1)\n",
            "  -pt, --person-tokens PERSON_TOKENS\n",
            "      add person tokens to history. adds __p1__ in front of input text and\n",
            "      __p2__ in front of past labels when available or past utterances generated\n",
            "      by the model. these are added to the dictionary during initialization.\n",
            "      (default: False)\n",
            "  --split-lines SPLIT_LINES\n",
            "      split the dialogue history on newlines and save in separate vectors\n",
            "      (default: False)\n",
            "  --delimiter DELIMITER\n",
            "      Join history lines with this token, defaults to newline (default: )\n",
            "  -gpu, --gpu GPU\n",
            "      which GPU to use (default: -1)\n",
            "  --no-cuda\n",
            "      disable GPUs even if available. otherwise, will use GPUs if available on\n",
            "      the device. (default: False)\n",
            "\n",
            "Optimizer Arguments:\n",
            "  -opt, --optimizer {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}\n",
            "      Choose between pytorch optimizers. Any member of torch.optim should be\n",
            "      valid. (default: sgd)\n",
            "  -lr, --learningrate LEARNINGRATE\n",
            "      Learning rate (default: 1)\n",
            "  -clip, --gradient-clip GRADIENT_CLIP\n",
            "      gradient clipping using l2 norm (default: 0.1)\n",
            "  --adafactor-eps ADAFACTOR_EPS\n",
            "      Epsilon values for adafactor optimizer: regularization constants for\n",
            "      square gradient and parameter scale respectively (default: 1e-30,1e-3)\n",
            "  -mom, --momentum MOMENTUM\n",
            "      if applicable, momentum value for optimizer. (default: 0)\n",
            "  --nesterov NESTEROV\n",
            "      if applicable, whether to use nesterov momentum. (default: True)\n",
            "  -nu, --nus NUS\n",
            "      if applicable, nu value(s) for optimizer. can use a single value like 0.7\n",
            "      or a comma-separated tuple like 0.7,1.0 (default: 0.7)\n",
            "  -beta, --betas BETAS\n",
            "      if applicable, beta value(s) for optimizer. can use a single value like\n",
            "      0.9 or a comma-separated tuple like 0.9,0.999 (default: 0.9,0.999)\n",
            "  -wdecay, --weight-decay WEIGHT_DECAY\n",
            "      Weight decay on the weights. (default: None)\n",
            "\n",
            "BPEHelper Arguments:\n",
            "  --bpe-vocab BPE_VOCAB\n",
            "      path to pre-trained tokenizer vocab (default: None)\n",
            "  --bpe-merge BPE_MERGE\n",
            "      path to pre-trained tokenizer merge (default: None)\n",
            "\n",
            "Learning Rate Scheduler:\n",
            "  --lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}\n",
            "      Learning rate scheduler. (default: reduceonplateau)\n",
            "  --lr-scheduler-patience LR_SCHEDULER_PATIENCE\n",
            "      LR scheduler patience. In number of validation runs. If using fixed\n",
            "      scheduler, LR is decayed every <patience> validations. (default: 3)\n",
            "  --lr-scheduler-decay LR_SCHEDULER_DECAY\n",
            "      Decay factor for LR scheduler, or how much LR is multiplied by when it is\n",
            "      lowered. (default: 0.5)\n",
            "  --max-lr-steps MAX_LR_STEPS\n",
            "      Number of train steps the scheduler should take after warmup. Training is\n",
            "      terminated after this many steps. This should only be set for --lr-\n",
            "      scheduler cosine or linear (default: -1)\n",
            "  --invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA\n",
            "      Constant used only to find the lr multiplier for the invsqrt scheduler.\n",
            "      Must be set for --lr-scheduler invsqrt (default: -1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKGUWyKTwVtX"
      },
      "source": [
        "You'll notice the options are give as commandline arguments. We control our options via `argparse`. The option names are relatively predictable: `--init-model` becomes `init_model`; `--num-epochs` becomes `num_epochs` and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLwGAq1wZJb"
      },
      "source": [
        "# Looking at model predictions\n",
        "\n",
        "We have shown how we can chat with a model ourselves, interactively. We might want to inspect how the model reacts with a fixed set of inputs. Let's use that model we just trained!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZgs6OlvJ-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "95b89fa8-22a6-4261-d72e-e82b07517810"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3QKTjdwokh"
      },
      "source": [
        "Whoa wait a second! The model isn't giving any responses? That's because we set `--skip-generation true` to speed up training. We need to turn that back off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLiq-vuowamh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "241342eb-6ac6-4a0c-91c5-674ab6b7e0c7"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
            "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that ' s terrible ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good . i hope you are okay .\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MR0rn0ZwyxQ"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYuaSPWrw0Il"
      },
      "source": [
        "# Bringing your own datasets\n",
        "\n",
        "What if you want to build your own dataset in ParlAI? Of course you can do that!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgJi8XHwtph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9656b678-9dfc-4cfa-d1eb-3cb13aa14608"
      },
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        # opt is the command line arguments.\n",
        "        \n",
        "        # What is this shared thing?\n",
        "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
        "        \n",
        "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
        "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
        "        # the fold name (train/valid/test) + \".txt\"\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        # filename tells us where to load from.\n",
        "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        # setup_data should yield tuples of ((text, label), new_episode)\n",
        "        # That is ((str, str), bool)\n",
        "        \n",
        "        # first episode\n",
        "        # notice how we have call, response, and then True? The True indicates this is a first message\n",
        "        # in a conversation\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        # Next we have the second turn. This time, the last element is False, indicating we're still going\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        # second episode. We need to have True again!\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "EPOCH DONE\n",
            "[ loaded 2 episodes with a total of 6 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvwxi6gXw8jU"
      },
      "source": [
        "Notice how the data corresponds to the utterances we provided? In reality, we'd normally want to load up a data file, loop through it, and yield the tuples from processed data. But for this simple example, it works well.\n",
        "\n",
        "We can now use our teacher in the standard places! Let's see how the model we trained earlier behaves with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyZQnxAw5HG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "7240d04f-f9d3-4188-9bc8-c67f99b873b5"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['task'] to my_teacher (previously: empathetic_dialogues )]\n",
            "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i am good , how are you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzvSHAy0meK"
      },
      "source": [
        "Note that the `register_teacher` decorator makes the commands aware of your teacher. If you leave it off, the commands won't be able to locate it. If you want to use your teacher on the command line, you'll need to put it in a very specific filename: `parlai/agents/my_teacher/agents.py`, and you'll need to name the class `DefaultTeacher` instead of `MyTeacher`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJj7Lhs00oOB"
      },
      "source": [
        "# Creating your own models\n",
        "\n",
        "As a start, we'll implement a *very* simple agent. This agent will just sort of respond with \"hello X, my name is Y\", where X is based on the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykhtFDrxCPo"
      },
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser, partial_opt):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # similar to the teacher, we have the Opt and the shared memory objects!\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # Gather the last word from the other user's input\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # Always return a string like this.\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1SZmy_s0sGd"
      },
      "source": [
        "Let's try seeing how this agent behaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcS1UIFH0pb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "bcc52fbf-49f8-47ed-e5db-09e5b6c57b54"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model='hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello you, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello goodbye, I'm Alice\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hey, I'm Alice\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello vu?, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello chance, I'm Alice\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvcRSGS0wQE"
      },
      "source": [
        "Notice how it read the words from the user, and provides its name from the command line argument? We can also interact with it easily enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xd5CaG00tv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4d9aee8c-d390-46c7-e685-febcd5a5b91c"
      },
      "source": [
        "Interactive.main(model='hello', name='Bob')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[  interactive_task: True ]\n",
            "[  name: Bob ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /usr/local/lib/python3.6/dist-packages/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /usr/local/lib/python3.6/dist-packages/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: interactive ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: hello ]\n",
            "[  model_file: None ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[creating task(s): interactive]\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi, who are you?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m My name is Stephen\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello Stephen, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello stranger!, I'm Bob\u001b[0;0m\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAe1hytf1BPk"
      },
      "source": [
        "Similar to the teacher, the call to `register_agent` makes it available for use in commands. If you forget the `register_agent` decorator, you won't be able to refer to it. Similarly, if you wanted to use this model from the command line, you would need to save this code to a special folder: `parlai/agents/hello/hello.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aBbhKTO1DEE"
      },
      "source": [
        "## Creating a neural network model\n",
        "\n",
        "The base Agent class is very simple, but it also provides extremely little functionality. We have created solid abstractions for creating neural-network type models. [`TorchGeneratorAgent`](https://parl.ai/docs/torch_agent.html#module-parlai.core.torch_generator_agent) is one our common abstractions, and it assumes a model which outputs one-word-at-a-time.\n",
        "\n",
        "The following is from our [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) agent. It's a simple RNN model, trained like a Machine Translation model. The Model is too complex to go over in this document, but please feel free to [read our TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZh-T903wh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import parlai.core.torch_generator_agent as tga\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size.\n",
        "    Pay particular attention to the ``forward`` output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        # must call super on all nn.Modules.\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        \"\"\"\n",
        "        Perform the forward pass for the encoder.\n",
        "        Input *must* be input_tokens, which are the context tokens given\n",
        "        as a matrix of lookup IDs.\n",
        "        :param input_tokens:\n",
        "            Input tokens as a bsz x seqlen LongTensor.\n",
        "            Likely will contain padding.\n",
        "        :return:\n",
        "            You can return anything you like; it is will be passed verbatim\n",
        "            into the decoder for conditioning. However, it should be something\n",
        "            you can easily manipulate in ``reorder_encoder_states``.\n",
        "            This particular implementation returns the hidden and cell states from the\n",
        "            LSTM.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input_tokens)\n",
        "        _output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
        "    current incremental state on each forward pass.\n",
        "    Pay particular note to the ``forward``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, encoder_state, incr_state=None):\n",
        "        \"\"\"\n",
        "        Run forward pass.\n",
        "        :param input:\n",
        "            The currently generated tokens from the decoder.\n",
        "        :param encoder_state:\n",
        "            The output from the encoder module.\n",
        "        :parm incr_state:\n",
        "            The previous hidden state of the decoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input)\n",
        "        if incr_state is None:\n",
        "            # this is our very first call. We want to seed the LSTM with the\n",
        "            # hidden state of the decoder\n",
        "            state = encoder_state\n",
        "        else:\n",
        "            # We've generated some tokens already, so we can reuse the existing\n",
        "            # decoder state\n",
        "            state = incr_state\n",
        "\n",
        "        # get the new output and decoder incremental state\n",
        "        output, incr_state = self.lstm(embedded, state)\n",
        "\n",
        "        return output, incr_state\n",
        "\n",
        "\n",
        "class ExampleModel(tga.TorchGeneratorModel):\n",
        "    \"\"\"\n",
        "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
        "    re-order encoder states and decoder incremental states.\n",
        "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
        "    final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary, hidden_size=1024):\n",
        "        super().__init__(\n",
        "            padding_idx=dictionary[dictionary.null_token],\n",
        "            start_idx=dictionary[dictionary.start_token],\n",
        "            end_idx=dictionary[dictionary.end_token],\n",
        "            unknown_idx=dictionary[dictionary.unk_token],\n",
        "        )\n",
        "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
        "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
        "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
        "\n",
        "    def output(self, decoder_output):\n",
        "        \"\"\"\n",
        "        Perform the final output -> logits transformation.\n",
        "        \"\"\"\n",
        "        return F.linear(decoder_output, self.embeddings.weight)\n",
        "\n",
        "    def reorder_encoder_states(self, encoder_states, indices):\n",
        "        \"\"\"\n",
        "        Reorder the encoder states to select only the given batch indices.\n",
        "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
        "        Typically you will just want to index select on the batch dimension.\n",
        "        \"\"\"\n",
        "        h, c = encoder_states\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
        "        \"\"\"\n",
        "        Reorder the decoder states to select only the given batch indices.\n",
        "        This method can be a stub which always returns None; this will result in the\n",
        "        decoder doing a complete forward pass for every single token, making generation\n",
        "        O(n^2). However, if any state can be cached, then this method should be\n",
        "        implemented to reduce the generation complexity to O(n).\n",
        "        \"\"\"\n",
        "        h, c = incr_state\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "\n",
        "@register_agent(\"my_first_lstm\")\n",
        "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Example agent.\n",
        "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
        "    implements ``build_model``, but we will want to include additional command line\n",
        "    parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, argparser, partial_opt):\n",
        "        \"\"\"\n",
        "        Add CLI arguments.\n",
        "        \"\"\"\n",
        "        # Make sure to add all of TorchGeneratorAgent's arguments\n",
        "        super().add_cmdline_args(argparser)\n",
        "\n",
        "        # Add custom arguments only for this model.\n",
        "        group = argparser.add_argument_group('Example TGA Agent')\n",
        "        group.add_argument(\n",
        "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
        "        )\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construct the model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
        "        # Optionally initialize pre-trained embeddings by copying them from another\n",
        "        # source: GloVe, fastText, etc.\n",
        "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfR9w_Hm1HHY"
      },
      "source": [
        "Of course, now we can train with our new model. Let's train it on our toy task that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMXpogz1E-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "3fe8275b-a7cd-491a-a761-fbd5849e2a7c"
      },
      "source": [
        "# of course, we can train the model! Let's Train it on our silly toy task from above\n",
        "!rm -rf my_first_lstm\n",
        "!mkdir -p my_first_lstm\n",
        "\n",
        "TrainModel.main(\n",
        "    model='my_first_lstm',\n",
        "    model_file='my_first_lstm/model',\n",
        "    task='my_teacher',\n",
        "    batchsize=1,\n",
        "    validation_every_n_secs=10,\n",
        "    max_train_time=60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 6.00/6.00 [00:00<00:00, 1.91kex/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ building dictionary first... ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            " ~~ Loading from train.txt ~~ \n",
            "Dictionary: saving dictionary to my_first_lstm/model.dict\n",
            "[ dictionary built with 30 tokens in 0s ]\n",
            "[ no model with opt yet at: my_first_lstm/model(.opt) ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "[ training... ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "     clip  exs  gnorm  gpu_mem   loss  lr   ppl  token_acc  total_train_updates   tpb  updates\n",
            "   .01641 1828  1.368    .9171 .04105   1 1.042      .9942                 1828 3.328     1828\n",
            "\n",
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "    gpu_mem  lr  total_train_updates\n",
            "      .9171   1                 1828\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.06s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9171     0   1    1          1                 1828 3.333\n",
            "\n",
            "[ new best accuracy: 1 ]\n",
            "[ saving best valid model: my_first_lstm/model ]\n",
            "[ task solved! stopping. ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.05s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from test.txt ~~ \n",
            "[ running eval: test ]\n",
            "[ eval completed in 0.05s ]\n",
            "test:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHrruVd1KnK"
      },
      "source": [
        "Let's see how it does. It should reproduce the data perfectly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqFpdrE1Iif",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "1a89571c-2a42-48b8-9752-dffc4a58e557"
      },
      "source": [
        "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: This is it\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzrDhPS1QCW"
      },
      "source": [
        "Unsurprisingly, we got perfect accuracy. This is because the data set is only a handful of utterances, and we can perfectly memorize it in this LSTM. Nonetheless, a great success!\n",
        "\n",
        "# What's next!\n",
        "\n",
        "The sky's the limit! Be sure to check out our [GitHub](https://github.com/facebookresearch/ParlAI) and [Follow ParlAI on Twitter](https://twitter.com/parlai_parley). We're eager to hear what you are using ParlAI for!\n",
        "\n",
        "Here are some other great resources:\n",
        "- [Our research page](https://parl.ai/projects/)\n",
        "- [ParlAI Documentations](https://parl.ai/docs/index.html)\n",
        "- [Tutorial: Writing a Ranker model](https://parl.ai/docs/tutorial_torch_ranker_agent.html)\n",
        "- [Tutorial: Using Mechanical Turk](https://parl.ai/docs/tutorial_mturk.html)\n",
        "- [Tutorial: Connecting to chat services](https://parl.ai/docs/tutorial_chat_service.html)"
      ]
    }
  ]
}