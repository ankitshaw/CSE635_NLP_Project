{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsb-Cvf6lnVX"
      },
      "source": [
        "<img src=\"https://parl.ai/docs/_static/img/parlai.png\" width=\"700\"/>\n",
        "\n",
        "**Author**: Stephen Roller ([GitHub](https://github.com/stephenroller), [Twitter](https://twitter.com/stephenroller))\n",
        "\n",
        "\n",
        "# Welcome to the ParlAI interactive tutorial\n",
        "\n",
        "In this tutorial we will:\n",
        "\n",
        "- Chat with a neural network model!\n",
        "- Show how to use common commands in ParlAI, like inspecting data and model outputs.\n",
        "- See where to find information about many options.\n",
        "- Show how to fine-tune a pretrained model on a specific task\n",
        "- Add our own datasets to ParlAI\n",
        "- And add our own models to ParlAI\n",
        "\n",
        "We won't be running any examples of using Amazon Mechanical Turk, or connecting to Chat services, but you can check out our [docs](https://parl.ai/docs/) for more information on these areas.\n",
        "\n",
        "**Note:** *Make sure you're running this session with a GPU attached.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_bFnOWslsj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7e7bdb-2975-4660-8fc3-a9677386e1a4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 30 20:08:41 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMxd1KIRl9Xm"
      },
      "source": [
        "## Installing parlai\n",
        "\n",
        "We need to install ParlAI. Since we're in Google Colab, we can assume PyTorch and similar dependencies are installed already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i93Mn_I7MOEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d2e67a-21cc-425f-f5cf-7ba71ce23382"
      },
      "source": [
        "!pip install -q parlai\n",
        "!pip install -q subword_nmt # extra requirement we need for this tutorial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.3/266.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.9/95.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.6/547.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.1/110.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.4/145.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.8/140.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.2/141.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.2/141.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.1/136.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "rich 13.3.4 requires markdown-it-py<3.0.0,>=2.2.0, but you have markdown-it-py 0.5.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtVz5dCUmFkN"
      },
      "source": [
        "# Chatting with a model\n",
        "\n",
        "Let's start by chatting interactively with a model file from our model zoo! We'll pick our \"tutorial transformer generator\" model, which is a generative transformer trained on pushshift.io Reddit. You can take a look at the [model zoo](https://parl.ai/docs/zoo.html) for a more complete list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJGRtMKmIWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e286328-fe89-4b44-8e0d-1ab1cf5ac656"
      },
      "source": [
        "# Import the Interactive script\n",
        "from parlai.scripts.interactive import Interactive\n",
        "\n",
        "# call it with particular args\n",
        "Interactive.main(\n",
        "    # the model_file is a filename path pointing to a particular model dump.\n",
        "    # Model files that begin with \"zoo:\" are special files distributed by the ParlAI team.\n",
        "    # They'll be automatically downloaded when you ask to use them.\n",
        "    model_file='zoo:tutorial_transformer_generator/model'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:02:27 | building data: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n",
            "15:02:27 | Downloading http://parl.ai/downloads/_models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/tutorial_transformer_generator_v1.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading tutorial_transformer_generator_v1.tar.gz: 100%|██████████| 1.12G/1.12G [01:30<00:00, 12.4MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15:04:22 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model (previously: /checkpoint/roller/20190909/cleanreddit/585/model)\u001b[0m\n",
            "15:04:22 | \u001b[33mLoading model with `--beam-block-full-context false`\u001b[0m\n",
            "15:04:22 | Using CUDA\n",
            "15:04:22 | loading dictionary from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "15:04:22 | num words = 54944\n",
            "15:04:22 | TransformerGenerator: full interactive mode on.\n",
            "15:04:23 | \u001b[33mDEPRECATED: XLM should only be used for backwards compatibility, as it involves a less-stable layernorm operation.\u001b[0m\n",
            "15:04:38 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "15:04:38 | Loading existing model params from /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "15:04:39 | Opt:\n",
            "15:04:39 |     activation: gelu\n",
            "15:04:39 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "15:04:39 |     adam_eps: 1e-06\n",
            "15:04:39 |     add_p1_after_newln: False\n",
            "15:04:39 |     aggregate_micro: False\n",
            "15:04:39 |     allow_missing_init_opts: False\n",
            "15:04:39 |     attention_dropout: 0.0\n",
            "15:04:39 |     batch_length_range: 5\n",
            "15:04:39 |     batch_sort_cache_type: pop\n",
            "15:04:39 |     batch_sort_field: text\n",
            "15:04:39 |     batchsize: 48\n",
            "15:04:39 |     beam_block_full_context: False\n",
            "15:04:39 |     beam_block_list_filename: None\n",
            "15:04:39 |     beam_block_ngram: 3\n",
            "15:04:39 |     beam_context_block_ngram: 3\n",
            "15:04:39 |     beam_delay: 30\n",
            "15:04:39 |     beam_length_penalty: 0.65\n",
            "15:04:39 |     beam_min_length: 10\n",
            "15:04:39 |     beam_min_n_best: 3\n",
            "15:04:39 |     beam_size: 8\n",
            "15:04:39 |     betas: '[0.9, 0.98]'\n",
            "15:04:39 |     bpe_add_prefix_space: None\n",
            "15:04:39 |     bpe_debug: False\n",
            "15:04:39 |     bpe_dropout: None\n",
            "15:04:39 |     bpe_merge: None\n",
            "15:04:39 |     bpe_vocab: None\n",
            "15:04:39 |     checkpoint_activations: False\n",
            "15:04:39 |     compute_tokenized_bleu: False\n",
            "15:04:39 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "15:04:39 |     datatype: train:stream\n",
            "15:04:39 |     delimiter: '\\n'\n",
            "15:04:39 |     dict_build_first: True\n",
            "15:04:39 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "15:04:39 |     dict_endtoken: __end__\n",
            "15:04:39 |     dict_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "15:04:39 |     dict_include_test: False\n",
            "15:04:39 |     dict_include_valid: False\n",
            "15:04:39 |     dict_initpath: None\n",
            "15:04:39 |     dict_language: english\n",
            "15:04:39 |     dict_loaded: True\n",
            "15:04:39 |     dict_lower: True\n",
            "15:04:39 |     dict_max_ngram_size: -1\n",
            "15:04:39 |     dict_maxexs: -1\n",
            "15:04:39 |     dict_maxtokens: -1\n",
            "15:04:39 |     dict_minfreq: 0\n",
            "15:04:39 |     dict_nulltoken: __null__\n",
            "15:04:39 |     dict_starttoken: __start__\n",
            "15:04:39 |     dict_textfields: text,labels\n",
            "15:04:39 |     dict_tokenizer: bpe\n",
            "15:04:39 |     dict_unktoken: __unk__\n",
            "15:04:39 |     display_add_fields: \n",
            "15:04:39 |     display_examples: False\n",
            "15:04:39 |     display_prettify: False\n",
            "15:04:39 |     distributed_world_size: 64\n",
            "15:04:39 |     download_path: None\n",
            "15:04:39 |     dropout: 0.1\n",
            "15:04:39 |     dynamic_batching: None\n",
            "15:04:39 |     embedding_projection: random\n",
            "15:04:39 |     embedding_size: 512\n",
            "15:04:39 |     embedding_type: random\n",
            "15:04:39 |     embeddings_scale: True\n",
            "15:04:39 |     eval_batchsize: None\n",
            "15:04:39 |     evaltask: None\n",
            "15:04:39 |     ffn_size: 2048\n",
            "15:04:39 |     force_fp16_tokens: True\n",
            "15:04:39 |     fp16: True\n",
            "15:04:39 |     fp16_impl: safe\n",
            "15:04:39 |     gpu: 0\n",
            "15:04:39 |     gradient_clip: 10.0\n",
            "15:04:39 |     hide_labels: False\n",
            "15:04:39 |     history_add_global_end_token: None\n",
            "15:04:39 |     history_reversed: False\n",
            "15:04:39 |     history_size: -1\n",
            "15:04:39 |     image_cropsize: 224\n",
            "15:04:39 |     image_mode: raw\n",
            "15:04:39 |     image_size: 256\n",
            "15:04:39 |     inference: beam\n",
            "15:04:39 |     init_model: None\n",
            "15:04:39 |     init_opt: None\n",
            "15:04:39 |     interactive_mode: True\n",
            "15:04:39 |     interactive_task: True\n",
            "15:04:39 |     invsqrt_lr_decay_gamma: -1\n",
            "15:04:39 |     is_debug: False\n",
            "15:04:39 |     label_truncate: 128\n",
            "15:04:39 |     learn_positional_embeddings: True\n",
            "15:04:39 |     learningrate: 0.0005\n",
            "15:04:39 |     local_human_candidates_file: None\n",
            "15:04:39 |     log_every_n_secs: 30.0\n",
            "15:04:39 |     log_keep_fields: all\n",
            "15:04:39 |     loglevel: info\n",
            "15:04:39 |     lr_scheduler: invsqrt\n",
            "15:04:39 |     lr_scheduler_decay: 0.5\n",
            "15:04:39 |     lr_scheduler_patience: 3\n",
            "15:04:39 |     max_train_time: -1\n",
            "15:04:39 |     metrics: default\n",
            "15:04:39 |     model: transformer/generator\n",
            "15:04:39 |     model_file: /usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "15:04:39 |     model_parallel: False\n",
            "15:04:39 |     momentum: 0\n",
            "15:04:39 |     multitask_weights: [1]\n",
            "15:04:39 |     n_decoder_layers: -1\n",
            "15:04:39 |     n_encoder_layers: -1\n",
            "15:04:39 |     n_heads: 16\n",
            "15:04:39 |     n_layers: 8\n",
            "15:04:39 |     n_positions: 512\n",
            "15:04:39 |     n_segments: 0\n",
            "15:04:39 |     nesterov: True\n",
            "15:04:39 |     no_cuda: False\n",
            "15:04:39 |     num_epochs: 5.0\n",
            "15:04:39 |     numthreads: 1\n",
            "15:04:39 |     numworkers: 4\n",
            "15:04:39 |     nus: [0.7]\n",
            "15:04:39 |     optimizer: fused_adam\n",
            "15:04:39 |     outfile: \n",
            "15:04:39 |     output_scaling: 1.0\n",
            "15:04:39 |     override: \"{'model_file': '/usr/local/lib/python3.7/dist-packages/data/models/tutorial_transformer_generator/model'}\"\n",
            "15:04:39 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "15:04:39 |     person_tokens: False\n",
            "15:04:39 |     port: 61337\n",
            "15:04:39 |     pytorch_context_length: -1\n",
            "15:04:39 |     pytorch_datapath: None\n",
            "15:04:39 |     pytorch_include_labels: True\n",
            "15:04:39 |     pytorch_preprocess: False\n",
            "15:04:39 |     pytorch_teacher_batch_sort: False\n",
            "15:04:39 |     pytorch_teacher_dataset: None\n",
            "15:04:39 |     pytorch_teacher_task: None\n",
            "15:04:39 |     rank_candidates: False\n",
            "15:04:39 |     relu_dropout: 0.0\n",
            "15:04:39 |     save_after_valid: True\n",
            "15:04:39 |     save_every_n_secs: -1\n",
            "15:04:39 |     save_format: conversations\n",
            "15:04:39 |     share_word_embeddings: True\n",
            "15:04:39 |     short_final_eval: True\n",
            "15:04:39 |     show_advanced_args: False\n",
            "15:04:39 |     shuffle: False\n",
            "15:04:39 |     single_turn: False\n",
            "15:04:39 |     skip_generation: False\n",
            "15:04:39 |     special_tok_lst: None\n",
            "15:04:39 |     split_lines: False\n",
            "15:04:39 |     starttime: Oct13_15-04\n",
            "15:04:39 |     task: internal:new_reddit:presorted\n",
            "15:04:39 |     temperature: 1.0\n",
            "15:04:39 |     tensorboard_log: False\n",
            "15:04:39 |     text_truncate: 512\n",
            "15:04:39 |     topk: 10\n",
            "15:04:39 |     topp: 0.9\n",
            "15:04:39 |     truncate: -1\n",
            "15:04:39 |     update_freq: 1\n",
            "15:04:39 |     use_reply: label\n",
            "15:04:39 |     validation_cutoff: 1.0\n",
            "15:04:39 |     validation_every_n_epochs: -1\n",
            "15:04:39 |     validation_every_n_secs: 1800.0\n",
            "15:04:39 |     validation_max_exs: 9920\n",
            "15:04:39 |     validation_metric: ppl\n",
            "15:04:39 |     validation_metric_mode: min\n",
            "15:04:39 |     validation_patience: 0\n",
            "15:04:39 |     validation_share_agent: False\n",
            "15:04:39 |     variant: xlm\n",
            "15:04:39 |     verbose: False\n",
            "15:04:39 |     warmup_rate: 0.0001\n",
            "15:04:40 |     warmup_updates: 20000\n",
            "15:04:40 |     weight_decay: 0.01\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "15:04:40 | creating task(s): interactive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hfUEgovmWay"
      },
      "source": [
        "The same on the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.interactive --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_hGrZGGmaWF"
      },
      "source": [
        "# Taking a look at some data\n",
        "\n",
        "We can look at look into a specific dataset. Let's look into the \"empathetic dialogues\" dataset, which aims to teach models how to respond with text expressing the appropriate emotion. We have over existing 80 datasets in ParlAI. You can take a full look in our [task list](https://parl.ai/docs/tasks.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqckSXqlmWuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c4b5c2-0264-42fd-a650-caaf3f4fbdc3"
      },
      "source": [
        "# The display_data script is used to show the contents of a particular task.\n",
        "# By default, we show the train\n",
        "from parlai.scripts.display_data import DisplayData\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21:19:04 | Opt:\n",
            "21:19:04 |     allow_missing_init_opts: False\n",
            "21:19:04 |     batchsize: 1\n",
            "21:19:04 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "21:19:04 |     datatype: train:ordered\n",
            "21:19:04 |     dict_class: None\n",
            "21:19:04 |     display_add_fields: \n",
            "21:19:04 |     download_path: None\n",
            "21:19:04 |     dynamic_batching: None\n",
            "21:19:04 |     hide_labels: False\n",
            "21:19:04 |     ignore_agent_reply: True\n",
            "21:19:04 |     image_cropsize: 224\n",
            "21:19:04 |     image_mode: raw\n",
            "21:19:04 |     image_size: 256\n",
            "21:19:04 |     init_model: None\n",
            "21:19:04 |     init_opt: None\n",
            "21:19:04 |     is_debug: False\n",
            "21:19:04 |     loglevel: info\n",
            "21:19:04 |     max_display_len: 1000\n",
            "21:19:04 |     model: None\n",
            "21:19:04 |     model_file: None\n",
            "21:19:04 |     multitask_weights: [1]\n",
            "21:19:04 |     mutators: None\n",
            "21:19:04 |     num_examples: 5\n",
            "21:19:04 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 5}\"\n",
            "21:19:04 |     parlai_home: /usr/local/lib/python3.10/dist-packages\n",
            "21:19:04 |     starttime: Apr29_21-19\n",
            "21:19:04 |     task: empathetic_dialogues\n",
            "21:19:04 |     teacher_seed: None\n",
            "21:19:04 |     train_experiencer_only: False\n",
            "21:19:04 |     verbose: False\n",
            "21:19:04 | creating task(s): empathetic_dialogues\n",
            "[building data: /usr/local/lib/python3.10/dist-packages/data/empatheticdialogues]\n",
            "21:19:04 | Downloading http://parl.ai/downloads/empatheticdialogues/empatheticdialogues.tar.gz to /usr/local/lib/python3.10/dist-packages/data/empatheticdialogues/empatheticdialogues.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading empatheticdialogues.tar.gz: 100%|██████████| 28.0M/28.0M [00:00<00:00, 38.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mI remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, we felt like the only people in the world.\u001b[0;0m\n",
            "   \u001b[1;94mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "\u001b[0mThis was a best friend. I miss her.\u001b[0;0m\n",
            "   \u001b[1;94mWhere has she gone?\u001b[0;0m\n",
            "\u001b[0mWe no longer talk.\u001b[0;0m\n",
            "   \u001b[1;94mOh was this something that happened because of an argument?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mWas this a friend you were in love with, or just a best friend?\u001b[0;0m\n",
            "   \u001b[1;94mThis was a best friend. I miss her.\u001b[0;0m\n",
            "\u001b[0mWhere has she gone?\u001b[0;0m\n",
            "   \u001b[1;94mWe no longer talk.\u001b[0;0m\n",
            "21:19:07 | loaded 39057 episodes with a total of 64636 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9C6oHq87zGx"
      },
      "source": [
        "The black, unindented text is the _prompt_, while the blue text is the _label_. That is, the label is what we will be training the model to mimic.\n",
        "\n",
        "We can also ask to see fewer examples, and get them from the validation set instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNSBetWmfGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e13563e-a749-49f6-b6b0-19d2c7473448"
      },
      "source": [
        "# we can instead ask to see fewer examples, and get them from the valid set.\n",
        "DisplayData.main(task='empathetic_dialogues', num_examples=3, datatype='valid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21:19:49 | Opt:\n",
            "21:19:49 |     allow_missing_init_opts: False\n",
            "21:19:49 |     batchsize: 1\n",
            "21:19:49 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n",
            "21:19:49 |     datatype: valid\n",
            "21:19:49 |     dict_class: None\n",
            "21:19:49 |     display_add_fields: \n",
            "21:19:49 |     download_path: None\n",
            "21:19:49 |     dynamic_batching: None\n",
            "21:19:49 |     hide_labels: False\n",
            "21:19:49 |     ignore_agent_reply: True\n",
            "21:19:49 |     image_cropsize: 224\n",
            "21:19:49 |     image_mode: raw\n",
            "21:19:49 |     image_size: 256\n",
            "21:19:49 |     init_model: None\n",
            "21:19:49 |     init_opt: None\n",
            "21:19:49 |     loglevel: info\n",
            "21:19:49 |     max_display_len: 1000\n",
            "21:19:49 |     model: None\n",
            "21:19:49 |     model_file: None\n",
            "21:19:49 |     multitask_weights: [1]\n",
            "21:19:49 |     mutators: None\n",
            "21:19:49 |     num_examples: 3\n",
            "21:19:49 |     override: \"{'task': 'empathetic_dialogues', 'num_examples': 3, 'datatype': 'valid'}\"\n",
            "21:19:49 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n",
            "21:19:49 |     remove_political_convos: False\n",
            "21:19:49 |     starttime: Apr16_21-19\n",
            "21:19:49 |     task: empathetic_dialogues\n",
            "21:19:49 |     train_experiencer_only: False\n",
            "21:19:49 |     verbose: False\n",
            "21:19:49 | creating task(s): empathetic_dialogues\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "   \u001b[1;94mAre you fine now?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "   \u001b[1;94mCool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues - - -\u001b[0;0m\n",
            "\u001b[0mA few weeks ago, I was walking through my hallway, minding my own business, when all of a sudden a hand reached out from under a table and grabbed my ankle. I was so suprised. I thought i was got. Turns out, it was my son. \u001b[0;0m\n",
            "   \u001b[1;94mThat's funny, hope he didn't give you a heart attack.\u001b[0;0m\n",
            "21:19:49 | loaded 2769 episodes with a total of 5738 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSrgRrEmdS-"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_data --task empathetic_dialogues\n",
        "```\n",
        "or a bit shorter\n",
        "```\n",
        "python -m parlai.scripts.display_data -t empathetic_dialogues\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_M8Zr86n2_G"
      },
      "source": [
        "# Training a model\n",
        "\n",
        "Well it's one thing looking at data, but what if we want to train our own model (from scratch)? Let's train a very simple seq2seq LSTM with attention, to respond to empathetic dialogues.\n",
        "\n",
        "To get some extra performance, we'll initialize using GloVe embeddings, but we will cap the training time to 2 minutes for this tutorial. It won't perform very well, but that's okay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBhVQycSn2q_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6bfde9d5-278c-477d-a2a7-528a1288e110"
      },
      "source": [
        "# we'll save it in the \"from_scratch_model\" directory\n",
        "!rm -rf from_scratch_model\n",
        "!mkdir -p from_scratch_model\n",
        "\n",
        "from parlai.scripts.train_model import TrainModel\n",
        "TrainModel.main(\n",
        "    # we MUST provide a filename\n",
        "    model_file='from_scratch_model/model',\n",
        "    # train on empathetic dialogues\n",
        "    task='empathetic_dialogues',\n",
        "    # limit training time to 2 minutes, and a batchsize of 16\n",
        "    max_train_time=2 * 60,\n",
        "    batchsize=16,\n",
        "    \n",
        "    # we specify the model type as seq2seq\n",
        "    model='seq2seq',\n",
        "    # some hyperparamter choices. We'll use attention. We could use pretrained\n",
        "    # embeddings too, with embedding_type='fasttext', but they take a long\n",
        "    # time to download.\n",
        "    attention='dot',\n",
        "    # tie the word embeddings of the encoder/decoder/softmax.\n",
        "    lookuptable='all',\n",
        "    # truncate text and labels at 64 tokens, for memory and time savings\n",
        "    truncate=64,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21:50:28 | building dictionary first...\n",
            "21:50:28 | Opt:\n",
            "21:50:28 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "21:50:28 |     adam_eps: 1e-08\n",
            "21:50:28 |     add_p1_after_newln: False\n",
            "21:50:28 |     aggregate_micro: False\n",
            "21:50:28 |     allow_missing_init_opts: False\n",
            "21:50:28 |     attention: dot\n",
            "21:50:28 |     attention_length: 48\n",
            "21:50:28 |     attention_time: post\n",
            "21:50:28 |     batchsize: 1\n",
            "21:50:28 |     beam_block_full_context: True\n",
            "21:50:28 |     beam_block_list_filename: None\n",
            "21:50:28 |     beam_block_ngram: -1\n",
            "21:50:28 |     beam_context_block_ngram: -1\n",
            "21:50:28 |     beam_delay: 30\n",
            "21:50:28 |     beam_length_penalty: 0.65\n",
            "21:50:28 |     beam_min_length: 1\n",
            "21:50:28 |     beam_size: 1\n",
            "21:50:28 |     betas: '(0.9, 0.999)'\n",
            "21:50:28 |     bidirectional: False\n",
            "21:50:28 |     bpe_add_prefix_space: None\n",
            "21:50:28 |     bpe_debug: False\n",
            "21:50:28 |     bpe_dropout: None\n",
            "21:50:28 |     bpe_merge: None\n",
            "21:50:28 |     bpe_vocab: None\n",
            "21:50:28 |     compute_tokenized_bleu: False\n",
            "21:50:28 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "21:50:28 |     datatype: train\n",
            "21:50:28 |     decoder: same\n",
            "21:50:28 |     delimiter: '\\n'\n",
            "21:50:28 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:50:28 |     dict_endtoken: __end__\n",
            "21:50:28 |     dict_file: from_scratch_model/model.dict\n",
            "21:50:28 |     dict_include_test: False\n",
            "21:50:28 |     dict_include_valid: False\n",
            "21:50:28 |     dict_initpath: None\n",
            "21:50:28 |     dict_language: english\n",
            "21:50:28 |     dict_loaded: False\n",
            "21:50:28 |     dict_lower: False\n",
            "21:50:28 |     dict_max_ngram_size: -1\n",
            "21:50:28 |     dict_maxexs: -1\n",
            "21:50:28 |     dict_maxtokens: -1\n",
            "21:50:28 |     dict_minfreq: 0\n",
            "21:50:28 |     dict_nulltoken: __null__\n",
            "21:50:28 |     dict_starttoken: __start__\n",
            "21:50:28 |     dict_textfields: text,labels\n",
            "21:50:28 |     dict_tokenizer: re\n",
            "21:50:28 |     dict_unktoken: __unk__\n",
            "21:50:28 |     display_examples: False\n",
            "21:50:28 |     download_path: None\n",
            "21:50:28 |     dropout: 0.1\n",
            "21:50:28 |     dynamic_batching: None\n",
            "21:50:28 |     embedding_projection: random\n",
            "21:50:28 |     embedding_type: random\n",
            "21:50:28 |     embeddingsize: 128\n",
            "21:50:28 |     eval_batchsize: None\n",
            "21:50:28 |     eval_dynamic_batching: None\n",
            "21:50:28 |     evaltask: None\n",
            "21:50:28 |     final_extra_opt: \n",
            "21:50:28 |     force_fp16_tokens: False\n",
            "21:50:28 |     fp16: False\n",
            "21:50:28 |     fp16_impl: safe\n",
            "21:50:28 |     gpu: -1\n",
            "21:50:28 |     gpu_beam_blocking: False\n",
            "21:50:28 |     gradient_clip: 0.1\n",
            "21:50:28 |     hiddensize: 128\n",
            "21:50:28 |     hide_labels: False\n",
            "21:50:28 |     history_add_global_end_token: None\n",
            "21:50:28 |     history_reversed: False\n",
            "21:50:28 |     history_size: -1\n",
            "21:50:28 |     image_cropsize: 224\n",
            "21:50:28 |     image_mode: no_image_model\n",
            "21:50:28 |     image_size: 256\n",
            "21:50:28 |     inference: greedy\n",
            "21:50:28 |     init_model: None\n",
            "21:50:28 |     init_opt: None\n",
            "21:50:28 |     input_dropout: 0.0\n",
            "21:50:28 |     interactive_mode: False\n",
            "21:50:28 |     invsqrt_lr_decay_gamma: -1\n",
            "21:50:28 |     is_debug: False\n",
            "21:50:28 |     label_truncate: None\n",
            "21:50:28 |     learningrate: 1\n",
            "21:50:28 |     load_from_checkpoint: True\n",
            "21:50:28 |     log_every_n_secs: -1\n",
            "21:50:28 |     log_every_n_steps: 50\n",
            "21:50:28 |     log_keep_fields: all\n",
            "21:50:28 |     loglevel: info\n",
            "21:50:28 |     lookuptable: all\n",
            "21:50:28 |     lr_scheduler: reduceonplateau\n",
            "21:50:28 |     lr_scheduler_decay: 0.5\n",
            "21:50:28 |     lr_scheduler_patience: 3\n",
            "21:50:28 |     max_train_steps: -1\n",
            "21:50:28 |     max_train_time: 120.0\n",
            "21:50:28 |     metrics: default\n",
            "21:50:28 |     model: seq2seq\n",
            "21:50:28 |     model_file: from_scratch_model/model\n",
            "21:50:28 |     momentum: 0\n",
            "21:50:28 |     multitask_weights: [1]\n",
            "21:50:28 |     mutators: None\n",
            "21:50:28 |     nesterov: True\n",
            "21:50:28 |     no_cuda: False\n",
            "21:50:28 |     num_epochs: -1\n",
            "21:50:28 |     num_workers: 0\n",
            "21:50:28 |     numlayers: 2\n",
            "21:50:28 |     numsoftmax: 1\n",
            "21:50:28 |     nus: (0.7,)\n",
            "21:50:28 |     optimizer: sgd\n",
            "21:50:28 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "21:50:28 |     parlai_home: /usr/local/lib/python3.10/dist-packages\n",
            "21:50:28 |     person_tokens: False\n",
            "21:50:28 |     rank_candidates: False\n",
            "21:50:28 |     rnn_class: lstm\n",
            "21:50:28 |     save_after_valid: False\n",
            "21:50:28 |     save_every_n_secs: -1\n",
            "21:50:28 |     save_format: conversations\n",
            "21:50:28 |     seed: None\n",
            "21:50:28 |     short_final_eval: False\n",
            "21:50:28 |     skip_generation: False\n",
            "21:50:28 |     special_tok_lst: None\n",
            "21:50:28 |     split_lines: False\n",
            "21:50:28 |     starttime: Apr29_21-50\n",
            "21:50:28 |     task: empathetic_dialogues\n",
            "21:50:28 |     teacher_seed: None\n",
            "21:50:28 |     temperature: 1.0\n",
            "21:50:28 |     tensorboard_log: False\n",
            "21:50:28 |     tensorboard_logdir: None\n",
            "21:50:28 |     text_truncate: None\n",
            "21:50:28 |     topk: 10\n",
            "21:50:28 |     topp: 0.9\n",
            "21:50:28 |     train_experiencer_only: False\n",
            "21:50:28 |     truncate: 64\n",
            "21:50:28 |     update_freq: 1\n",
            "21:50:28 |     use_reply: label\n",
            "21:50:28 |     validation_cutoff: 1.0\n",
            "21:50:28 |     validation_every_n_epochs: -1\n",
            "21:50:28 |     validation_every_n_secs: -1\n",
            "21:50:28 |     validation_every_n_steps: -1\n",
            "21:50:28 |     validation_max_exs: -1\n",
            "21:50:28 |     validation_metric: accuracy\n",
            "21:50:28 |     validation_metric_mode: None\n",
            "21:50:28 |     validation_patience: 10\n",
            "21:50:28 |     validation_share_agent: False\n",
            "21:50:28 |     verbose: False\n",
            "21:50:28 |     wandb_entity: None\n",
            "21:50:28 |     wandb_log: False\n",
            "21:50:28 |     wandb_log_model: False\n",
            "21:50:28 |     wandb_name: None\n",
            "21:50:28 |     wandb_project: None\n",
            "21:50:28 |     warmup_rate: 0.0001\n",
            "21:50:28 |     warmup_updates: -1\n",
            "21:50:28 |     weight_decay: None\n",
            "21:50:28 |     world_logs: \n",
            "21:50:28 | creating task(s): empathetic_dialogues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building dictionary: 100%|██████████| 64.6k/64.6k [00:04<00:00, 13.9kex/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21:50:34 | Saving dictionary to from_scratch_model/model.dict\n",
            "21:50:34 | dictionary built with 22419 tokens in 0.0s\n",
            "21:50:34 | No model with opt yet at: from_scratch_model/model(.opt)\n",
            "21:50:34 | Using CUDA\n",
            "21:50:34 | loading dictionary from from_scratch_model/model.dict\n",
            "21:50:34 | num words = 22419\n",
            "21:50:34 | Total parameters: 3,453,203 (3,453,203 trainable)\n",
            "21:50:34 | Opt:\n",
            "21:50:34 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "21:50:34 |     adam_eps: 1e-08\n",
            "21:50:34 |     add_p1_after_newln: False\n",
            "21:50:34 |     aggregate_micro: False\n",
            "21:50:34 |     allow_missing_init_opts: False\n",
            "21:50:34 |     attention: dot\n",
            "21:50:34 |     attention_length: 48\n",
            "21:50:35 |     attention_time: post\n",
            "21:50:35 |     batchsize: 16\n",
            "21:50:35 |     beam_block_full_context: True\n",
            "21:50:35 |     beam_block_list_filename: None\n",
            "21:50:35 |     beam_block_ngram: -1\n",
            "21:50:35 |     beam_context_block_ngram: -1\n",
            "21:50:35 |     beam_delay: 30\n",
            "21:50:35 |     beam_length_penalty: 0.65\n",
            "21:50:35 |     beam_min_length: 1\n",
            "21:50:35 |     beam_size: 1\n",
            "21:50:35 |     betas: '(0.9, 0.999)'\n",
            "21:50:35 |     bidirectional: False\n",
            "21:50:35 |     bpe_add_prefix_space: None\n",
            "21:50:35 |     bpe_debug: False\n",
            "21:50:35 |     bpe_dropout: None\n",
            "21:50:35 |     bpe_merge: None\n",
            "21:50:35 |     bpe_vocab: None\n",
            "21:50:35 |     compute_tokenized_bleu: False\n",
            "21:50:35 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "21:50:35 |     datatype: train\n",
            "21:50:35 |     decoder: same\n",
            "21:50:35 |     delimiter: '\\n'\n",
            "21:50:35 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:50:35 |     dict_endtoken: __end__\n",
            "21:50:35 |     dict_file: from_scratch_model/model.dict\n",
            "21:50:35 |     dict_include_test: False\n",
            "21:50:35 |     dict_include_valid: False\n",
            "21:50:35 |     dict_initpath: None\n",
            "21:50:35 |     dict_language: english\n",
            "21:50:35 |     dict_loaded: True\n",
            "21:50:35 |     dict_lower: False\n",
            "21:50:35 |     dict_max_ngram_size: -1\n",
            "21:50:35 |     dict_maxexs: -1\n",
            "21:50:35 |     dict_maxtokens: -1\n",
            "21:50:35 |     dict_minfreq: 0\n",
            "21:50:35 |     dict_nulltoken: __null__\n",
            "21:50:35 |     dict_starttoken: __start__\n",
            "21:50:35 |     dict_textfields: text,labels\n",
            "21:50:35 |     dict_tokenizer: re\n",
            "21:50:35 |     dict_unktoken: __unk__\n",
            "21:50:35 |     display_examples: False\n",
            "21:50:35 |     download_path: None\n",
            "21:50:35 |     dropout: 0.1\n",
            "21:50:35 |     dynamic_batching: None\n",
            "21:50:35 |     embedding_projection: random\n",
            "21:50:35 |     embedding_type: random\n",
            "21:50:35 |     embeddingsize: 128\n",
            "21:50:35 |     eval_batchsize: None\n",
            "21:50:35 |     eval_dynamic_batching: None\n",
            "21:50:35 |     evaltask: None\n",
            "21:50:35 |     final_extra_opt: \n",
            "21:50:35 |     force_fp16_tokens: False\n",
            "21:50:35 |     fp16: False\n",
            "21:50:35 |     fp16_impl: safe\n",
            "21:50:35 |     gpu: -1\n",
            "21:50:35 |     gpu_beam_blocking: False\n",
            "21:50:35 |     gradient_clip: 0.1\n",
            "21:50:35 |     hiddensize: 128\n",
            "21:50:35 |     hide_labels: False\n",
            "21:50:35 |     history_add_global_end_token: None\n",
            "21:50:35 |     history_reversed: False\n",
            "21:50:35 |     history_size: -1\n",
            "21:50:35 |     image_cropsize: 224\n",
            "21:50:35 |     image_mode: raw\n",
            "21:50:35 |     image_size: 256\n",
            "21:50:35 |     inference: greedy\n",
            "21:50:35 |     init_model: None\n",
            "21:50:35 |     init_opt: None\n",
            "21:50:35 |     input_dropout: 0.0\n",
            "21:50:35 |     interactive_mode: False\n",
            "21:50:35 |     invsqrt_lr_decay_gamma: -1\n",
            "21:50:35 |     is_debug: False\n",
            "21:50:35 |     label_truncate: None\n",
            "21:50:35 |     learningrate: 1\n",
            "21:50:35 |     load_from_checkpoint: True\n",
            "21:50:35 |     log_every_n_secs: -1\n",
            "21:50:35 |     log_every_n_steps: 50\n",
            "21:50:35 |     log_keep_fields: all\n",
            "21:50:35 |     loglevel: info\n",
            "21:50:35 |     lookuptable: all\n",
            "21:50:35 |     lr_scheduler: reduceonplateau\n",
            "21:50:35 |     lr_scheduler_decay: 0.5\n",
            "21:50:35 |     lr_scheduler_patience: 3\n",
            "21:50:35 |     max_train_steps: -1\n",
            "21:50:35 |     max_train_time: 120.0\n",
            "21:50:35 |     metrics: default\n",
            "21:50:35 |     model: seq2seq\n",
            "21:50:35 |     model_file: from_scratch_model/model\n",
            "21:50:35 |     momentum: 0\n",
            "21:50:35 |     multitask_weights: [1]\n",
            "21:50:35 |     mutators: None\n",
            "21:50:35 |     nesterov: True\n",
            "21:50:35 |     no_cuda: False\n",
            "21:50:35 |     num_epochs: -1\n",
            "21:50:35 |     num_workers: 0\n",
            "21:50:35 |     numlayers: 2\n",
            "21:50:35 |     numsoftmax: 1\n",
            "21:50:35 |     nus: (0.7,)\n",
            "21:50:35 |     optimizer: sgd\n",
            "21:50:35 |     override: \"{'model_file': 'from_scratch_model/model', 'task': 'empathetic_dialogues', 'max_train_time': 120.0, 'batchsize': 16, 'model': 'seq2seq', 'attention': 'dot', 'lookuptable': 'all', 'truncate': 64}\"\n",
            "21:50:35 |     parlai_home: /usr/local/lib/python3.10/dist-packages\n",
            "21:50:35 |     person_tokens: False\n",
            "21:50:35 |     rank_candidates: False\n",
            "21:50:35 |     rnn_class: lstm\n",
            "21:50:35 |     save_after_valid: False\n",
            "21:50:35 |     save_every_n_secs: -1\n",
            "21:50:35 |     save_format: conversations\n",
            "21:50:35 |     seed: None\n",
            "21:50:35 |     short_final_eval: False\n",
            "21:50:35 |     skip_generation: False\n",
            "21:50:35 |     special_tok_lst: None\n",
            "21:50:35 |     split_lines: False\n",
            "21:50:35 |     starttime: Apr29_21-50\n",
            "21:50:35 |     task: empathetic_dialogues\n",
            "21:50:35 |     teacher_seed: None\n",
            "21:50:35 |     temperature: 1.0\n",
            "21:50:35 |     tensorboard_log: False\n",
            "21:50:35 |     tensorboard_logdir: None\n",
            "21:50:35 |     text_truncate: None\n",
            "21:50:35 |     topk: 10\n",
            "21:50:35 |     topp: 0.9\n",
            "21:50:35 |     train_experiencer_only: False\n",
            "21:50:35 |     truncate: 64\n",
            "21:50:35 |     update_freq: 1\n",
            "21:50:35 |     use_reply: label\n",
            "21:50:35 |     validation_cutoff: 1.0\n",
            "21:50:35 |     validation_every_n_epochs: -1\n",
            "21:50:35 |     validation_every_n_secs: -1\n",
            "21:50:35 |     validation_every_n_steps: -1\n",
            "21:50:35 |     validation_max_exs: -1\n",
            "21:50:35 |     validation_metric: accuracy\n",
            "21:50:35 |     validation_metric_mode: None\n",
            "21:50:35 |     validation_patience: 10\n",
            "21:50:35 |     validation_share_agent: False\n",
            "21:50:35 |     verbose: False\n",
            "21:50:35 |     wandb_entity: None\n",
            "21:50:35 |     wandb_log: False\n",
            "21:50:35 |     wandb_log_model: False\n",
            "21:50:35 |     wandb_name: None\n",
            "21:50:35 |     wandb_project: None\n",
            "21:50:35 |     warmup_rate: 0.0001\n",
            "21:50:35 |     warmup_updates: -1\n",
            "21:50:35 |     weight_decay: None\n",
            "21:50:35 |     world_logs: \n",
            "21:50:35 | creating task(s): empathetic_dialogues\n",
            "21:50:36 | training...\n",
            "21:50:40 | time:4s total_exs:800 total_steps:50 epochs:0.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.01     1 448.7  5075  .08875      1.968 180.9  800  1.023   .07864 16.22 9.201   1 258.4  2922  .00125     .07375 9909   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .0654         0                   50 707.1 7997 11.31\n",
            "\n",
            "21:50:44 | time:8s total_exs:1600 total_steps:100 epochs:0.02\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.94     1 442.1  5807   .0900      2.311 210.2  800  1.033   .02043 16.64 8.813   1 264.9  3479   .0075      .0825 6722   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .0965         0                  100 706.9 9286 13.14\n",
            "\n",
            "21:50:48 | time:12s total_exs:2400 total_steps:150 epochs:0.04\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.75     1 472.3  5650   .0975      2.232 191.4  800    1.1   .02043 16.14 8.544   1 257.2  3076   .0025      .0650 5138   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1139         0                  150 729.5 8726 11.97\n",
            "\n",
            "21:50:53 | time:17s total_exs:3200 total_steps:200 epochs:0.05\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.14     1 444.3  5093   .1025       2.37 183.4  800  1.138   .02039 16.02 8.444   1 255.9  2934   .0025     .02125 4649   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1167         0                  200 700.3 8027 11.47\n",
            "\n",
            "21:50:56 | time:21s total_exs:4000 total_steps:250 epochs:0.06\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.02     1 460.8  6031   .0900      2.223 209.4  800   1.15   .02046 16.29 8.242   1 259.8  3400   .0050      .0525 3796   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1297         0                  250 720.5 9431 13.09\n",
            "\n",
            "21:51:00 | time:25s total_exs:4800 total_steps:300 epochs:0.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.28     1 463.6  5967   .1013      2.301 205.9  800  1.135   .02045 17.16 8.162   1   273  3514   .0075     .09625 3507   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1361         0                  300 736.6 9481 12.88\n",
            "\n",
            "21:51:05 | time:30s total_exs:5600 total_steps:350 epochs:0.09\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.06     1 444.7  4387  .08625       2.27 157.9  800  1.166   .02042 16.27 8.036   1 259.8  2564   .0025     .03375 3090   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1378         0                  350 704.5 6951 9.868\n",
            "\n",
            "21:51:10 | time:34s total_exs:6400 total_steps:400 epochs:0.10\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.59     1 459.7  5561  .09875      2.859 193.6  800  1.159   .02045 16.72 7.946   1 265.1  3207   .0050      .1512 2825   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1428         0                  400 724.7 8768 12.1\n",
            "\n",
            "21:51:15 | time:39s total_exs:7200 total_steps:450 epochs:0.11\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.32     1 451.4  4468   .0975      2.107 158.4  800  1.212   .02044 16.21 7.865   1 258.7  2560  .00375     .04875 2604   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1439         0                  450 710.1 7028 9.899\n",
            "\n",
            "21:51:20 | time:44s total_exs:8000 total_steps:500 epochs:0.12\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.12     1   440  4105  .07375       1.62 149.3  800  1.212   .02042 15.63 7.772   1 249.1  2324  .00375     .06125 2374   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1493         0                  500 689.1 6429 9.331\n",
            "\n",
            "21:51:23 | time:47s total_exs:8800 total_steps:550 epochs:0.14\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.24     1 439.7  6880  .08375      1.762 250.3  800  1.249   .02033 15.21 7.633   1 242.1  3788  .00125      .0800 2066   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1595         0                  550 681.8 10668 15.65\n",
            "\n",
            "21:51:27 | time:51s total_exs:9600 total_steps:600 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.96     1 460.1  7119   .1000      2.197 247.5  800  1.227   .02041  16.2 7.597   1 258.9  4006  .00125     .01625 1993   \n",
            "    token_acc  token_em  total_train_updates   tpb   tps   ups  \n",
            "        .1568         0                  600 719.1 11125 15.48\n",
            "\n",
            "21:51:31 | time:55s total_exs:10400 total_steps:650 epochs:0.16\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.84     1 447.2  5633   .0925      1.894 201.5  800  1.241   .02042  16.3 7.555   1 260.6  3282  .00125     .00875 1909   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps  ups  \n",
            "        .1587         0                  650 707.8 8915 12.6\n",
            "\n",
            "21:51:35 | time:59s total_exs:11200 total_steps:700 epochs:0.17\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.56     1 461.7  4881   .1000      2.705 169.2  800  1.265   .02046 15.72 7.534   1 250.8  2651   .0050     .04375 1870   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1573         0                  700 712.5 7532 10.57\n",
            "\n",
            "21:51:39 | time:63s total_exs:12000 total_steps:750 epochs:0.19\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.21     1 455.5  6070   .0850      1.742 213.2  800   1.29   .02047 15.86 7.465   1 252.8  3369   .0050     .05875 1745   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1628         0                  750 708.3 9439 13.33\n",
            "\n",
            "21:51:43 | time:67s total_exs:12800 total_steps:800 epochs:0.20\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.02     1 458.5  6292   .0925      2.365 219.6  800  1.273   .02047 16.08 7.403   1 256.2  3516  .00625      .0675 1640   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1635         0                  800 714.6 9808 13.73\n",
            "\n",
            "21:51:47 | time:71s total_exs:13600 total_steps:850 epochs:0.21\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.57     1 471.4  5242   .1087      2.105 177.9  800  1.252   .02044 16.16  7.32   1 258.5  2874  .00125      .0100 1511   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1731         0                  850 729.9 8117 11.12\n",
            "\n",
            "21:51:51 | time:75s total_exs:14400 total_steps:900 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   29.71     1 444.3  5856   .0850      1.938 210.9  800  1.282   .02043 16.18  7.21   1 258.6  3408  .00125      .0150 1353   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1740         0                  900 702.9 9264 13.18\n",
            "\n",
            "21:51:55 | time:79s total_exs:15200 total_steps:950 epochs:0.24\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.31     1 454.7  6297   .0925      1.895 221.6  800  1.296   .02044 16.48 7.251   1   263  3643  .00375     .04625 1410   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1704         0                  950 717.7 9940 13.85\n",
            "\n",
            "21:51:58 | time:83s total_exs:16000 total_steps:1000 epochs:0.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "      30     1 452.1  6257   .0850      1.742 221.4  800  1.314   .02043  16.2 7.194   1 258.7  3581   .0025      .0250 1331   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1768         0                 1000 710.8 9837 13.85\n",
            "\n",
            "21:52:03 | time:88s total_exs:16800 total_steps:1050 epochs:0.26\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   30.67     1 446.6  4355   .1000      2.759   156  800  1.297   .02047 16.47 7.189   1 261.8  2553  .00625      .1075 1324   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1723         0                 1050 708.4 6908 9.754\n",
            "\n",
            "21:52:07 | time:91s total_exs:17600 total_steps:1100 epochs:0.27\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.95     1 460.5  6346   .1125      3.174 220.5  800  1.322   .02046 16.59 7.159   1   265  3652   .0025      .0325 1285   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1725         0                 1100 725.4 9998 13.79\n",
            "\n",
            "21:52:11 | time:95s total_exs:18400 total_steps:1150 epochs:0.28\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.62     1 468.1  5932   .0925      2.368 202.7  800  1.325   .02048 16.49 7.002   1 261.9  3319   .0075      .1175 1099   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1849         0                 1150 730.1 9251 12.67\n",
            "\n",
            "21:52:16 | time:100s total_exs:19200 total_steps:1200 epochs:0.30\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.04     1 461.3  4822   .0950      2.211 167.3  800  1.349   .02047 16.52 6.986   1 263.9  2759  .00375      .0300 1082   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1820         0                 1200 725.2 7581 10.46\n",
            "\n",
            "21:52:20 | time:104s total_exs:20000 total_steps:1250 epochs:0.31\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   33.22     1   474  5755   .1200      3.596 194.3  800  1.365   .02051  16.5 7.028   1 262.6  3188  .00625     .08875 1128   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1795         0                 1250 736.5 8942 12.14\n",
            "\n",
            "21:52:24 | time:108s total_exs:20800 total_steps:1300 epochs:0.32\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gnorm  gpu_mem  llen  loss  lr  ltpb  ltps  ltrunc  ltrunclen  ppl  \\\n",
            "   31.47     1 465.6  6082   .1037       2.37   209  800  1.314   .01928 16.78 7.002   1 268.5  3507       0          0 1099   \n",
            "    token_acc  token_em  total_train_updates   tpb  tps   ups  \n",
            "        .1778         0                 1300 734.1 9589 13.07\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8d19b1fffe63>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparlai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m TrainModel.main(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# we MUST provide a filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'from_scratch_model/model'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_kwargs\u001b[0;34m(cls, kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_from_parser_and_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/script.py\u001b[0m in \u001b[0;36m_run_from_parser_and_opt\u001b[0;34m(cls, opt, parser)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \"\"\"\n\u001b[1;32m   1009\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_train_log\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m             \u001b[0;31m# we've already done what we need in these\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/scripts/train_model.py\u001b[0m in \u001b[0;36mtrain_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    915\u001b[0m                 \u001b[0;31m# do one example / batch of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                     \u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparley\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mStopTrainException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Stopping from {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mparley\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0;31m# The agent acts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mbatch_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_observations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0;31m# We possibly execute this action in the world.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/worlds.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, agent_idx, batch_observation)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_act'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0mbatch_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_observation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Store the actions locally in each world.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworlds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/torch_agent.py\u001b[0m in \u001b[0;36mbatch_act\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m   2237\u001b[0m             \u001b[0;31m# register the start of updates for later counting when they occur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ups'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalTimerMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2239\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, batch, return_output)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot compute loss without a label.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mscore_view\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ys, prev_enc, maxlen, bsz, *xs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# use teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_forced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\u001b[0m in \u001b[0;36mdecode_forced\u001b[0;34m(self, encoder_states, ys)\u001b[0m\n\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_initial_forced_decoder_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/agents/seq2seq/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, encoder_output, incremental_state)\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m                 \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/parlai/agents/seq2seq/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xes, hidden, attn_params)\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0mhid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'concat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0;31m# concat hidden state and encoder outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai i -mf zoo:fits/bart_sq_gen/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ie22NsH7N61",
        "outputId": "55337262-de09-48e9-e5a1-f986029ad768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-29 21:32:22.541708: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-29 21:32:24.469309: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-29 21:32:27.552897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-29 21:32:27.553469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-29 21:32:27.553722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "21:32:31 | building data: /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model_v0.1.tar.gz\n",
            "21:32:31 | Downloading http://parl.ai/downloads/_models/fits/bart_sq_gen/model_v0.1.tar.gz to /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model_v0.1.tar.gz\n",
            "Downloading model_v0.1.tar.gz: 100% 750M/750M [00:15<00:00, 48.3MB/s]\n",
            "21:32:59 | \u001b[33mOverriding opt[\"model_file\"] to /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model (previously: zoo:fits/bart_sq_gen/model)\u001b[0m\n",
            "21:32:59 | Using CUDA\n",
            "21:32:59 | loading dictionary from /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model.dict\n",
            "21:32:59 | num words = 50264\n",
            "21:32:59 | Downloading https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe to /usr/local/lib/python3.10/dist-packages/data/gpt2/vocab.bpe\n",
            "Downloading vocab.bpe: 0.00B [00:00, ?B/s]\n",
            "21:32:59 | Downloading https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json to /usr/local/lib/python3.10/dist-packages/data/gpt2/encoder.json\n",
            "Downloading encoder.json: 0.00B [00:00, ?B/s]\n",
            "21:33:00 | Bart: full interactive mode on.\n",
            "21:33:07 | Total parameters: 406,286,336 (406,286,336 trainable)\n",
            "21:33:07 | Loading existing model params from /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model\n",
            "21:33:08 | Opt:\n",
            "21:33:08 |     activation: gelu\n",
            "21:33:08 |     adafactor_eps: '[1e-30, 0.001]'\n",
            "21:33:08 |     adam_eps: 1e-08\n",
            "21:33:08 |     add_p1_after_newln: False\n",
            "21:33:08 |     aggregate_micro: False\n",
            "21:33:08 |     allow_missing_init_opts: False\n",
            "21:33:08 |     attention_dropout: 0.1\n",
            "21:33:08 |     batchsize: 64\n",
            "21:33:08 |     beam_block_full_context: True\n",
            "21:33:08 |     beam_block_list_filename: None\n",
            "21:33:08 |     beam_block_ngram: -1\n",
            "21:33:08 |     beam_context_block_ngram: -1\n",
            "21:33:08 |     beam_delay: 30\n",
            "21:33:08 |     beam_length_penalty: 0.65\n",
            "21:33:08 |     beam_min_length: 1\n",
            "21:33:08 |     beam_size: 1\n",
            "21:33:08 |     betas: '[0.9, 0.999]'\n",
            "21:33:08 |     bpe_add_prefix_space: None\n",
            "21:33:08 |     bpe_debug: False\n",
            "21:33:08 |     bpe_dropout: None\n",
            "21:33:08 |     bpe_merge: None\n",
            "21:33:08 |     bpe_vocab: None\n",
            "21:33:08 |     checkpoint_activations: False\n",
            "21:33:08 |     compute_tokenized_bleu: False\n",
            "21:33:08 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "21:33:08 |     datatype: train\n",
            "21:33:08 |     delimiter: '\\n'\n",
            "21:33:08 |     dialog_history: full\n",
            "21:33:08 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:33:08 |     dict_endtoken: __end__\n",
            "21:33:08 |     dict_file: /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model.dict\n",
            "21:33:08 |     dict_include_test: False\n",
            "21:33:08 |     dict_include_valid: False\n",
            "21:33:08 |     dict_initpath: None\n",
            "21:33:08 |     dict_language: english\n",
            "21:33:08 |     dict_loaded: True\n",
            "21:33:08 |     dict_lower: False\n",
            "21:33:08 |     dict_max_ngram_size: -1\n",
            "21:33:08 |     dict_maxexs: -1\n",
            "21:33:08 |     dict_maxtokens: -1\n",
            "21:33:08 |     dict_minfreq: 0\n",
            "21:33:08 |     dict_nulltoken: __null__\n",
            "21:33:08 |     dict_starttoken: __start__\n",
            "21:33:08 |     dict_textfields: text,labels\n",
            "21:33:08 |     dict_tokenizer: gpt2\n",
            "21:33:08 |     dict_unktoken: __unk__\n",
            "21:33:08 |     display_add_fields: \n",
            "21:33:08 |     display_examples: False\n",
            "21:33:08 |     display_prettify: False\n",
            "21:33:08 |     download_path: None\n",
            "21:33:08 |     dropout: 0.1\n",
            "21:33:08 |     dynamic_batching: None\n",
            "21:33:08 |     embedding_projection: random\n",
            "21:33:08 |     embedding_size: 1024\n",
            "21:33:08 |     embedding_type: random\n",
            "21:33:08 |     embeddings_scale: False\n",
            "21:33:08 |     eval_batchsize: 128\n",
            "21:33:08 |     eval_dynamic_batching: None\n",
            "21:33:08 |     evaltask: None\n",
            "21:33:08 |     ffn_size: 4096\n",
            "21:33:08 |     final_extra_opt: \n",
            "21:33:08 |     force_fp16_tokens: True\n",
            "21:33:08 |     fp16: True\n",
            "21:33:08 |     fp16_impl: safe\n",
            "21:33:08 |     gpu: -1\n",
            "21:33:08 |     gpu_beam_blocking: False\n",
            "21:33:08 |     gradient_clip: 0.1\n",
            "21:33:08 |     hide_labels: False\n",
            "21:33:08 |     history_add_global_end_token: None\n",
            "21:33:08 |     history_reversed: False\n",
            "21:33:08 |     history_size: -1\n",
            "21:33:08 |     image_cropsize: 224\n",
            "21:33:08 |     image_mode: raw\n",
            "21:33:08 |     image_size: 256\n",
            "21:33:08 |     include_persona: True\n",
            "21:33:08 |     inference: greedy\n",
            "21:33:08 |     init_fairseq_model: None\n",
            "21:33:08 |     init_model: /private/home/jingxu23/ParlAI/data/models/bart/bart_large/model\n",
            "21:33:08 |     init_opt: None\n",
            "21:33:08 |     interactive_mode: True\n",
            "21:33:08 |     interactive_task: True\n",
            "21:33:08 |     invsqrt_lr_decay_gamma: -1\n",
            "21:33:08 |     is_debug: False\n",
            "21:33:08 |     label_truncate: 128\n",
            "21:33:08 |     learn_positional_embeddings: True\n",
            "21:33:08 |     learningrate: 1e-06\n",
            "21:33:08 |     local_human_candidates_file: None\n",
            "21:33:08 |     log_every_n_secs: -1\n",
            "21:33:08 |     log_every_n_steps: 20\n",
            "21:33:08 |     log_keep_fields: all\n",
            "21:33:08 |     loglevel: info\n",
            "21:33:08 |     lr_scheduler: reduceonplateau\n",
            "21:33:08 |     lr_scheduler_decay: 0.5\n",
            "21:33:08 |     lr_scheduler_patience: 3\n",
            "21:33:08 |     max_train_steps: 10000\n",
            "21:33:08 |     max_train_time: 84672.0\n",
            "21:33:08 |     metrics: default\n",
            "21:33:08 |     metrics_per_bot_nickname: None\n",
            "21:33:08 |     metrics_per_feedback: None\n",
            "21:33:08 |     model: bart\n",
            "21:33:08 |     model_file: /usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model\n",
            "21:33:08 |     model_parallel: True\n",
            "21:33:08 |     momentum: 0\n",
            "21:33:08 |     multitask_weights: '[3.0, 1.0, 1.0, 0.0, 4.0]'\n",
            "21:33:08 |     mutators: None\n",
            "21:33:08 |     n_decoder_layers: 12\n",
            "21:33:08 |     n_encoder_layers: 12\n",
            "21:33:08 |     n_heads: 16\n",
            "21:33:08 |     n_layers: 2\n",
            "21:33:08 |     n_positions: 1024\n",
            "21:33:08 |     n_segments: 0\n",
            "21:33:08 |     nesterov: True\n",
            "21:33:08 |     no_cuda: False\n",
            "21:33:08 |     num_epochs: -1\n",
            "21:33:08 |     num_workers: 0\n",
            "21:33:08 |     nus: [0.7]\n",
            "21:33:08 |     only_last_search_query: False\n",
            "21:33:08 |     optimizer: adam\n",
            "21:33:08 |     outfile: \n",
            "21:33:08 |     output_conversion_path: None\n",
            "21:33:08 |     output_scaling: 1.0\n",
            "21:33:08 |     override: \"{'model_file': '/usr/local/lib/python3.10/dist-packages/data/models/fits/bart_sq_gen/model'}\"\n",
            "21:33:08 |     parlai_home: /checkpoint/jingxu23/projects/continual_learning/sweeps/sqgen_bart_sweep1_20220309/ParlAI\n",
            "21:33:08 |     person_tokens: False\n",
            "21:33:08 |     query_source: human_gold\n",
            "21:33:08 |     rank_candidates: False\n",
            "21:33:08 |     relu_dropout: 0.0\n",
            "21:33:08 |     save_after_valid: True\n",
            "21:33:08 |     save_every_n_secs: -1\n",
            "21:33:08 |     save_format: conversations\n",
            "21:33:08 |     share_word_embeddings: True\n",
            "21:33:08 |     short_final_eval: False\n",
            "21:33:08 |     single_turn: False\n",
            "21:33:08 |     skip_empty_text: True\n",
            "21:33:08 |     skip_generation: False\n",
            "21:33:08 |     special_tok_lst: None\n",
            "21:33:08 |     split_lines: False\n",
            "21:33:08 |     starttime: Mar09_12-05\n",
            "21:33:08 |     task: None\n",
            "21:33:08 |     temperature: 1.0\n",
            "21:33:08 |     tensorboard_log: False\n",
            "21:33:08 |     tensorboard_logdir: None\n",
            "21:33:08 |     text_truncate: 512\n",
            "21:33:08 |     topk: 10\n",
            "21:33:08 |     topp: 0.9\n",
            "21:33:08 |     truncate: -1\n",
            "21:33:08 |     update_freq: 2\n",
            "21:33:08 |     use_reply: label\n",
            "21:33:08 |     validation_cutoff: 1.0\n",
            "21:33:08 |     validation_every_n_epochs: -1\n",
            "21:33:08 |     validation_every_n_secs: -1\n",
            "21:33:08 |     validation_every_n_steps: 100\n",
            "21:33:08 |     validation_max_exs: -1\n",
            "21:33:08 |     validation_metric: ppl\n",
            "21:33:08 |     validation_metric_mode: min\n",
            "21:33:08 |     validation_patience: 10\n",
            "21:33:08 |     validation_share_agent: False\n",
            "21:33:08 |     variant: bart\n",
            "21:33:08 |     verbose: False\n",
            "21:33:08 |     wandb_entity: None\n",
            "21:33:08 |     wandb_log: True\n",
            "21:33:08 |     wandb_name: 644.job_3\n",
            "21:33:08 |     wandb_project: sqgen_bart_sweep1\n",
            "21:33:08 |     warmup_rate: 0.0001\n",
            "21:33:08 |     warmup_updates: 100\n",
            "21:33:08 |     weight_decay: None\n",
            "21:33:08 |     world_logs: \n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "21:33:08 | creating task(s): interactive\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m how can I reach univesity in buffalo new york for best study and fun\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/parlai\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/__main__.py\", line 14, in main\n",
            "    superscript_main()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/script.py\", line 325, in superscript_main\n",
            "    return SCRIPT_REGISTRY[cmd].klass._run_from_parser_and_opt(opt, parser)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/script.py\", line 108, in _run_from_parser_and_opt\n",
            "    return script.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/scripts/interactive.py\", line 118, in run\n",
            "    return interactive(self.opt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/scripts/interactive.py\", line 93, in interactive\n",
            "    world.parley()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/tasks/interactive/worlds.py\", line 89, in parley\n",
            "    acts[1] = agents[1].act()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/torch_agent.py\", line 2148, in act\n",
            "    response = self.batch_act([self.observation])[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/torch_agent.py\", line 2244, in batch_act\n",
            "    output = self.eval_step(batch)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\", line 901, in eval_step\n",
            "    beam_preds_scores, beams = self._generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\", line 1223, in _generate\n",
            "    b.advance(score[i], _ts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/parlai/core/torch_generator_agent.py\", line 1599, in advance\n",
            "    self.partial_hyps[path_selection.hypothesis_ids.long()],\n",
            "RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvA77Zwkoviq"
      },
      "source": [
        "Our perplexity and F1 (word overlap) scores are pretty bad, and our BLEU-4 score is nearly 0. That's okay, we would normally want to train for well over an hour. Feel free to change the max_train_time above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QTiTn7aoxv9"
      },
      "source": [
        "## Performance is pretty bad there. Can we improve it?\n",
        "\n",
        "The easiest way to improve it is to *initialize* using a *pretrained model*, utilizing *transfer learning*. Let's use the one from the interactive session at the beginning of the chat!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Jt9bHTn1dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c170b881-f11d-458e-90ac-b2dcf17df5ab"
      },
      "source": [
        "!rm -rf from_pretrained\n",
        "!mkdir -p from_pretrained\n",
        "\n",
        "TrainModel.main(\n",
        "    # similar to before\n",
        "    task='empathetic_dialogues', \n",
        "    model='transformer/generator',\n",
        "    model_file='from_pretrained/model',\n",
        "    \n",
        "    # initialize with a pretrained model\n",
        "    init_model='zoo:tutorial_transformer_generator/model',\n",
        "    \n",
        "    # arguments we get from the pretrained model.\n",
        "    # Unfortunately, these must be looked up separately for each model.\n",
        "    n_heads=16, n_layers=8, n_positions=512, text_truncate=512,\n",
        "    label_truncate=128, ffn_size=2048, embedding_size=512,\n",
        "    activation='gelu', variant='xlm',\n",
        "    dict_lower=True, dict_tokenizer='bpe',\n",
        "    dict_file='zoo:tutorial_transformer_generator/model.dict',\n",
        "    learn_positional_embeddings=True,\n",
        "    \n",
        "    # some training arguments, specific to this fine-tuning\n",
        "    # use a small learning rate with ADAM optimizer\n",
        "    lr=1e-5, optimizer='adam',\n",
        "    warmup_updates=100,\n",
        "    # early stopping on perplexity\n",
        "    validation_metric='ppl',\n",
        "    # train at most 10 minutes, and validate every 0.25 epochs\n",
        "    max_train_time=600, validation_every_n_epochs=0.25,\n",
        "    \n",
        "    # depend on your gpu. If you have a V100, this is good\n",
        "    batchsize=12, fp16=True, fp16_impl='mem_efficient',\n",
        "    \n",
        "    # speeds up validation\n",
        "    skip_generation=True,\n",
        "    \n",
        "    # helps us cram more examples into our gpu at a time\n",
        "    dynamic_batching='full',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21:52:35 | building dictionary first...\n",
            "21:52:35 | No model with opt yet at: from_pretrained/model(.opt)\n",
            "21:52:35 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,download_path: None,loglevel: info,dynamic_batching: full,verbose: False,is_debug: False,datapath: /usr/local/lib/python3.10/dist-packages/data,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,load_from_checkpoint: True,world_logs: ,save_format: conversations,seed: None,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,wandb_log_model: False,teacher_seed: None,mutators: None,train_experiencer_only: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,gpu_beam_blocking: False,interactive_mode: False,fp16_impl: mem_efficient,force_fp16_tokens: False,adafactor_eps: (1e-30, 0.001),history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.10/dist-packages\u001b[0m\n",
            "21:52:35 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "21:52:35 | Using CUDA\n",
            "21:52:35 | loading dictionary from /usr/local/lib/python3.10/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "21:52:35 | num words = 54944\n",
            "21:52:36 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "21:52:36 | Loading existing model params from /usr/local/lib/python3.10/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "21:52:42 | \u001b[33mDetected a fine-tune run. Resetting the optimizer.\u001b[0m\n",
            "21:52:42 | \u001b[33mOptimizer was reset. Also resetting LR scheduler.\u001b[0m\n",
            "21:52:42 | Opt:\n",
            "21:52:42 |     activation: gelu\n",
            "21:52:42 |     adafactor_eps: '(1e-30, 0.001)'\n",
            "21:52:42 |     adam_eps: 1e-08\n",
            "21:52:42 |     add_p1_after_newln: False\n",
            "21:52:42 |     aggregate_micro: False\n",
            "21:52:42 |     allow_missing_init_opts: False\n",
            "21:52:42 |     attention_dropout: 0.0\n",
            "21:52:42 |     batchsize: 12\n",
            "21:52:42 |     beam_block_full_context: True\n",
            "21:52:42 |     beam_block_list_filename: None\n",
            "21:52:42 |     beam_block_ngram: -1\n",
            "21:52:42 |     beam_context_block_ngram: -1\n",
            "21:52:42 |     beam_delay: 30\n",
            "21:52:42 |     beam_length_penalty: 0.65\n",
            "21:52:42 |     beam_min_length: 1\n",
            "21:52:42 |     beam_size: 1\n",
            "21:52:42 |     betas: '(0.9, 0.999)'\n",
            "21:52:42 |     bpe_add_prefix_space: None\n",
            "21:52:42 |     bpe_debug: False\n",
            "21:52:42 |     bpe_dropout: None\n",
            "21:52:42 |     bpe_merge: None\n",
            "21:52:42 |     bpe_vocab: None\n",
            "21:52:42 |     checkpoint_activations: False\n",
            "21:52:42 |     compute_tokenized_bleu: False\n",
            "21:52:42 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "21:52:42 |     datatype: train\n",
            "21:52:42 |     delimiter: '\\n'\n",
            "21:52:42 |     dict_class: parlai.core.dict:DictionaryAgent\n",
            "21:52:42 |     dict_endtoken: __end__\n",
            "21:52:42 |     dict_file: /usr/local/lib/python3.10/dist-packages/data/models/tutorial_transformer_generator/model.dict\n",
            "21:52:42 |     dict_include_test: False\n",
            "21:52:42 |     dict_include_valid: False\n",
            "21:52:42 |     dict_initpath: None\n",
            "21:52:42 |     dict_language: english\n",
            "21:52:42 |     dict_loaded: True\n",
            "21:52:42 |     dict_lower: True\n",
            "21:52:42 |     dict_max_ngram_size: -1\n",
            "21:52:42 |     dict_maxexs: -1\n",
            "21:52:42 |     dict_maxtokens: -1\n",
            "21:52:42 |     dict_minfreq: 0\n",
            "21:52:42 |     dict_nulltoken: __null__\n",
            "21:52:42 |     dict_starttoken: __start__\n",
            "21:52:42 |     dict_textfields: text,labels\n",
            "21:52:42 |     dict_tokenizer: bpe\n",
            "21:52:42 |     dict_unktoken: __unk__\n",
            "21:52:42 |     display_examples: False\n",
            "21:52:42 |     download_path: None\n",
            "21:52:42 |     dropout: 0.0\n",
            "21:52:42 |     dynamic_batching: full\n",
            "21:52:42 |     embedding_projection: random\n",
            "21:52:42 |     embedding_size: 512\n",
            "21:52:42 |     embedding_type: random\n",
            "21:52:42 |     embeddings_scale: True\n",
            "21:52:42 |     eval_batchsize: None\n",
            "21:52:42 |     eval_dynamic_batching: None\n",
            "21:52:42 |     evaltask: None\n",
            "21:52:42 |     ffn_size: 2048\n",
            "21:52:42 |     final_extra_opt: \n",
            "21:52:42 |     force_fp16_tokens: False\n",
            "21:52:42 |     fp16: True\n",
            "21:52:42 |     fp16_impl: mem_efficient\n",
            "21:52:42 |     gpu: -1\n",
            "21:52:42 |     gpu_beam_blocking: False\n",
            "21:52:42 |     gradient_clip: 0.1\n",
            "21:52:42 |     hide_labels: False\n",
            "21:52:42 |     history_add_global_end_token: None\n",
            "21:52:42 |     history_reversed: False\n",
            "21:52:42 |     history_size: -1\n",
            "21:52:42 |     image_cropsize: 224\n",
            "21:52:42 |     image_mode: raw\n",
            "21:52:42 |     image_size: 256\n",
            "21:52:42 |     inference: greedy\n",
            "21:52:42 |     init_model: /usr/local/lib/python3.10/dist-packages/data/models/tutorial_transformer_generator/model\n",
            "21:52:42 |     init_opt: None\n",
            "21:52:42 |     interactive_mode: False\n",
            "21:52:42 |     invsqrt_lr_decay_gamma: -1\n",
            "21:52:42 |     is_debug: False\n",
            "21:52:42 |     label_truncate: 128\n",
            "21:52:42 |     learn_positional_embeddings: True\n",
            "21:52:42 |     learningrate: 1e-05\n",
            "21:52:42 |     load_from_checkpoint: True\n",
            "21:52:42 |     log_every_n_secs: -1\n",
            "21:52:42 |     log_every_n_steps: 50\n",
            "21:52:42 |     log_keep_fields: all\n",
            "21:52:42 |     loglevel: info\n",
            "21:52:42 |     lr_scheduler: reduceonplateau\n",
            "21:52:42 |     lr_scheduler_decay: 0.5\n",
            "21:52:42 |     lr_scheduler_patience: 3\n",
            "21:52:42 |     max_train_steps: -1\n",
            "21:52:42 |     max_train_time: 600.0\n",
            "21:52:42 |     metrics: default\n",
            "21:52:42 |     model: transformer/generator\n",
            "21:52:42 |     model_file: from_pretrained/model\n",
            "21:52:42 |     model_parallel: False\n",
            "21:52:42 |     momentum: 0\n",
            "21:52:42 |     multitask_weights: [1]\n",
            "21:52:42 |     mutators: None\n",
            "21:52:42 |     n_decoder_layers: -1\n",
            "21:52:42 |     n_encoder_layers: -1\n",
            "21:52:42 |     n_heads: 16\n",
            "21:52:42 |     n_layers: 8\n",
            "21:52:42 |     n_positions: 512\n",
            "21:52:42 |     n_segments: 0\n",
            "21:52:42 |     nesterov: True\n",
            "21:52:42 |     no_cuda: False\n",
            "21:52:42 |     num_epochs: -1\n",
            "21:52:42 |     num_workers: 0\n",
            "21:52:42 |     nus: (0.7,)\n",
            "21:52:42 |     optimizer: mem_eff_adam\n",
            "21:52:42 |     output_scaling: 1.0\n",
            "21:52:42 |     override: \"{'task': 'empathetic_dialogues', 'model': 'transformer/generator', 'model_file': 'from_pretrained/model', 'init_model': 'zoo:tutorial_transformer_generator/model', 'n_heads': 16, 'n_layers': 8, 'n_positions': 512, 'text_truncate': 512, 'label_truncate': 128, 'ffn_size': 2048, 'embedding_size': 512, 'activation': 'gelu', 'variant': 'xlm', 'dict_lower': True, 'dict_tokenizer': 'bpe', 'dict_file': '/usr/local/lib/python3.10/dist-packages/data/models/tutorial_transformer_generator/model.dict', 'learn_positional_embeddings': True, 'learningrate': 1e-05, 'optimizer': 'adam', 'warmup_updates': 100, 'validation_metric': 'ppl', 'max_train_time': 600.0, 'validation_every_n_epochs': 0.25, 'batchsize': 12, 'fp16': True, 'fp16_impl': 'mem_efficient', 'skip_generation': True, 'dynamic_batching': 'full'}\"\n",
            "21:52:42 |     parlai_home: /usr/local/lib/python3.10/dist-packages\n",
            "21:52:42 |     person_tokens: False\n",
            "21:52:42 |     rank_candidates: False\n",
            "21:52:42 |     relu_dropout: 0.0\n",
            "21:52:42 |     save_after_valid: False\n",
            "21:52:42 |     save_every_n_secs: -1\n",
            "21:52:42 |     save_format: conversations\n",
            "21:52:42 |     seed: None\n",
            "21:52:42 |     share_word_embeddings: True\n",
            "21:52:42 |     short_final_eval: False\n",
            "21:52:42 |     skip_generation: True\n",
            "21:52:42 |     special_tok_lst: None\n",
            "21:52:42 |     split_lines: False\n",
            "21:52:42 |     starttime: Apr29_21-52\n",
            "21:52:42 |     task: empathetic_dialogues\n",
            "21:52:42 |     teacher_seed: None\n",
            "21:52:42 |     temperature: 1.0\n",
            "21:52:42 |     tensorboard_log: False\n",
            "21:52:42 |     tensorboard_logdir: None\n",
            "21:52:42 |     text_truncate: 512\n",
            "21:52:42 |     topk: 10\n",
            "21:52:42 |     topp: 0.9\n",
            "21:52:42 |     train_experiencer_only: False\n",
            "21:52:42 |     truncate: -1\n",
            "21:52:42 |     update_freq: 1\n",
            "21:52:42 |     use_reply: label\n",
            "21:52:42 |     validation_cutoff: 1.0\n",
            "21:52:42 |     validation_every_n_epochs: 0.25\n",
            "21:52:42 |     validation_every_n_secs: -1\n",
            "21:52:42 |     validation_every_n_steps: -1\n",
            "21:52:42 |     validation_max_exs: -1\n",
            "21:52:42 |     validation_metric: ppl\n",
            "21:52:42 |     validation_metric_mode: None\n",
            "21:52:42 |     validation_patience: 10\n",
            "21:52:42 |     validation_share_agent: False\n",
            "21:52:42 |     variant: xlm\n",
            "21:52:42 |     verbose: False\n",
            "21:52:42 |     wandb_entity: None\n",
            "21:52:42 |     wandb_log: False\n",
            "21:52:42 |     wandb_log_model: False\n",
            "21:52:42 |     wandb_name: None\n",
            "21:52:42 |     wandb_project: None\n",
            "21:52:42 |     warmup_rate: 0.0001\n",
            "21:52:42 |     warmup_updates: 100\n",
            "21:52:42 |     weight_decay: None\n",
            "21:52:42 |     world_logs: \n",
            "21:52:42 | creating task(s): empathetic_dialogues\n",
            "21:52:45 | training...\n",
            "21:52:46 | Overflow: setting loss scale to 65536.0\n",
            "21:52:48 | Overflow: setting loss scale to 32768.0\n",
            "21:53:00 | time:14s total_exs:5204 total_steps:50 epochs:0.08\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss        lr  ltpb  ltps  \\\n",
            "   26.32 .9600  2740  9728       0          0 369.6 5204             40632  4.111    .4532 16.22 2.858 5.001e-06  1688  5995   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0001922   .0001922 17.42      .3944  .0009608                   50 4428 15723 3.552\n",
            "\n",
            "21:53:13 | time:28s total_exs:9884 total_steps:100 epochs:0.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.84     1  2981 10950       0          0 343.9 4680             32768  4.333    .4532 16.63 2.811 9.9e-06  1556  5717   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 16.62      .4001  .0002137                  100 4537 16667 3.676\n",
            "\n",
            "21:53:26 | Overflow: setting loss scale to 16384.0\n",
            "21:53:28 | time:42s total_exs:14536 total_steps:150 epochs:0.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    32.4 .9800  3015 10667       0          0 329.2 4652             30802   4.18    .4291 16.55 2.731 9.9e-06  1540  5448   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 15.34      .4073   .000215                  150 4554 16115 3.54\n",
            "\n",
            "21:53:33 | time:48s total_exs:16180 total_steps:167 epochs:0.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   28.36     1  2742  8510       0          0 300.1 1644             16384  3.843    .4302 18.97  2.75 9.9e-06  1834  5692   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 15.64      .4055         0                  167 4576 14202 3.105\n",
            "\n",
            "21:53:33 | creating task(s): empathetic_dialogues\n",
            "21:53:35 | running eval: valid\n",
            "21:53:41 | eval completed in 6.65s\n",
            "21:53:41 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 36199       0          0 894.9 5738    .0916 16.01 2.536 9.9e-06  1371 14328       0          0 12.62   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4315   .001046                  167 4835 50527\n",
            "\u001b[0m\n",
            "21:53:41 | \u001b[1;32mnew best ppl: 12.62\u001b[0m\n",
            "21:53:41 | saving best valid model: from_pretrained/model\n",
            "21:53:41 | Saving dictionary to from_pretrained/model.dict\n",
            "21:53:51 | Overflow: setting loss scale to 8192.0\n",
            "21:54:01 | time:76s total_exs:20928 total_steps:217 epochs:0.32\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.24 .9800  3062  9899       0          0   307 4748             10650  4.088    .4489 16.34 2.703 9.9e-06  1552  5017   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.92      .4125  .0004212                  217 4613 14916 3.234\n",
            "\n",
            "21:54:16 | time:90s total_exs:25756 total_steps:267 epochs:0.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.48     1  3040 10754       0          0 341.6 4828              8192  4.168    .4348 16.51 2.674 9.9e-06  1594  5640   \n",
            "     ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002071    .001657 14.5      .4140  .0002071                  267 4634 16394 3.539\n",
            "\n",
            "21:54:31 | time:106s total_exs:30396 total_steps:317 epochs:0.47\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.89     1  2959  9712       0          0 304.6 4640              8192   4.06    .4460 17.57 2.688 9.9e-06  1630  5351   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.71      .4159   .000431                  317 4590 15063 3.283\n",
            "\n",
            "21:54:36 | time:111s total_exs:32372 total_steps:337 epochs:0.50\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "      28     1  2766 10169       0          0 363.2 1976              8192  4.167    .4378 15.99 2.681 9.9e-06  1580  5810   \n",
            "    ltrunc  ltrunclen  ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 14.6      .4117  .0005061                  337 4346 15978 3.68\n",
            "\n",
            "21:54:36 | running eval: valid\n",
            "21:54:43 | eval completed in 6.83s\n",
            "21:54:43 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 35319       0          0 873.1 5738   .09165 16.01 2.503 9.9e-06  1413 13979       0          0 12.22   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4362   .001046                  337 4984 49298\n",
            "\u001b[0m\n",
            "21:54:43 | \u001b[1;32mnew best ppl: 12.22 (previous best was 12.62)\u001b[0m\n",
            "21:54:43 | saving best valid model: from_pretrained/model\n",
            "21:55:03 | time:138s total_exs:37248 total_steps:387 epochs:0.58\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   28.41     1  2771  9477       0          0 333.5 4876              8192    4.3    .4539 16.43  2.67 9.9e-06  1602  5480   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.44      .4149         0                  387 4373 14957 3.421\n",
            "\n",
            "21:55:18 | time:152s total_exs:41860 total_steps:437 epochs:0.65\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.49     1  2996 10433       0          0 321.2 4612              8192  4.305    .4433 17.12 2.668 9.9e-06  1579  5498   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.42      .4170  .0004337                  437 4576 15931 3.484\n",
            "\n",
            "21:55:32 | time:167s total_exs:46524 total_steps:487 epochs:0.72\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.66     1  2953 10028       0          0 316.8 4664              8192  4.247    .4303 16.77 2.668 9.9e-06  1564  5312   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002144    .003859 14.41      .4156         0                  487 4517 15340 3.397\n",
            "\n",
            "21:55:38 | time:173s total_exs:48576 total_steps:509 epochs:0.75\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.19     1  2816 10238       0          0 339.1 2052              8192  4.384    .4601  16.8 2.677 9.9e-06  1567  5699   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.55      .4127         0                  509 4383 15937 3.641\n",
            "\n",
            "21:55:38 | running eval: valid\n",
            "21:55:46 | eval completed in 7.28s\n",
            "21:55:46 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3627 33522       0          0 828.7 5738   .09175 16.01 2.488 9.9e-06  1435 13268       0          0 12.04   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4386   .001046                  509 5062 46790\n",
            "\u001b[0m\n",
            "21:55:46 | \u001b[1;32mnew best ppl: 12.04 (previous best was 12.22)\u001b[0m\n",
            "21:55:46 | saving best valid model: from_pretrained/model\n",
            "21:56:05 | time:200s total_exs:53304 total_steps:559 epochs:0.82\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.03     1  3029 10114       0          0 315.8 4728              8192  4.313    .4512 16.84 2.654 9.9e-06  1592  5317   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.21      .4161         0                  559 4621 15431 3.342\n",
            "\n",
            "21:56:20 | time:215s total_exs:57968 total_steps:609 epochs:0.90\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.77     1  2963  9905       0          0 311.8 4664              8192  4.579    .4601 16.79 2.659 9.9e-06  1566  5235   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0004288    .003002 14.29      .4171  .0008576                  609 4529 15140 3.344\n",
            "\n",
            "21:56:37 | time:232s total_exs:62800 total_steps:659 epochs:0.97\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.32     1  3124  9533       0          0 294.9 4832              8192  4.168    .4433 16.43  2.64 9.9e-06  1588  4846   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.01      .4202         0                  659 4711 14379 3.053\n",
            "\n",
            "21:56:44 | time:239s total_exs:64772 total_steps:681 epochs:1.00\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.82     1  3032  9601       0          0 283.9 1972              8192   4.13    .4305 17.63  2.65 9.9e-06  1580  5003   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "   .001014    .004564 14.15      .4158  .0005071                  681 4611 14604 3.17\n",
            "\n",
            "21:56:44 | running eval: valid\n",
            "21:56:50 | eval completed in 6.27s\n",
            "21:56:50 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 39354       0          0 972.9 5738    .0922 16.01 2.478 9.9e-06  1392 15576       0          0 11.92   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4409  .0008714                  681 4909 54930\n",
            "\u001b[0m\n",
            "21:56:50 | \u001b[1;32mnew best ppl: 11.92 (previous best was 12.04)\u001b[0m\n",
            "21:56:50 | saving best valid model: from_pretrained/model\n",
            "21:57:10 | time:265s total_exs:69300 total_steps:731 epochs:1.07\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.31     1  2926  9748       0          0 301.7 4528              8192  4.302    .4490  17.1 2.634 9.9e-06  1549  5159   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.93      .4202  .0004417                  731 4475 14907 3.333\n",
            "\n",
            "21:57:25 | time:280s total_exs:74008 total_steps:781 epochs:1.14\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.12     1  2931  9861       0          0 316.8 4708              8192  4.094    .4523 16.96 2.634 9.9e-06  1597  5372   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002124    .001274 13.93      .4202   .001062                  781 4527 15233 3.366\n",
            "\n",
            "21:57:39 | time:294s total_exs:78616 total_steps:831 epochs:1.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.09     1  2958 10272       0          0 320.1 4608              8192  4.148    .4444 16.63 2.621 9.9e-06  1532  5322   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .000217    .003906 13.75      .4224  .0008681                  831 4490 15594 3.476\n",
            "\n",
            "21:57:47 | time:302s total_exs:81024 total_steps:855 epochs:1.25\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   28.45     1  2854  8958       0          0 314.9 2408              8192  4.284    .4512 16.17 2.642 9.9e-06  1622  5091   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 14.05      .4169         0                  855 4477 14049 3.141\n",
            "\n",
            "21:57:47 | running eval: valid\n",
            "21:57:53 | eval completed in 6.28s\n",
            "21:57:53 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3413 38365       0          0 948.4 5738   .09172 16.01 2.471 9.9e-06  1351 15185       0          0 11.84   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4411   .001046                  855 4764 53549\n",
            "\u001b[0m\n",
            "21:57:53 | \u001b[1;32mnew best ppl: 11.84 (previous best was 11.92)\u001b[0m\n",
            "21:57:53 | saving best valid model: from_pretrained/model\n",
            "21:58:13 | time:328s total_exs:85608 total_steps:905 epochs:1.32\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.37     1  2968  9906       0          0   306 4584              8192  4.145    .4532 16.97 2.637 9.9e-06  1556  5193   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.97      .4207         0                  905 4524 15099 3.339\n",
            "\n",
            "21:58:28 | time:343s total_exs:90308 total_steps:955 epochs:1.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.98     1  2913  9763       0          0 315.1 4700              8192   4.18    .4404 16.69 2.638 9.9e-06  1569  5260   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.98      .4197  .0002128                  955 4482 15023 3.353\n",
            "\n",
            "21:58:42 | time:357s total_exs:95020 total_steps:1005 epochs:1.47\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.63     1  2981 10515       0          0 332.5 4712              8192  4.212    .4462 17.16 2.628 9.9e-06  1617  5705   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.84      .4154  .0004244                 1005 4598 16220 3.529\n",
            "\n",
            "21:58:49 | time:363s total_exs:97204 total_steps:1026 epochs:1.50\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   28.66     1  2980 10034       0          0 350.1 2184              8192  4.001    .4310 15.47 2.579 9.9e-06  1609  5417   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 13.18      .4255   .001832                 1026 4589 15451 3.37\n",
            "\n",
            "21:58:49 | running eval: valid\n",
            "21:58:55 | eval completed in 6.32s\n",
            "21:58:55 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 38165       0          0 943.5 5738   .09178 16.01 2.465 9.9e-06  1413 15106       0          0 11.76   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4418   .001046                 1026 4984 53271\n",
            "\u001b[0m\n",
            "21:58:55 | \u001b[1;32mnew best ppl: 11.76 (previous best was 11.84)\u001b[0m\n",
            "21:58:55 | saving best valid model: from_pretrained/model\n",
            "21:59:16 | time:390s total_exs:101876 total_steps:1076 epochs:1.58\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.72     1  2871  9064       0          0   295 4672              8192  4.138    .4490 16.68 2.634 9.9e-06  1559  4922   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.93      .4216  .0004281                 1076 4429 13985 3.158\n",
            "\n",
            "21:59:31 | time:406s total_exs:106340 total_steps:1126 epochs:1.65\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.66     1  2916  9642       0          0 295.2 4464              8192  4.304    .4311 17.21 2.607 9.9e-06  1536  5081   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.56      .4214         0                 1126 4452 14723 3.308\n",
            "\n",
            "21:59:45 | time:420s total_exs:110952 total_steps:1176 epochs:1.72\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.26     1  3068 10636       0          0 319.8 4612              8192  4.217    .4302 16.84 2.609 9.9e-06  1553  5385   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.59      .4232  .0006505                 1176 4621 16021 3.469\n",
            "\n",
            "21:59:51 | time:426s total_exs:113400 total_steps:1199 epochs:1.75\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    28.7     1  3055 11372       0          0 396.2 2448              8192   4.66    .4147 15.13 2.555 9.9e-06  1611  5996   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 12.88      .4302   .000817                 1199 4665 17368 3.725\n",
            "\n",
            "21:59:51 | running eval: valid\n",
            "21:59:59 | eval completed in 7.70s\n",
            "21:59:59 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3413 32630       0          0 806.7 5738    .0914 16.01 2.459 9.9e-06  1351 12915       0          0 11.69   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4425   .001046                 1199 4764 45545\n",
            "\u001b[0m\n",
            "21:59:59 | \u001b[1;32mnew best ppl: 11.69 (previous best was 11.76)\u001b[0m\n",
            "21:59:59 | saving best valid model: from_pretrained/model\n",
            "22:00:18 | time:453s total_exs:118124 total_steps:1249 epochs:1.83\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.51     1  2977 10209       0          0   324 4724              8192  4.363    .4601 17.03   2.6 9.9e-06  1609  5519   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "         0          0 13.46      .4246  .0002117                 1249 4586 15728 3.43\n",
            "\n",
            "22:00:34 | time:468s total_exs:123068 total_steps:1299 epochs:1.90\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   29.85     1  2951  9789       0          0 327.9 4944              8192  4.092    .4532 16.74 2.621 9.9e-06  1655  5489   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002023    .001214 13.75      .4224  .0004045                 1299 4606 15278 3.318\n",
            "\n",
            "22:00:48 | time:483s total_exs:127544 total_steps:1349 epochs:1.97\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.93     1  2948 10270       0          0 311.9 4476              8192  4.445    .4601 16.28 2.568 9.9e-06  1457  5078   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.04      .4290  .0006702                 1349 4405 15349 3.485\n",
            "\n",
            "22:00:55 | time:489s total_exs:129624 total_steps:1371 epochs:2.01\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   32.98     1  3118 10542       0          0 319.7 2080              8192  4.571    .4434 16.53 2.582 9.9e-06  1562  5283   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0004808   .0004808 13.22      .4262         0                 1371 4680 15825 3.387\n",
            "\n",
            "22:00:55 | running eval: valid\n",
            "22:01:01 | eval completed in 6.48s\n",
            "22:01:01 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3517 38059       0          0 940.8 5738   .09212 16.01 2.454 9.9e-06  1392 15064       0          0 11.63   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4435    .00122                 1371 4909 53123\n",
            "\u001b[0m\n",
            "22:01:01 | \u001b[1;32mnew best ppl: 11.63 (previous best was 11.69)\u001b[0m\n",
            "22:01:01 | saving best valid model: from_pretrained/model\n",
            "22:01:20 | time:515s total_exs:134256 total_steps:1421 epochs:2.08\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "    30.8     1  2853 10133       0          0   329 4632              8192    4.3    .4490 16.92 2.583 9.9e-06  1567  5566   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.24      .4261  .0006477                 1421 4421 15699 3.553\n",
            "\n",
            "22:01:35 | time:529s total_exs:138956 total_steps:1471 epochs:2.15\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.89     1  2997 10387       0          0 325.7 4700              8192  4.166    .4533 16.67  2.57 9.9e-06  1567  5430   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002128     .00383 13.06      .4290  .0006383                 1471 4564 15816 3.468\n",
            "\n",
            "22:01:50 | time:545s total_exs:143532 total_steps:1521 epochs:2.22\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.86     1  2824  9318       0          0 301.9 4576              8192  4.498    .4532 16.77 2.591 9.9e-06  1535  5063   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps  ups  \n",
            "   .0002185    .003934 13.34      .4258   .001311                 1521 4359 14381  3.3\n",
            "\n",
            "22:01:57 | time:552s total_exs:145840 total_steps:1544 epochs:2.26\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   30.39     1  3050  9606       0          0 316.1 2308              8192  4.031    .4222 16.88  2.57 9.9e-06  1694  5335   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.06      .4271  .0004333                 1544 4744 14941 3.152\n",
            "\n",
            "22:01:57 | running eval: valid\n",
            "22:02:03 | eval completed in 6.37s\n",
            "22:02:03 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3571 38217       0          0 944.8 5738   .09141 16.01  2.45 9.9e-06  1413 15126       0          0 11.59   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4442    .00122                 1544 4984 53343\n",
            "\u001b[0m\n",
            "22:02:03 | \u001b[1;32mnew best ppl: 11.59 (previous best was 11.63)\u001b[0m\n",
            "22:02:03 | saving best valid model: from_pretrained/model\n",
            "22:02:23 | time:578s total_exs:150272 total_steps:1594 epochs:2.32\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   33.67     1  2984  9909       0          0 294.3 4432              8192  4.254    .4461 17.43 2.596 9.9e-06  1545  5128   \n",
            "     ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "   .0002256    .001805 13.42      .4270  .0004513                 1594 4529 15037 3.323\n",
            "\n",
            "22:02:38 | time:593s total_exs:155132 total_steps:1644 epochs:2.40\n",
            "    clen  clip  ctpb  ctps  ctrunc  ctrunclen  exps  exs  fp16_loss_scalar  gnorm  gpu_mem  llen  loss      lr  ltpb  ltps  \\\n",
            "   31.54     1  3066 10498       0          0 332.8 4860              8192  4.082    .4292 16.35 2.567 9.9e-06  1590  5443   \n",
            "    ltrunc  ltrunclen   ppl  token_acc  token_em  total_train_updates  tpb   tps   ups  \n",
            "         0          0 13.02      .4302  .0006173                 1644 4656 15941 3.425\n",
            "\n",
            "22:02:45 | max_train_time elapsed:600.2632162570953s\n",
            "22:02:46 | \u001b[33mOverriding opt[\"init_model\"] to zoo:tutorial_transformer_generator/model (previously: /usr/local/lib/python3.10/dist-packages/data/models/tutorial_transformer_generator/model)\u001b[0m\n",
            "22:02:46 | \u001b[33mOverriding opt[\"optimizer\"] to adam (previously: mem_eff_adam)\u001b[0m\n",
            "22:02:46 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: allow_missing_init_opts: False,loglevel: info,dynamic_batching: full,is_debug: False,final_extra_opt: ,eval_dynamic_batching: None,num_workers: 0,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_steps: -1,world_logs: ,save_format: conversations,seed: None,log_keep_fields: all,tensorboard_logdir: None,wandb_log: False,wandb_name: None,wandb_project: None,wandb_entity: None,wandb_log_model: False,teacher_seed: None,mutators: None,train_experiencer_only: False,n_encoder_layers: -1,n_decoder_layers: -1,model_parallel: False,checkpoint_activations: False,beam_block_full_context: True,beam_length_penalty: 0.65,topk: 10,topp: 0.9,beam_delay: 30,beam_block_list_filename: None,temperature: 1.0,compute_tokenized_bleu: False,gpu_beam_blocking: False,fp16_impl: mem_efficient,force_fp16_tokens: True,adafactor_eps: 1e-30,0.001,history_reversed: False,history_add_global_end_token: None,special_tok_lst: None,bpe_vocab: None,bpe_merge: None,bpe_add_prefix_space: None,bpe_dropout: None,invsqrt_lr_decay_gamma: -1,parlai_home: /usr/local/lib/python3.10/dist-packages,dict_loaded: True,download_path: None,verbose: False,datapath: /usr/local/lib/python3.10/dist-packages/data,load_from_checkpoint: True,interactive_mode: False\u001b[0m\n",
            "22:02:46 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
            "--show-advanced-args False --task internal:new_reddit:presorted --datatype train:stream --numthreads 1 --batchsize 48 --num-epochs 5.0 --max-train-time -1 --validation-every-n-secs 1800.0 --save-after-valid True --validation-every-n-epochs -1 --validation-max-exs 9920 --short-final-eval True --validation-patience 0 --validation-metric-mode min --dict-build-first True --numworkers 4 --pytorch-preprocess False --pytorch-teacher-batch-sort False --batch-sort-cache-type pop --batch-length-range 5 --shuffle False --batch-sort-field text --pytorch-context-length -1 --pytorch-include-labels True --log-every-n-secs 30.0 --distributed-world-size 64 --port 61337 --dropout 0.1 --beam-size 8 --beam-min-n-best 3 --beam-min-length 10 --skip-generation False --inference beam --optimizer fused_adam --learningrate 0.0005 --gradient-clip 10.0 --adam-eps 1e-06 --betas 0.9,0.98 --weight-decay 0.01 --lr-scheduler invsqrt --warmup-updates 20000 --gpu 0 --beam-block-ngram 3 --beam-context-block-ngram 3\u001b[0m\n",
            "22:02:46 | Using CUDA\n",
            "22:02:46 | loading dictionary from from_pretrained/model.dict\n",
            "22:02:46 | num words = 54944\n",
            "22:02:47 | Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "22:02:47 | Loading existing model params from from_pretrained/model\n",
            "22:02:49 | creating task(s): empathetic_dialogues\n",
            "22:02:51 | running eval: valid\n",
            "22:02:59 | eval completed in 7.78s\n",
            "22:02:59 | \u001b[1mvalid:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   40.45  3464 31646       0          0 782.3 5738   .08009 16.01  2.45 9.9e-06  1371 12526       0          0 11.59   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4442   .001046                 1544 4835 44172\n",
            "\u001b[0m\n",
            "22:02:59 | creating task(s): empathetic_dialogues\n",
            "22:03:00 | running eval: test\n",
            "22:03:07 | eval completed in 6.66s\n",
            "22:03:07 | \u001b[1mtest:\n",
            "    clen  ctpb  ctps  ctrunc  ctrunclen  exps  exs  gpu_mem  llen  loss      lr  ltpb  ltps  ltrunc  ltrunclen   ppl  \\\n",
            "   43.69  3590 37881       0          0 867.1 5259   .08009 16.23 2.472 9.9e-06  1334 14073       0          0 11.85   \n",
            "    token_acc  token_em  total_train_updates  tpb   tps  \n",
            "        .4413  .0005705                 1544 4923 51954\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'exs': SumMetric(5738),\n",
              "  'clen': AverageMetric(40.45),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(16.01),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.45),\n",
              "  'ppl': PPLMetric(11.59),\n",
              "  'token_acc': AverageMetric(0.4442),\n",
              "  'token_em': AverageMetric(0.001046),\n",
              "  'exps': GlobalTimerMetric(782.3),\n",
              "  'ltpb': GlobalAverageMetric(1371),\n",
              "  'ltps': GlobalTimerMetric(1.253e+04),\n",
              "  'ctpb': GlobalAverageMetric(3464),\n",
              "  'ctps': GlobalTimerMetric(3.165e+04),\n",
              "  'tpb': GlobalAverageMetric(4835),\n",
              "  'tps': GlobalTimerMetric(4.417e+04),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'gpu_mem': GlobalAverageMetric(0.08009),\n",
              "  'total_train_updates': GlobalFixedMetric(1544)},\n",
              " {'exs': SumMetric(5259),\n",
              "  'clen': AverageMetric(43.69),\n",
              "  'ctrunc': AverageMetric(0),\n",
              "  'ctrunclen': AverageMetric(0),\n",
              "  'llen': AverageMetric(16.23),\n",
              "  'ltrunc': AverageMetric(0),\n",
              "  'ltrunclen': AverageMetric(0),\n",
              "  'loss': AverageMetric(2.472),\n",
              "  'ppl': PPLMetric(11.85),\n",
              "  'token_acc': AverageMetric(0.4413),\n",
              "  'token_em': AverageMetric(0.0005705),\n",
              "  'exps': GlobalTimerMetric(867.1),\n",
              "  'ltpb': GlobalAverageMetric(1334),\n",
              "  'ltps': GlobalTimerMetric(1.407e+04),\n",
              "  'ctpb': GlobalAverageMetric(3590),\n",
              "  'ctps': GlobalTimerMetric(3.788e+04),\n",
              "  'tpb': GlobalAverageMetric(4923),\n",
              "  'tps': GlobalTimerMetric(5.195e+04),\n",
              "  'lr': GlobalAverageMetric(9.9e-06),\n",
              "  'gpu_mem': GlobalAverageMetric(0.08009),\n",
              "  'total_train_updates': GlobalFixedMetric(1544)})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iBZXTLRvIjb"
      },
      "source": [
        "## Wow that's a lot of options? Where do I find more info?\n",
        "\n",
        "As you might have noticed, there are a LOT of options to ParlAI. You're best reading the [ParlAI docs](https://parl.ai/docs) to find a list of hyperparameters. We provide lists of the command-line args for both models\n",
        "\n",
        "You can get some guidance in this notebook by using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pl8VVl5plfm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "453c1f82-a7ff-4df7-c7ad-f728c6e67854"
      },
      "source": [
        "# note that if you want to see model-specific arguments, you must specify a model name\n",
        "print(TrainModel.help(model='seq2seq'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: TrainModel [-h] [-o INIT_OPT] [-v] [-t TASK]\n",
            "                  [-dt {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}]\n",
            "                  [-nt NUMTHREADS] [-bs BATCHSIZE] [-dynb {None,batchsort,full}]\n",
            "                  [-dp DATAPATH] [-m MODEL] [-mf MODEL_FILE] [-im INIT_MODEL]\n",
            "                  [-et EVALTASK] [-eps NUM_EPOCHS] [-ttim MAX_TRAIN_TIME]\n",
            "                  [-vtim VALIDATION_EVERY_N_SECS] [-stim SAVE_EVERY_N_SECS]\n",
            "                  [-sval SAVE_AFTER_VALID] [-veps VALIDATION_EVERY_N_EPOCHS]\n",
            "                  [-vp VALIDATION_PATIENCE] [-vmt VALIDATION_METRIC]\n",
            "                  [-vmm {max,min}] [-mcs METRICS] [-micro AGGREGATE_MICRO]\n",
            "                  [-tblog TENSORBOARD_LOG] [-hs HIDDENSIZE] [-esz EMBEDDINGSIZE]\n",
            "                  [-nl NUMLAYERS] [-dr DROPOUT] [-bi BIDIRECTIONAL]\n",
            "                  [-att {none,concat,general,dot,local}]\n",
            "                  [-attl ATTENTION_LENGTH] [--attention-time {pre,post}]\n",
            "                  [-rnn {rnn,gru,lstm}] [-dec {same,shared}]\n",
            "                  [-lt {unique,enc_dec,dec_out,all}] [-soft NUMSOFTMAX]\n",
            "                  [-idr INPUT_DROPOUT] [--beam-size BEAM_SIZE]\n",
            "                  [--beam-min-length BEAM_MIN_LENGTH]\n",
            "                  [--beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM]\n",
            "                  [--beam-block-ngram BEAM_BLOCK_NGRAM]\n",
            "                  [--beam-length-penalty BEAM_LENGTH_PENALTY]\n",
            "                  [--inference {topk,beam,nucleus,delayedbeam,greedy}]\n",
            "                  [--topk TOPK] [--topp TOPP] [--beam-delay BEAM_DELAY]\n",
            "                  [--temperature TEMPERATURE]\n",
            "                  [--compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU]\n",
            "                  [-i INTERACTIVE_MODE]\n",
            "                  [-emb {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}]\n",
            "                  [-embp EMBEDDING_PROJECTION] [--fp16 FP16]\n",
            "                  [--fp16-impl {apex,mem_efficient}]\n",
            "                  [-opt {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}]\n",
            "                  [-lr LEARNINGRATE] [-clip GRADIENT_CLIP]\n",
            "                  [--adafactor-eps ADAFACTOR_EPS] [-mom MOMENTUM]\n",
            "                  [--nesterov NESTEROV] [-nu NUS] [-beta BETAS]\n",
            "                  [-wdecay WEIGHT_DECAY] [-rc RANK_CANDIDATES] [-tr TRUNCATE]\n",
            "                  [--text-truncate TEXT_TRUNCATE]\n",
            "                  [--label-truncate LABEL_TRUNCATE] [-histsz HISTORY_SIZE]\n",
            "                  [-pt PERSON_TOKENS] [--split-lines SPLIT_LINES]\n",
            "                  [--delimiter DELIMITER] [-gpu GPU | --no-cuda]\n",
            "                  [--bpe-vocab BPE_VOCAB] [--bpe-merge BPE_MERGE]\n",
            "                  [--lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}]\n",
            "                  [--lr-scheduler-patience LR_SCHEDULER_PATIENCE]\n",
            "                  [--lr-scheduler-decay LR_SCHEDULER_DECAY]\n",
            "                  [--max-lr-steps MAX_LR_STEPS]\n",
            "                  [--invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA]\n",
            "\n",
            "Train a model\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help\n",
            "      show this help message and exit\n",
            "\n",
            "Main ParlAI Arguments:\n",
            "  -o, --init-opt INIT_OPT\n",
            "      Path to json file of options. Note: Further Command-line arguments\n",
            "      override file-based options. (default: None)\n",
            "  -v, --show-advanced-args\n",
            "      Show hidden command line options (advanced users only) (default: False)\n",
            "  -t, --task TASK\n",
            "      ParlAI task(s), e.g. \"babi:Task1\" or \"babi,cbt\" (default: None)\n",
            "  -dt, --datatype {train,train:stream,train:ordered,train:ordered:stream,train:stream:ordered,train:evalmode,train:evalmode:stream,train:evalmode:ordered,train:evalmode:ordered:stream,train:evalmode:stream:ordered,valid,valid:stream,test,test:stream}\n",
            "      choose from: train, train:ordered, valid, test. to stream data add\n",
            "      \":stream\" to any option (e.g., train:stream). by default: train is random\n",
            "      with replacement, valid is ordered, test is ordered. (default: train)\n",
            "  -nt, --numthreads NUMTHREADS\n",
            "      number of threads. Used for hogwild if batchsize is 1, else for number of\n",
            "      threads in threadpool loading, (default: 1)\n",
            "  -bs, --batchsize BATCHSIZE\n",
            "      batch size for minibatch training schemes (default: 1)\n",
            "  -dynb, --dynamic-batching {None,batchsort,full}\n",
            "      Use dynamic batching (default: None)\n",
            "  -dp, --datapath DATAPATH\n",
            "      path to datasets, defaults to {parlai_dir}/data (default: None)\n",
            "\n",
            "ParlAI Model Arguments:\n",
            "  -m, --model MODEL\n",
            "      the model class name. can match parlai/agents/<model> for agents in that\n",
            "      directory, or can provide a fully specified module for `from X import Y`\n",
            "      via `-m X:Y` (e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)\n",
            "      (default: None)\n",
            "  -mf, --model-file MODEL_FILE\n",
            "      model file name for loading and saving models (default: None)\n",
            "  -im, --init-model INIT_MODEL\n",
            "      load model weights and dict from this file (default: None)\n",
            "\n",
            "Training Loop Arguments:\n",
            "  -et, --evaltask EVALTASK\n",
            "      task to use for valid/test (defaults to the one used for training)\n",
            "      (default: None)\n",
            "  -eps, --num-epochs NUM_EPOCHS\n",
            "  -ttim, --max-train-time MAX_TRAIN_TIME\n",
            "  -vtim, --validation-every-n-secs VALIDATION_EVERY_N_SECS\n",
            "      Validate every n seconds. Saves model to model_file (if set) whenever best\n",
            "      val metric is found (default: -1)\n",
            "  -stim, --save-every-n-secs SAVE_EVERY_N_SECS\n",
            "      Saves the model to model_file.checkpoint after every n seconds (default\n",
            "      -1, never). (default: -1)\n",
            "  -sval, --save-after-valid SAVE_AFTER_VALID\n",
            "      Saves the model to model_file.checkpoint after every validation (default\n",
            "      False).\n",
            "  -veps, --validation-every-n-epochs VALIDATION_EVERY_N_EPOCHS\n",
            "      Validate every n epochs. Saves model to model_file (if set) whenever best\n",
            "      val metric is found (default: -1)\n",
            "  -vp, --validation-patience VALIDATION_PATIENCE\n",
            "      number of iterations of validation where result does not improve before we\n",
            "      stop training (default: 10)\n",
            "  -vmt, --validation-metric VALIDATION_METRIC\n",
            "      key into report table for selecting best validation (default: accuracy)\n",
            "  -vmm, --validation-metric-mode {max,min}\n",
            "      how to optimize validation metric (max or min) (default: None)\n",
            "  -mcs, --metrics METRICS\n",
            "      list of metrics to show/compute, e.g. all, default,or give a list split by\n",
            "      , like ppl,f1,accuracy,hits@1,rouge,bleuthe rouge metrics will be computed\n",
            "      as rouge-1, rouge-2 and rouge-l (default: default)\n",
            "  -micro, --aggregate-micro AGGREGATE_MICRO\n",
            "      Report micro-averaged metrics instead of macro averaged metrics. (default:\n",
            "      False)\n",
            "\n",
            "Tensorboard Arguments:\n",
            "  -tblog, --tensorboard-log TENSORBOARD_LOG\n",
            "      Tensorboard logging of metrics, default is False\n",
            "\n",
            "Seq2Seq Arguments:\n",
            "  -hs, --hiddensize HIDDENSIZE\n",
            "      size of the hidden layers (default: 128)\n",
            "  -esz, --embeddingsize EMBEDDINGSIZE\n",
            "      size of the token embeddings (default: 128)\n",
            "  -nl, --numlayers NUMLAYERS\n",
            "      number of hidden layers (default: 2)\n",
            "  -dr, --dropout DROPOUT\n",
            "      dropout rate (default: 0.1)\n",
            "  -bi, --bidirectional BIDIRECTIONAL\n",
            "      whether to encode the context with a bidirectional rnn (default: False)\n",
            "  -att, --attention {none,concat,general,dot,local}\n",
            "      Choices: none, concat, general, local. If set local, also set attention-\n",
            "      length. (see arxiv.org/abs/1508.04025) (default: none)\n",
            "  -attl, --attention-length ATTENTION_LENGTH\n",
            "      Length of local attention. (default: 48)\n",
            "  --attention-time {pre,post}\n",
            "      Whether to apply attention before or after decoding. (default: post)\n",
            "  -rnn, --rnn-class {rnn,gru,lstm}\n",
            "      Choose between different types of RNNs. (default: lstm)\n",
            "  -dec, --decoder {same,shared}\n",
            "      Choose between different decoder modules. Default \"same\" uses same class\n",
            "      as encoder, while \"shared\" also uses the same weights. Note that shared\n",
            "      disabled some encoder options--in particular, bidirectionality. (default:\n",
            "      same)\n",
            "  -lt, --lookuptable {unique,enc_dec,dec_out,all}\n",
            "      The encoder, decoder, and output modules can share weights, or not. Unique\n",
            "      has independent embeddings for each. Enc_dec shares the embedding for the\n",
            "      encoder and decoder. Dec_out shares decoder embedding and output weights.\n",
            "      All shares all three weights. (default: unique)\n",
            "  -soft, --numsoftmax NUMSOFTMAX\n",
            "      default 1, if greater then uses mixture of softmax (see\n",
            "      arxiv.org/abs/1711.03953). (default: 1)\n",
            "  -idr, --input-dropout INPUT_DROPOUT\n",
            "      Probability of replacing tokens with UNK in training. (default: 0.0)\n",
            "\n",
            "Torch Generator Agent:\n",
            "  --beam-size BEAM_SIZE\n",
            "      Beam size, if 1 then greedy search (default: 1)\n",
            "  --beam-min-length BEAM_MIN_LENGTH\n",
            "      Minimum length of prediction to be generated by the beam search (default:\n",
            "      1)\n",
            "  --beam-context-block-ngram BEAM_CONTEXT_BLOCK_NGRAM\n",
            "      Size n-grams to block in beam search from the context. val <= 0 implies no\n",
            "      blocking (default: -1)\n",
            "  --beam-block-ngram BEAM_BLOCK_NGRAM\n",
            "      Size n-grams to block in beam search. val <= 0 implies no blocking\n",
            "      (default: -1)\n",
            "  --beam-length-penalty BEAM_LENGTH_PENALTY\n",
            "      Applies a length penalty. Set to 0 for no penalty. (default: 0.65)\n",
            "  --inference {topk,beam,nucleus,delayedbeam,greedy}\n",
            "      Generation algorithm (default: greedy)\n",
            "  --topk TOPK\n",
            "      K used in Top K sampling (default: 10)\n",
            "  --topp TOPP\n",
            "      p used in nucleus sampling (default: 0.9)\n",
            "  --beam-delay BEAM_DELAY\n",
            "      used in delayedbeam search (default: 30)\n",
            "  --temperature TEMPERATURE\n",
            "      temperature to add during decoding (default: 1.0)\n",
            "  --compute-tokenized-bleu COMPUTE_TOKENIZED_BLEU\n",
            "      if true, compute tokenized bleu scores (default: False)\n",
            "\n",
            "TorchAgent Arguments:\n",
            "  -i, --interactive-mode INTERACTIVE_MODE\n",
            "      Whether in full interactive mode or not, which means generating text or\n",
            "      retrieving from a full set of candidates, which is necessary to actually\n",
            "      do full dialogue. However, during training or quick validation (e.g. PPL\n",
            "      for generation or ranking a few candidates for ranking models) you might\n",
            "      want these set to off. Typically, scripts can set their preferred default\n",
            "      behavior at the start, e.g. eval scripts. (default: False)\n",
            "  -emb, --embedding-type {random,glove,glove-fixed,fasttext,fasttext-fixed,fasttext_cc,fasttext_cc-fixed}\n",
            "      Choose between different strategies for initializing word embeddings.\n",
            "      Default is random, but can also preinitialize from Glove or Fasttext.\n",
            "      Preinitialized embeddings can also be fixed so they are not updated during\n",
            "      training. (default: random)\n",
            "  -embp, --embedding-projection EMBEDDING_PROJECTION\n",
            "      If pretrained embeddings have a different dimensionality than your\n",
            "      embedding size, strategy for projecting to the correct size. If the\n",
            "      dimensions are the same, this is ignored unless you append \"-force\" to\n",
            "      your choice. (default: random)\n",
            "  --fp16 FP16\n",
            "      Use fp16 computations. (default: False)\n",
            "  --fp16-impl {apex,mem_efficient}\n",
            "      Implementation of FP16 to use (default: apex)\n",
            "  -rc, --rank-candidates RANK_CANDIDATES\n",
            "      Whether the model should parse candidates for ranking. (default: False)\n",
            "  -tr, --truncate TRUNCATE\n",
            "      Truncate input lengths to increase speed / use less memory. (default: -1)\n",
            "  --text-truncate TEXT_TRUNCATE\n",
            "      Text input truncation length: if not specified, this will default to\n",
            "      `truncate` (default: None)\n",
            "  --label-truncate LABEL_TRUNCATE\n",
            "      Label truncation length: if not specified, this will default to `truncate`\n",
            "      (default: None)\n",
            "  -histsz, --history-size HISTORY_SIZE\n",
            "      Number of past dialog utterances to remember. (default: -1)\n",
            "  -pt, --person-tokens PERSON_TOKENS\n",
            "      add person tokens to history. adds __p1__ in front of input text and\n",
            "      __p2__ in front of past labels when available or past utterances generated\n",
            "      by the model. these are added to the dictionary during initialization.\n",
            "      (default: False)\n",
            "  --split-lines SPLIT_LINES\n",
            "      split the dialogue history on newlines and save in separate vectors\n",
            "      (default: False)\n",
            "  --delimiter DELIMITER\n",
            "      Join history lines with this token, defaults to newline (default: )\n",
            "  -gpu, --gpu GPU\n",
            "      which GPU to use (default: -1)\n",
            "  --no-cuda\n",
            "      disable GPUs even if available. otherwise, will use GPUs if available on\n",
            "      the device. (default: False)\n",
            "\n",
            "Optimizer Arguments:\n",
            "  -opt, --optimizer {adadelta,adagrad,adam,adamw,sparseadam,adamax,asgd,sgd,rprop,rmsprop,optimizer,lbfgs,mem_eff_adam,adafactor}\n",
            "      Choose between pytorch optimizers. Any member of torch.optim should be\n",
            "      valid. (default: sgd)\n",
            "  -lr, --learningrate LEARNINGRATE\n",
            "      Learning rate (default: 1)\n",
            "  -clip, --gradient-clip GRADIENT_CLIP\n",
            "      gradient clipping using l2 norm (default: 0.1)\n",
            "  --adafactor-eps ADAFACTOR_EPS\n",
            "      Epsilon values for adafactor optimizer: regularization constants for\n",
            "      square gradient and parameter scale respectively (default: 1e-30,1e-3)\n",
            "  -mom, --momentum MOMENTUM\n",
            "      if applicable, momentum value for optimizer. (default: 0)\n",
            "  --nesterov NESTEROV\n",
            "      if applicable, whether to use nesterov momentum. (default: True)\n",
            "  -nu, --nus NUS\n",
            "      if applicable, nu value(s) for optimizer. can use a single value like 0.7\n",
            "      or a comma-separated tuple like 0.7,1.0 (default: 0.7)\n",
            "  -beta, --betas BETAS\n",
            "      if applicable, beta value(s) for optimizer. can use a single value like\n",
            "      0.9 or a comma-separated tuple like 0.9,0.999 (default: 0.9,0.999)\n",
            "  -wdecay, --weight-decay WEIGHT_DECAY\n",
            "      Weight decay on the weights. (default: None)\n",
            "\n",
            "BPEHelper Arguments:\n",
            "  --bpe-vocab BPE_VOCAB\n",
            "      path to pre-trained tokenizer vocab (default: None)\n",
            "  --bpe-merge BPE_MERGE\n",
            "      path to pre-trained tokenizer merge (default: None)\n",
            "\n",
            "Learning Rate Scheduler:\n",
            "  --lr-scheduler {reduceonplateau,none,fixed,invsqrt,cosine,linear}\n",
            "      Learning rate scheduler. (default: reduceonplateau)\n",
            "  --lr-scheduler-patience LR_SCHEDULER_PATIENCE\n",
            "      LR scheduler patience. In number of validation runs. If using fixed\n",
            "      scheduler, LR is decayed every <patience> validations. (default: 3)\n",
            "  --lr-scheduler-decay LR_SCHEDULER_DECAY\n",
            "      Decay factor for LR scheduler, or how much LR is multiplied by when it is\n",
            "      lowered. (default: 0.5)\n",
            "  --max-lr-steps MAX_LR_STEPS\n",
            "      Number of train steps the scheduler should take after warmup. Training is\n",
            "      terminated after this many steps. This should only be set for --lr-\n",
            "      scheduler cosine or linear (default: -1)\n",
            "  --invsqrt-lr-decay-gamma INVSQRT_LR_DECAY_GAMMA\n",
            "      Constant used only to find the lr multiplier for the invsqrt scheduler.\n",
            "      Must be set for --lr-scheduler invsqrt (default: -1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKGUWyKTwVtX"
      },
      "source": [
        "You'll notice the options are give as commandline arguments. We control our options via `argparse`. The option names are relatively predictable: `--init-model` becomes `init_model`; `--num-epochs` becomes `num_epochs` and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgLwGAq1wZJb"
      },
      "source": [
        "# Looking at model predictions\n",
        "\n",
        "We have shown how we can chat with a model ourselves, interactively. We might want to inspect how the model reacts with a fixed set of inputs. Let's use that model we just trained!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCZgs6OlvJ-q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "95b89fa8-22a6-4261-d72e-e82b07517810"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: No response\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H3QKTjdwokh"
      },
      "source": [
        "Whoa wait a second! The model isn't giving any responses? That's because we set `--skip-generation true` to speed up training. We need to turn that back off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLiq-vuowamh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "241342eb-6ac6-4a0c-91c5-674ab6b7e0c7"
      },
      "source": [
        "from parlai.scripts.display_model import DisplayModel\n",
        "DisplayModel.main(\n",
        "    task='empathetic_dialogues',\n",
        "    model_file='from_pretrained/model',\n",
        "    num_examples=2,\n",
        "    skip_generation=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['num_examples'] to 2 (previously: None )]\n",
            "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): empathetic_dialogues]\n",
            "[EmpatheticDialoguesTeacher] Only use experiencer side? True, datatype: valid\n",
            "\u001b[1;31m- - - NEW EPISODE: empathetic_dialogues- - -\u001b[0;0m\n",
            "\u001b[0mToday,as i was leaving for work in the morning,i had a tire burst in the middle of a busy road. That scared the hell out of me!\u001b[0;0m\n",
            "\u001b[1;94m    labels: Are you fine now?\u001b[0;0m\n",
            "\u001b[0;95m     model: oh no ! that ' s terrible ! did you get a new tire ?\u001b[0;0m\n",
            "\u001b[0mYeah,i'm doing alright now, but with minor injuries.\u001b[0;0m\n",
            "\u001b[1;94m    labels: Cool :) Is your car damaged a lot?\u001b[0;0m\n",
            "\u001b[0;95m     model: that ' s good . i hope you are okay .\u001b[0;0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MR0rn0ZwyxQ"
      },
      "source": [
        "On the command line:\n",
        "```bash\n",
        "python -m parlai.scripts.display_model --task empathetic_dialogues --model-file zoo:tutorial_transformer_generator/model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYuaSPWrw0Il"
      },
      "source": [
        "# Bringing your own datasets\n",
        "\n",
        "What if you want to build your own dataset in ParlAI? Of course you can do that!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SgJi8XHwtph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9656b678-9dfc-4cfa-d1eb-3cb13aa14608"
      },
      "source": [
        "from parlai.core.teachers import register_teacher, DialogTeacher\n",
        "\n",
        "@register_teacher(\"my_teacher\")\n",
        "class MyTeacher(DialogTeacher):\n",
        "    def __init__(self, opt, shared=None):\n",
        "        # opt is the command line arguments.\n",
        "        \n",
        "        # What is this shared thing?\n",
        "        # We make many copies of a teacher, one-per-batchsize. Shared lets us store \n",
        "        \n",
        "        # We just need to set the \"datafile\".  This is boilerplate, but differs in many teachers.\n",
        "        # The \"datafile\" is the filename where we will load the data from. In this case, we'll set it to\n",
        "        # the fold name (train/valid/test) + \".txt\"\n",
        "        opt['datafile'] = opt['datatype'].split(':')[0] + \".txt\"\n",
        "        super().__init__(opt, shared)\n",
        "    \n",
        "    def setup_data(self, datafile):\n",
        "        # filename tells us where to load from.\n",
        "        # We'll just use some hardcoded data, but show how you could read the filename here:\n",
        "        print(f\" ~~ Loading from {datafile} ~~ \")\n",
        "        \n",
        "        # setup_data should yield tuples of ((text, label), new_episode)\n",
        "        # That is ((str, str), bool)\n",
        "        \n",
        "        # first episode\n",
        "        # notice how we have call, response, and then True? The True indicates this is a first message\n",
        "        # in a conversation\n",
        "        yield ('Hello', 'Hi'), True\n",
        "        # Next we have the second turn. This time, the last element is False, indicating we're still going\n",
        "        yield ('How are you', 'I am fine'), False\n",
        "        yield (\"Let's say goodbye\", 'Goodbye!'), False\n",
        "        \n",
        "        # second episode. We need to have True again!\n",
        "        yield (\"Hey\", \"hi there\"), True\n",
        "        yield (\"Deja vu?\", \"Deja vu!\"), False\n",
        "        yield (\"Last chance\", \"This is it\"), False\n",
        "        \n",
        "        \n",
        "DisplayData.main(task=\"my_teacher\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "   \u001b[1;94mHi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "   \u001b[1;94mI am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "   \u001b[1;94mGoodbye!\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher - - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "   \u001b[1;94mhi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "   \u001b[1;94mDeja vu!\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "   \u001b[1;94mThis is it\u001b[0;0m\n",
            "EPOCH DONE\n",
            "[ loaded 2 episodes with a total of 6 examples ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvwxi6gXw8jU"
      },
      "source": [
        "Notice how the data corresponds to the utterances we provided? In reality, we'd normally want to load up a data file, loop through it, and yield the tuples from processed data. But for this simple example, it works well.\n",
        "\n",
        "We can now use our teacher in the standard places! Let's see how the model we trained earlier behaves with it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyZQnxAw5HG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "7240d04f-f9d3-4188-9bc8-c67f99b873b5"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model_file='from_pretrained/model', skip_generation=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ warning: overriding opt['task'] to my_teacher (previously: empathetic_dialogues )]\n",
            "[ warning: overriding opt['skip_generation'] to False (previously: True )]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from from_pretrained/model.dict\n",
            "[ num words =  54944 ]\n",
            "Total parameters: 87,508,992 (87,508,992 trainable)\n",
            "[ Loading existing model params from from_pretrained/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: i am good , how are you ?\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: i am fine\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: i ' ve just been in this place before\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOzvSHAy0meK"
      },
      "source": [
        "Note that the `register_teacher` decorator makes the commands aware of your teacher. If you leave it off, the commands won't be able to locate it. If you want to use your teacher on the command line, you'll need to put it in a very specific filename: `parlai/agents/my_teacher/agents.py`, and you'll need to name the class `DefaultTeacher` instead of `MyTeacher`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJj7Lhs00oOB"
      },
      "source": [
        "# Creating your own models\n",
        "\n",
        "As a start, we'll implement a *very* simple agent. This agent will just sort of respond with \"hello X, my name is Y\", where X is based on the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pykhtFDrxCPo"
      },
      "source": [
        "from parlai.core.agents import register_agent, Agent\n",
        "\n",
        "@register_agent(\"hello\")\n",
        "class HelloAgent(Agent):\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, parser, partial_opt):\n",
        "        parser.add_argument('--name', type=str, default='Alice', help=\"The agent's name.\")\n",
        "        return parser\n",
        "        \n",
        "    def __init__(self, opt, shared=None):\n",
        "        # similar to the teacher, we have the Opt and the shared memory objects!\n",
        "        super().__init__(opt, shared)\n",
        "        self.id = 'HelloAgent'\n",
        "        self.name = opt['name']\n",
        "    \n",
        "    def observe(self, observation):\n",
        "        # Gather the last word from the other user's input\n",
        "        words = observation.get('text', '').split()\n",
        "        if words:\n",
        "            self.last_word = words[-1]\n",
        "        else:\n",
        "            self.last_word = \"stranger!\"\n",
        "    \n",
        "    def act(self):\n",
        "        # Always return a string like this.\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'text': f\"Hello {self.last_word}, I'm {self.name}\",\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1SZmy_s0sGd"
      },
      "source": [
        "Let's try seeing how this agent behaves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcS1UIFH0pb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "bcc52fbf-49f8-47ed-e5db-09e5b6c57b54"
      },
      "source": [
        "DisplayModel.main(task='my_teacher', model='hello')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hello, I'm Alice\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello you, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello goodbye, I'm Alice\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello Hey, I'm Alice\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello vu?, I'm Alice\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: Hello chance, I'm Alice\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmvcRSGS0wQE"
      },
      "source": [
        "Notice how it read the words from the user, and provides its name from the command line argument? We can also interact with it easily enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xd5CaG00tv6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4d9aee8c-d390-46c7-e685-febcd5a5b91c"
      },
      "source": [
        "Interactive.main(model='hello', name='Bob')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ optional arguments: ] \n",
            "[  display_examples: False ]\n",
            "[  display_ignore_fields: label_candidates,text_candidates ]\n",
            "[  display_prettify: False ]\n",
            "[  interactive_task: True ]\n",
            "[  name: Bob ]\n",
            "[ Main ParlAI Arguments: ] \n",
            "[  batchsize: 1 ]\n",
            "[  datapath: /usr/local/lib/python3.6/dist-packages/data ]\n",
            "[  datatype: train ]\n",
            "[  download_path: /usr/local/lib/python3.6/dist-packages/downloads ]\n",
            "[  dynamic_batching: None ]\n",
            "[  hide_labels: False ]\n",
            "[  image_mode: raw ]\n",
            "[  init_opt: None ]\n",
            "[  multitask_weights: [1] ]\n",
            "[  numthreads: 1 ]\n",
            "[  show_advanced_args: False ]\n",
            "[  task: interactive ]\n",
            "[ ParlAI Model Arguments: ] \n",
            "[  dict_class: None ]\n",
            "[  init_model: None ]\n",
            "[  model: hello ]\n",
            "[  model_file: None ]\n",
            "[ Local Human Arguments: ] \n",
            "[  local_human_candidates_file: None ]\n",
            "[  single_turn: False ]\n",
            "[ ParlAI Image Preprocessing Arguments: ] \n",
            "[  image_cropsize: 224 ]\n",
            "[  image_size: 256 ]\n",
            "\u001b[1;31mEnter [DONE] if you want to end the episode, [EXIT] to quit.\u001b[0;0m\n",
            "[creating task(s): interactive]\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m hi, who are you?\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello you?, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m My name is Stephen\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello Stephen, I'm Bob\u001b[0;0m\n",
            "\u001b[0mEnter Your Message:\u001b[0;0m [EXIT]\n",
            "\u001b[0;34m[HelloAgent]:\u001b[0;0m \u001b[1mHello stranger!, I'm Bob\u001b[0;0m\n",
            "CHAT DONE \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAe1hytf1BPk"
      },
      "source": [
        "Similar to the teacher, the call to `register_agent` makes it available for use in commands. If you forget the `register_agent` decorator, you won't be able to refer to it. Similarly, if you wanted to use this model from the command line, you would need to save this code to a special folder: `parlai/agents/hello/hello.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aBbhKTO1DEE"
      },
      "source": [
        "## Creating a neural network model\n",
        "\n",
        "The base Agent class is very simple, but it also provides extremely little functionality. We have created solid abstractions for creating neural-network type models. [`TorchGeneratorAgent`](https://parl.ai/docs/torch_agent.html#module-parlai.core.torch_generator_agent) is one our common abstractions, and it assumes a model which outputs one-word-at-a-time.\n",
        "\n",
        "The following is from our [ExampleSeq2Seq](https://github.com/facebookresearch/ParlAI/blob/master/parlai/agents/examples/seq2seq.py) agent. It's a simple RNN model, trained like a Machine Translation model. The Model is too complex to go over in this document, but please feel free to [read our TorchGeneratorAgent tutorial](https://parl.ai/docs/tutorial_torch_generator_agent.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVrZh-T903wh"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import parlai.core.torch_generator_agent as tga\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Example encoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size.\n",
        "    Pay particular attention to the ``forward`` output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        # must call super on all nn.Modules.\n",
        "        super().__init__()\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        \"\"\"\n",
        "        Perform the forward pass for the encoder.\n",
        "        Input *must* be input_tokens, which are the context tokens given\n",
        "        as a matrix of lookup IDs.\n",
        "        :param input_tokens:\n",
        "            Input tokens as a bsz x seqlen LongTensor.\n",
        "            Likely will contain padding.\n",
        "        :return:\n",
        "            You can return anything you like; it is will be passed verbatim\n",
        "            into the decoder for conditioning. However, it should be something\n",
        "            you can easily manipulate in ``reorder_encoder_states``.\n",
        "            This particular implementation returns the hidden and cell states from the\n",
        "            LSTM.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input_tokens)\n",
        "        _output, hidden = self.lstm(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic example decoder, consisting of an embedding layer and a 1-layer LSTM with the\n",
        "    specified hidden size. Decoder allows for incremental decoding by ingesting the\n",
        "    current incremental state on each forward pass.\n",
        "    Pay particular note to the ``forward``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialization.\n",
        "        Arguments here can be used to provide hyperparameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embeddings = embeddings\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, encoder_state, incr_state=None):\n",
        "        \"\"\"\n",
        "        Run forward pass.\n",
        "        :param input:\n",
        "            The currently generated tokens from the decoder.\n",
        "        :param encoder_state:\n",
        "            The output from the encoder module.\n",
        "        :parm incr_state:\n",
        "            The previous hidden state of the decoder.\n",
        "        \"\"\"\n",
        "        embedded = self.embeddings(input)\n",
        "        if incr_state is None:\n",
        "            # this is our very first call. We want to seed the LSTM with the\n",
        "            # hidden state of the decoder\n",
        "            state = encoder_state\n",
        "        else:\n",
        "            # We've generated some tokens already, so we can reuse the existing\n",
        "            # decoder state\n",
        "            state = incr_state\n",
        "\n",
        "        # get the new output and decoder incremental state\n",
        "        output, incr_state = self.lstm(embedded, state)\n",
        "\n",
        "        return output, incr_state\n",
        "\n",
        "\n",
        "class ExampleModel(tga.TorchGeneratorModel):\n",
        "    \"\"\"\n",
        "    ExampleModel implements the abstract methods of TorchGeneratorModel to define how to\n",
        "    re-order encoder states and decoder incremental states.\n",
        "    It also instantiates the embedding table, encoder, and decoder, and defines the\n",
        "    final output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary, hidden_size=1024):\n",
        "        super().__init__(\n",
        "            padding_idx=dictionary[dictionary.null_token],\n",
        "            start_idx=dictionary[dictionary.start_token],\n",
        "            end_idx=dictionary[dictionary.end_token],\n",
        "            unknown_idx=dictionary[dictionary.unk_token],\n",
        "        )\n",
        "        self.embeddings = nn.Embedding(len(dictionary), hidden_size)\n",
        "        self.encoder = Encoder(self.embeddings, hidden_size)\n",
        "        self.decoder = Decoder(self.embeddings, hidden_size)\n",
        "\n",
        "    def output(self, decoder_output):\n",
        "        \"\"\"\n",
        "        Perform the final output -> logits transformation.\n",
        "        \"\"\"\n",
        "        return F.linear(decoder_output, self.embeddings.weight)\n",
        "\n",
        "    def reorder_encoder_states(self, encoder_states, indices):\n",
        "        \"\"\"\n",
        "        Reorder the encoder states to select only the given batch indices.\n",
        "        Since encoder_state can be arbitrary, you must implement this yourself.\n",
        "        Typically you will just want to index select on the batch dimension.\n",
        "        \"\"\"\n",
        "        h, c = encoder_states\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "    def reorder_decoder_incremental_state(self, incr_state, indices):\n",
        "        \"\"\"\n",
        "        Reorder the decoder states to select only the given batch indices.\n",
        "        This method can be a stub which always returns None; this will result in the\n",
        "        decoder doing a complete forward pass for every single token, making generation\n",
        "        O(n^2). However, if any state can be cached, then this method should be\n",
        "        implemented to reduce the generation complexity to O(n).\n",
        "        \"\"\"\n",
        "        h, c = incr_state\n",
        "        return h[:, indices, :], c[:, indices, :]\n",
        "\n",
        "\n",
        "@register_agent(\"my_first_lstm\")\n",
        "class Seq2seqAgent(tga.TorchGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Example agent.\n",
        "    Implements the interface for TorchGeneratorAgent. The minimum requirement is that it\n",
        "    implements ``build_model``, but we will want to include additional command line\n",
        "    parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def add_cmdline_args(cls, argparser, partial_opt):\n",
        "        \"\"\"\n",
        "        Add CLI arguments.\n",
        "        \"\"\"\n",
        "        # Make sure to add all of TorchGeneratorAgent's arguments\n",
        "        super().add_cmdline_args(argparser)\n",
        "\n",
        "        # Add custom arguments only for this model.\n",
        "        group = argparser.add_argument_group('Example TGA Agent')\n",
        "        group.add_argument(\n",
        "            '-hid', '--hidden-size', type=int, default=1024, help='Hidden size.'\n",
        "        )\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Construct the model.\n",
        "        \"\"\"\n",
        "\n",
        "        model = ExampleModel(self.dict, self.opt['hidden_size'])\n",
        "        # Optionally initialize pre-trained embeddings by copying them from another\n",
        "        # source: GloVe, fastText, etc.\n",
        "        self._copy_embeddings(model.embeddings.weight, self.opt['embedding_type'])\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfR9w_Hm1HHY"
      },
      "source": [
        "Of course, now we can train with our new model. Let's train it on our toy task that we created earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJMXpogz1E-_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "3fe8275b-a7cd-491a-a761-fbd5849e2a7c"
      },
      "source": [
        "# of course, we can train the model! Let's Train it on our silly toy task from above\n",
        "!rm -rf my_first_lstm\n",
        "!mkdir -p my_first_lstm\n",
        "\n",
        "TrainModel.main(\n",
        "    model='my_first_lstm',\n",
        "    model_file='my_first_lstm/model',\n",
        "    task='my_teacher',\n",
        "    batchsize=1,\n",
        "    validation_every_n_secs=10,\n",
        "    max_train_time=60,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary: 100%|██████████| 6.00/6.00 [00:00<00:00, 1.91kex/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ building dictionary first... ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            " ~~ Loading from train.txt ~~ \n",
            "Dictionary: saving dictionary to my_first_lstm/model.dict\n",
            "[ dictionary built with 30 tokens in 0s ]\n",
            "[ no model with opt yet at: my_first_lstm/model(.opt) ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from train.txt ~~ \n",
            "[ training... ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "     clip  exs  gnorm  gpu_mem   loss  lr   ppl  token_acc  total_train_updates   tpb  updates\n",
            "   .01641 1828  1.368    .9171 .04105   1 1.042      .9942                 1828 3.328     1828\n",
            "\n",
            "[ time:10.0s total_exs:1828 epochs:304.67 ]\n",
            "    gpu_mem  lr  total_train_updates\n",
            "      .9171   1                 1828\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.06s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9171     0   1    1          1                 1828 3.333\n",
            "\n",
            "[ new best accuracy: 1 ]\n",
            "[ saving best valid model: my_first_lstm/model ]\n",
            "[ task solved! stopping. ]\n",
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "[ running eval: valid ]\n",
            "[ eval completed in 0.05s ]\n",
            "valid:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from test.txt ~~ \n",
            "[ running eval: test ]\n",
            "[ eval completed in 0.05s ]\n",
            "test:\n",
            "    accuracy   bleu-4  exs  f1  gpu_mem  loss  lr  ppl  token_acc  total_train_updates   tpb\n",
            "           1 .0003337    6   1    .9131     0   1    1          1                 1828 3.333\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hHrruVd1KnK"
      },
      "source": [
        "Let's see how it does. It should reproduce the data perfectly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shqFpdrE1Iif",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "1a89571c-2a42-48b8-9752-dffc4a58e557"
      },
      "source": [
        "DisplayModel.main(model_file='my_first_lstm/model', task='my_teacher')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ Using CUDA ]\n",
            "Dictionary: loading dictionary from my_first_lstm/model.dict\n",
            "[ num words =  30 ]\n",
            "Total parameters: 16,824,320 (16,824,320 trainable)\n",
            "[ Loading existing model params from my_first_lstm/model ]\n",
            "[creating task(s): my_teacher]\n",
            " ~~ Loading from valid.txt ~~ \n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHello\u001b[0;0m\n",
            "\u001b[1;94m    labels: Hi\u001b[0;0m\n",
            "\u001b[0;95m     model: Hi\u001b[0;0m\n",
            "\u001b[0mHow are you\u001b[0;0m\n",
            "\u001b[1;94m    labels: I am fine\u001b[0;0m\n",
            "\u001b[0;95m     model: I am fine\u001b[0;0m\n",
            "\u001b[0mLet's say goodbye\u001b[0;0m\n",
            "\u001b[1;94m    labels: Goodbye!\u001b[0;0m\n",
            "\u001b[0;95m     model: Goodbye !\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: my_teacher- - -\u001b[0;0m\n",
            "\u001b[0mHey\u001b[0;0m\n",
            "\u001b[1;94m    labels: hi there\u001b[0;0m\n",
            "\u001b[0;95m     model: hi there\u001b[0;0m\n",
            "\u001b[0mDeja vu?\u001b[0;0m\n",
            "\u001b[1;94m    labels: Deja vu!\u001b[0;0m\n",
            "\u001b[0;95m     model: Deja vu !\u001b[0;0m\n",
            "\u001b[0mLast chance\u001b[0;0m\n",
            "\u001b[1;94m    labels: This is it\u001b[0;0m\n",
            "\u001b[0;95m     model: This is it\u001b[0;0m\n",
            "EPOCH DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data -t wizard_of_wikipedia -dt train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0MXv_3Ax_ek",
        "outputId": "b8f91143-5b88-43b3-a5e5-5e8abcf79aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-30 20:11:34.175036: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-30 20:11:35.055269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 20:11:36.309545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-30 20:11:36.310098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-30 20:11:36.310298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "20:11:46 | Opt:\n",
            "20:11:46 |     add_missing_turns: none\n",
            "20:11:46 |     allow_missing_init_opts: False\n",
            "20:11:46 |     batchsize: 1\n",
            "20:11:46 |     chosen_topic_delimiter: '\\n'\n",
            "20:11:46 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "20:11:46 |     datatype: train:ordered\n",
            "20:11:46 |     dict_class: None\n",
            "20:11:46 |     display_add_fields: \n",
            "20:11:46 |     download_path: None\n",
            "20:11:46 |     dynamic_batching: None\n",
            "20:11:46 |     hide_labels: False\n",
            "20:11:46 |     ignore_agent_reply: True\n",
            "20:11:46 |     image_cropsize: 224\n",
            "20:11:46 |     image_mode: raw\n",
            "20:11:46 |     image_size: 256\n",
            "20:11:46 |     include_checked_sentence: True\n",
            "20:11:46 |     include_knowledge: True\n",
            "20:11:46 |     include_knowledge_separator: False\n",
            "20:11:46 |     init_model: None\n",
            "20:11:46 |     init_opt: None\n",
            "20:11:46 |     is_debug: False\n",
            "20:11:46 |     label_type: response\n",
            "20:11:46 |     loglevel: info\n",
            "20:11:46 |     max_display_len: 1000\n",
            "20:11:46 |     model: None\n",
            "20:11:46 |     model_file: None\n",
            "20:11:46 |     multitask_weights: [1]\n",
            "20:11:46 |     mutators: None\n",
            "20:11:46 |     num_examples: 10\n",
            "20:11:46 |     num_topics: 5\n",
            "20:11:46 |     override: \"{'task': 'wizard_of_wikipedia', 'datatype': 'train'}\"\n",
            "20:11:46 |     parlai_home: /usr/local/lib/python3.10/dist-packages\n",
            "20:11:46 |     starttime: Apr30_20-11\n",
            "20:11:46 |     task: wizard_of_wikipedia\n",
            "20:11:46 |     teacher_seed: None\n",
            "20:11:46 |     verbose: False\n",
            "20:11:46 | creating task(s): wizard_of_wikipedia\n",
            "[building data: /usr/local/lib/python3.10/dist-packages/data/wizard_of_wikipedia]\n",
            "20:11:46 | Downloading http://parl.ai/downloads/wizard_of_wikipedia/wizard_of_wikipedia.tgz to /usr/local/lib/python3.10/dist-packages/data/wizard_of_wikipedia/wizard_of_wikipedia.tgz\n",
            "Downloading wizard_of_wikipedia.tgz: 100% 972M/972M [01:09<00:00, 14.0MB/s]\n",
            "20:14:05 | loading /usr/local/lib/python3.10/dist-packages/data/wizard_of_wikipedia/train.json\n",
            "20:14:15 | \u001b[33mSome data not being used. If you are not trying to reproduce the previous results, it is recommended that you run with the flag --add-missing-turns train or --add-missing-turns all.\u001b[0m\n",
            "\u001b[1;31m- - - NEW EPISODE: wizard_of_wikipedia - - -\u001b[0;0m\n",
            "\u001b[0mScience fiction\u001b[0;0m\n",
            "   \u001b[1;94mI think science fiction is an amazing genre for anything. Future science, technology, time travel, FTL travel, they're all such interesting concepts.\u001b[0;0m\n",
            "\u001b[0mI'm a huge fan of science fiction myself! \u001b[0;0m\n",
            "   \u001b[1;94mAwesome! I really love how sci-fi storytellers focus on political/social/philosophical issues that would still be around even in the future. Makes them relatable.\u001b[0;0m\n",
            "\u001b[0mI agree. One of my favorite forms of science fiction is anything related to time travel! I find it fascinating.\u001b[0;0m\n",
            "   \u001b[1;94mIt's not quite sci-fi, but my favorite version of time travel is in Harry Potter and the Prisoner of Azkaban. Breaks zero logical rules.\u001b[0;0m\n",
            "\u001b[0mAnd that's difficult to do when dealing with time travel. I actually haven't seen the latest Harry Potter movies. Guess it's time to check them out!\u001b[0;0m\n",
            "   \u001b[1;94mIf you really want a look at the potential negative consequences of scientific innovation, what you should check out is the TV show Fringe. Incredibly well written.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: wizard_of_wikipedia - - -\u001b[0;0m\n",
            "\u001b[0mInternet access\n",
            "Can you imagine the world without internet access? \u001b[0;0m\n",
            "   \u001b[1;94mNo I could not! I couldn't imagine living when internet access was rare and very few people had it!\u001b[0;0m\n",
            "\u001b[0mOh me either! It seems like such a long time ago. I wonder when Internet was first created?\u001b[0;0m\n",
            "   \u001b[1;94mIt used to be restricted, but around 1995, the restricted were lifted and commercial use of it began\u001b[0;0m\n",
            "\u001b[0mThat is awesome. I wonder why it was restricted? Probably because they only wanted government and big companies to use it at first.\u001b[0;0m\n",
            "   \u001b[1;94mYes, it was developed from a government funded projects to help with universities research and laboratories in the United States...I am so glad they expanded it! \u001b[0;0m\n",
            "\u001b[0mI am too, it makes life so much easier!\u001b[0;0m\n",
            "   \u001b[1;94mWhat is your favorite thing to do with internet access? I like being able to use my computer and smartphone to use my email and browse the world wide web\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: wizard_of_wikipedia - - -\u001b[0;0m\n",
            "\u001b[0mPharmacist\n",
            "I am just finishing my university course and I will be a qualified pharmacist.  I am so excited about finishing and getting out there into the world of work! Do you work yourself?\u001b[0;0m\n",
            "   \u001b[1;94mYes, I perform administrative duties as a pharmacy technician.\u001b[0;0m\n",
            "\u001b[0mFantastic so you know all about the field.  I completed my four year course and also two 6 month placements but its not the same until you actually start working.  Do you work in a busy pharmacy?\u001b[0;0m\n",
            "   \u001b[1;94mYes, I work directly with a lot of patients.\u001b[0;0m\n",
            "20:14:22 | loaded 18430 episodes with a total of 74092 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai display_data -t wizard_of_internet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKXTjGWazj84",
        "outputId": "e41c5d6e-2acc-467f-d51a-ab93f2937a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-30 20:16:15.084856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-30 20:16:16.109220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 20:16:17.544424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-30 20:16:17.544949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-30 20:16:17.545141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "20:16:27 | Opt:\n",
            "20:16:27 |     add_skip_search_if_gold_prepended: False\n",
            "20:16:27 |     allow_missing_init_opts: False\n",
            "20:16:27 |     batchsize: 1\n",
            "20:16:27 |     datapath: /usr/local/lib/python3.10/dist-packages/data\n",
            "20:16:27 |     datatype: train:ordered\n",
            "20:16:27 |     dict_class: None\n",
            "20:16:27 |     display_add_fields: \n",
            "20:16:27 |     download_path: None\n",
            "20:16:27 |     dynamic_batching: None\n",
            "20:16:27 |     hide_labels: False\n",
            "20:16:27 |     ignore_agent_reply: True\n",
            "20:16:27 |     image_cropsize: 224\n",
            "20:16:27 |     image_mode: raw\n",
            "20:16:27 |     image_size: 256\n",
            "20:16:27 |     include_persona: True\n",
            "20:16:27 |     init_model: None\n",
            "20:16:27 |     init_opt: None\n",
            "20:16:27 |     is_debug: False\n",
            "20:16:27 |     loglevel: info\n",
            "20:16:27 |     max_display_len: 1000\n",
            "20:16:27 |     model: None\n",
            "20:16:27 |     model_file: None\n",
            "20:16:27 |     multitask_weights: [1]\n",
            "20:16:27 |     mutators: None\n",
            "20:16:27 |     num_examples: 10\n",
            "20:16:27 |     override: \"{'task': 'wizard_of_internet'}\"\n",
            "20:16:27 |     parlai_home: /usr/local/lib/python3.10/dist-packages\n",
            "20:16:27 |     prepend_gold_knowledge: False\n",
            "20:16:27 |     skip_empty_text: True\n",
            "20:16:27 |     starttime: Apr30_20-16\n",
            "20:16:27 |     task: wizard_of_internet\n",
            "20:16:27 |     teacher_seed: None\n",
            "20:16:27 |     verbose: False\n",
            "20:16:27 | creating task(s): wizard_of_internet\n",
            "20:16:27 | [building data: /usr/local/lib/python3.10/dist-packages/data/wizard_of_interent]\n",
            "This may take a while but only heppens once.\n",
            "20:16:27 | Downloading http://parl.ai/downloads/wizard_of_internet/wizard_of_internet.tgz to /usr/local/lib/python3.10/dist-packages/data/wizard_of_interent/wizard_of_internet.tgz\n",
            "Downloading wizard_of_internet.tgz: 100% 1.32G/1.32G [01:06<00:00, 19.7MB/s]\n",
            "20:18:37 | Finished downloading dataset files successfully.\n",
            "20:18:37 | Loading data from /usr/local/lib/python3.10/dist-packages/data/wizard_of_interent/train.jsonl ...\n",
            "8614it [00:39, 219.60it/s]\n",
            "\u001b[1;31m- - - NEW EPISODE: WizInternetWizardTeacher - - -\u001b[0;0m\n",
            "\u001b[0mMy favorite movie is home.\n",
            "so emotional and educaive\n",
            "home is a child friendly movie with some amaizing lessons for kids\u001b[0;0m\n",
            "   \u001b[1;94mMy favorite movie is home. Which your favorite movie.\u001b[0;0m\n",
            "\u001b[0mhome too and smurfs so entertaining\u001b[0;0m\n",
            "   \u001b[1;94mI want to entertainment is very emotional and educative.\u001b[0;0m\n",
            "\u001b[0mit has rihannas voice in it \u001b[0;0m\n",
            "   \u001b[1;94mOk, I have to rihannas voice is very likely speech.\u001b[0;0m\n",
            "\u001b[0mand the song ring in my head every morning\u001b[0;0m\n",
            "   \u001b[1;94mI have to the useful and every night.\u001b[0;0m\n",
            "\u001b[0mi guess ill watch it again tonight\u001b[0;0m\n",
            "   \u001b[1;94mI want to night show in watching tv show so very enjoyable moment.\u001b[0;0m\n",
            "\u001b[1;31m- - - NEW EPISODE: WizInternetWizardTeacher - - -\u001b[0;0m\n",
            "\u001b[0mMy favorite tv show is 90 day fiancé.\n",
            "I like to find out which couples are still together.\u001b[0;0m\n",
            "   \u001b[1;94m90 Day Fiancé has been on the air since 2014.\u001b[0;0m\n",
            "\u001b[0mThat's a long time. How many are from Brazil?\u001b[0;0m\n",
            "   \u001b[1;94mI'm finding that five people were from Brazil on the show.\u001b[0;0m\n",
            "\u001b[0mAny of those from Brazil still together with their partner?\u001b[0;0m\n",
            "   \u001b[1;94mPaul and Karine are not only still together, but they just had a son.\u001b[0;0m\n",
            "\u001b[0mNice! Are they still filming new season of the show?\u001b[0;0m\n",
            "   \u001b[1;94mThat is surprisingly hard to find information about, it's been renewed through season 7 so far.\u001b[0;0m\n",
            "\u001b[0mI hope they make more shows. There have been lots of pillow talk and recap.\u001b[0;0m\n",
            "   \u001b[1;94mIt has two spin off series, Before the 90 Days and Happily Ever After\u001b[0;0m\n",
            "20:19:16 | loaded 8614 episodes with a total of 41476 examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!parlai train_model -t blended_skill_talk,wizard_of_wikipedia,convai2:normalized,empathetic_dialogues --multitask-weights 1,3,3,3 -vstep 200 -lstep 50 -bs 4 --model bart r2c2_base_400M/init_opt.opt --text-truncate 1000 --label-truncate 1000 --fp16 true -lr 1e-6 --lr-scheduler reduceonplateau --optimizer adamw --warmup-updates 100 --gradient-clip 1.0 --skip-generation true --dropout 0.1 --attention-dropout 0.0 -vp 5 -vmt ppl -vmm min -dynb full --model-file /tmp/test_train_r2c2_400m"
      ],
      "metadata": {
        "id": "Nn1C_fwK1fzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyvSV7zb3WCq",
        "outputId": "eb64a4db-75b9-48c1-e2ea-5bb7db4e1d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting praw\n",
            "  Downloading praw-7.7.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.5.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.15)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.0 prawcore-2.3.0 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Wikipedia-API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdTJ5g6c3iEG",
        "outputId": "9cc88e80-ddbc-4c62-8405-71d796d737d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Wikipedia-API\n",
            "  Downloading Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (1.26.15)\n",
            "Installing collected packages: Wikipedia-API\n",
            "Successfully installed Wikipedia-API-0.5.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install praw\n",
        "import praw\n",
        "import wikipediaapi\n",
        "import pandas as pd\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"PFSGLhAmS6XFjeH6DUL6uQ\",\n",
        "    client_secret=\"GfYobly4x7xfgrvDoVRR2tcxcOMjEQ\",\n",
        "    # redirect_uri=\"your_redirect_uri\",\n",
        "    user_agent=\"Dramatic-Act-4165\"\n",
        "    # username=\"your_username\",\n",
        "    # password=\"your_password\",\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the Reddit API client\n",
        "# reddit = praw.Reddit(client_id='YOUR_CLIENT_ID', client_secret='YOUR_CLIENT_SECRET', user_agent='YOUR_USER_AGENT')\n",
        "\n",
        "# Initialize the Wikipedia API client\n",
        "wiki = wikipediaapi.Wikipedia('en')\n",
        "\n",
        "# Define the list of subreddits to search for each topic\n",
        "subreddits = {\n",
        "    'Politics': 'politics',\n",
        "    'Environment': 'environment',\n",
        "    'Technology': 'technology',\n",
        "    'Healthcare': 'healthcare',\n",
        "    'Education': 'education'\n",
        "}\n",
        "\n",
        "# Extract data from Reddit\n",
        "reddit_data = []\n",
        "for topic, subreddit_name in subreddits.items():\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    for post in subreddit.search(topic, limit=1000000):\n",
        "        reddit_data.append({'Topic': topic, 'Title': post.title, 'Text': post.selftext})\n",
        "\n",
        "# Extract data from Wikipedia\n",
        "wiki_data = []\n",
        "for topic in subreddits.keys():\n",
        "    page = wiki.page(topic)\n",
        "    if page.exists():\n",
        "        wiki_data.append({'Topic': topic, 'Title': page.title, 'Text': page.text})\n",
        "\n",
        "# Combine the data into a single pandas dataframe\n",
        "data = reddit_data + wiki_data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZVsP50Z3TRf",
        "outputId": "11ac49a6-337e-4e9e-e188-ea67e2e834fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Topic                                              Title  \\\n",
            "0       Politics  Disney sues Florida Gov. Ron DeSantis, alleges...   \n",
            "1       Politics  Trump says if elected he will force federal wo...   \n",
            "2       Politics  Clarence Thomas slammed from across political ...   \n",
            "3       Politics  Florida Gov. Ron DeSantis Says Disney Lawsuit ...   \n",
            "4       Politics  Over half of registered voters say political a...   \n",
            "..           ...                                                ...   \n",
            "500     Politics                                           Politics   \n",
            "501  Environment                                        Environment   \n",
            "502   Technology                                         Technology   \n",
            "503   Healthcare                                        Health care   \n",
            "504    Education                                          Education   \n",
            "\n",
            "                                                  Text  \n",
            "0                                                       \n",
            "1                                                       \n",
            "2                                                       \n",
            "3                                                       \n",
            "4                                                       \n",
            "..                                                 ...  \n",
            "500  Politics (from Greek: Πολιτικά, politiká, 'aff...  \n",
            "501  Environment most often refers to:\\n\\nNatural e...  \n",
            "502  Technology is the application of knowledge for...  \n",
            "503  Health care, or healthcare, is the improvement...  \n",
            "504  Education is a purposeful activity directed at...  \n",
            "\n",
            "[505 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import wikipediaapi\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Initialize the Reddit API client\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"PFSGLhAmS6XFjeH6DUL6uQ\",\n",
        "    client_secret=\"GfYobly4x7xfgrvDoVRR2tcxcOMjEQ\",\n",
        "    # redirect_uri=\"your_redirect_uri\",\n",
        "    user_agent=\"Dramatic-Act-4165\"\n",
        "    # username=\"your_username\",\n",
        "    # password=\"your_password\",\n",
        ")\n",
        "\n",
        "# Initialize the Wikipedia API client\n",
        "wiki = wikipediaapi.Wikipedia('en')\n",
        "\n",
        "# Define the list of subreddits to search for each topic\n",
        "subreddits = {\n",
        "    'Politics': 'politics',\n",
        "    'Environment': 'environment',\n",
        "    'Technology': 'technology',\n",
        "    'Healthcare': 'healthcare',\n",
        "    'Education': 'education'\n",
        "}\n",
        "\n",
        "# Define the number of posts to search on each subreddit\n",
        "num_posts = 100\n",
        "\n",
        "# Define the number of top posts to select\n",
        "num_top_posts = 1\n",
        "nltk.download('stopwords')  # Download the stopwords\n",
        "# Define the set of stop words to remove\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Extract data from Reddit\n",
        "reddit_data = []\n",
        "for topic, subreddit_name in subreddits.items():\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    posts = subreddit.search(topic, limit=num_posts)\n",
        "    top_posts = sorted(posts, key=lambda post: post.score, reverse=True)[:num_top_posts]\n",
        "    for post in top_posts:\n",
        "        text = post.title + \" \" + post.selftext\n",
        "        text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags\n",
        "        text = \" \".join([word for word in text.split() if word not in stop_words]) # Remove stop words\n",
        "        reddit_data.append({'Topic': topic, 'Title': post.title, 'Text': text})\n",
        "\n",
        "# Extract data from Wikipedia\n",
        "wiki_data = []\n",
        "for topic in subreddits.keys():\n",
        "    page = wiki.page(topic)\n",
        "    if page.exists():\n",
        "        text = page.title + \" \" + page.text\n",
        "        text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags\n",
        "        text = \" \".join([word for word in text.split() if word not in stop_words]) # Remove stop words\n",
        "        wiki_data.append({'Topic': topic, 'Title': page.title, 'Text': text})\n",
        "\n",
        "# Combine the data into a single pandas dataframe\n",
        "# data = reddit_data + wiki_data\n",
        "# data = wiki_data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QApBKVxV9sw6",
        "outputId": "0d7fea4f-be09-47a3-c34d-08e9f853e8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Topic                                              Title  \\\n",
            "0     Politics  Megathread: Manhattan Grand Jury Votes To Indi...   \n",
            "1  Environment  75% of Americans believe it's better for the e...   \n",
            "2   Technology  Taylor Swift didn't sign $100 million FTX spon...   \n",
            "3   Healthcare                         US healthcare, as a comedy   \n",
            "4    Education  US Fascism Is Spreading Under the Guise of “Pa...   \n",
            "5     Politics                                           Politics   \n",
            "6  Environment                                        Environment   \n",
            "7   Technology                                         Technology   \n",
            "8   Healthcare                                        Health care   \n",
            "9    Education                                          Education   \n",
            "\n",
            "                                                Text  \n",
            "0  Megathread: Manhattan Grand Jury Votes To Indi...  \n",
            "1  75% Americans believe better environment house...  \n",
            "2  Taylor Swift sign $100 million FTX sponsorship...  \n",
            "3                              US healthcare, comedy  \n",
            "4  US Fascism Is Spreading Under Guise “Patriotic...  \n",
            "5  Politics Politics (from Greek: Πολιτικά, polit...  \n",
            "6  Environment Environment often refers to: Natur...  \n",
            "7  Technology Technology application knowledge ac...  \n",
            "8  Health care Health care, healthcare, improveme...  \n",
            "9  Education Education purposeful activity direct...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asyncpraw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K25CekaY_T41",
        "outputId": "d88578fe-49d0-4dec-ffd5-d64c53e77070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting asyncpraw\n",
            "  Downloading asyncpraw-7.7.0-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.5/196.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asyncprawcore<3,>=2.1\n",
            "  Downloading asyncprawcore-2.3.0-py3-none-any.whl (18 kB)\n",
            "Collecting aiosqlite<=0.17.0\n",
            "  Downloading aiosqlite-0.17.0-py3-none-any.whl (15 kB)\n",
            "Collecting aiofiles<1\n",
            "  Downloading aiofiles-0.8.0-py3-none-any.whl (13 kB)\n",
            "Collecting asyncio-extras<=1.3.2\n",
            "  Downloading asyncio_extras-1.3.2-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.8.4)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (20.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.4)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.5.0)\n",
            "Collecting async-generator>=1.3\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.4)\n",
            "Installing collected packages: async-generator, aiosqlite, aiofiles, asyncio-extras, asyncprawcore, asyncpraw\n",
            "Successfully installed aiofiles-0.8.0 aiosqlite-0.17.0 async-generator-1.10 asyncio-extras-1.3.2 asyncpraw-7.7.0 asyncprawcore-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import asyncpraw\n",
        "import wikipediaapi\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')  # Download the stopwords\n",
        "\n",
        "async def main():\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # Initialize the Reddit API client\n",
        "        # Initialize the Reddit API client\n",
        "        reddit = praw.Reddit(\n",
        "            client_id=\"PFSGLhAmS6XFjeH6DUL6uQ\",\n",
        "            client_secret=\"GfYobly4x7xfgrvDoVRR2tcxcOMjEQ\",\n",
        "            # redirect_uri=\"your_redirect_uri\",\n",
        "            user_agent=\"Dramatic-Act-4165\"\n",
        "            # username=\"your_username\",\n",
        "            # password=\"your_password\",\n",
        "        )\n",
        "\n",
        "\n",
        "        # Initialize the Wikipedia API client\n",
        "        wiki = wikipediaapi.Wikipedia('en')\n",
        "\n",
        "        # Define the list of subreddits to search for each topic\n",
        "        subreddits = {\n",
        "            'Politics': 'politics',\n",
        "            'Environment': 'environment',\n",
        "            'Technology': 'technology',\n",
        "            'Healthcare': 'healthcare',\n",
        "            'Education': 'education'\n",
        "        }\n",
        "\n",
        "        # Define the number of posts to search on each subreddit\n",
        "        num_posts = 100\n",
        "\n",
        "        # Define the number of top posts to select\n",
        "        num_top_posts = 1\n",
        "\n",
        "        # Define the set of stop words to remove\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Extract data from Reddit\n",
        "        reddit_data = []\n",
        "        for topic, subreddit_name in subreddits.items():\n",
        "            subreddit = await reddit.subreddit(subreddit_name)\n",
        "            async for post in subreddit.search(topic, limit=num_posts):\n",
        "                text = post.title + \" \" + post.selftext\n",
        "                text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags\n",
        "                text = \" \".join([word for word in text.split() if word not in stop_words]) # Remove stop words\n",
        "                reddit_data.append({'Topic': topic, 'Title': post.title, 'Text': text})\n",
        "                if len(reddit_data) >= len(subreddits) * num_top_posts:\n",
        "                    break\n",
        "\n",
        "        # Extract data from Wikipedia\n",
        "        wiki_data = []\n",
        "        for topic in subreddits.keys():\n",
        "            page = wiki.page(topic)\n",
        "            if page.exists():\n",
        "                text = page.title + \" \" + page.text\n",
        "                text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags\n",
        "                text = \" \".join([word for word in text.split() if word not in stop_words]) # Remove stop words\n",
        "                wiki_data.append({'Topic': topic, 'Title': page.title, 'Text': text})\n",
        "\n",
        "        # Combine the data into a single pandas dataframe\n",
        "        data = reddit_data + wiki_data\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Display the dataframe\n",
        "        print(df)\n",
        "\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.run_until_complete(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "pGbL-0yB_HYh",
        "outputId": "ea9e58da-f140-4466-bebf-775c4ec4d07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ad261ab0bb92>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0mnew_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/asyncio/base_events.py\u001b[0m in \u001b[0;36m_check_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzrDhPS1QCW"
      },
      "source": [
        "Unsurprisingly, we got perfect accuracy. This is because the data set is only a handful of utterances, and we can perfectly memorize it in this LSTM. Nonetheless, a great success!\n",
        "\n",
        "# What's next!\n",
        "\n",
        "The sky's the limit! Be sure to check out our [GitHub](https://github.com/facebookresearch/ParlAI) and [Follow ParlAI on Twitter](https://twitter.com/parlai_parley). We're eager to hear what you are using ParlAI for!\n",
        "\n",
        "Here are some other great resources:\n",
        "- [Our research page](https://parl.ai/projects/)\n",
        "- [ParlAI Documentations](https://parl.ai/docs/index.html)\n",
        "- [Tutorial: Writing a Ranker model](https://parl.ai/docs/tutorial_torch_ranker_agent.html)\n",
        "- [Tutorial: Using Mechanical Turk](https://parl.ai/docs/tutorial_mturk.html)\n",
        "- [Tutorial: Connecting to chat services](https://parl.ai/docs/tutorial_chat_service.html)"
      ]
    }
  ]
}